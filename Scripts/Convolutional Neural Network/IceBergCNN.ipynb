{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "* pickle - Used for importing created features\n",
    "* numpy - Used for working with arrays\n",
    "* TensorFlow - For creating deep neural network graphs and later processing them\n",
    "* Keras ImageDataGenerator - Used for randomly changing input data for more robust learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import the data\n",
    "The data is already pre-processed and we are just using mixture of horizontal and vertical data. This has provided me better classification result when compared against using only horizontal or vartical. Also, the performance is comparable (but not better :/) if we combine horizontal, verticle and summed horizontal and vertical images. But it reduces the size of our network. \n",
    "* provide the path of pickle files\n",
    "* Load and convert the data into numpy arrays/matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_train = open(\"train.pickle\",\"rb\")\n",
    "pickle_valid = open(\"valid.pickle\",\"rb\")\n",
    "\n",
    "trainX = pickle.load(pickle_train)\n",
    "validX = pickle.load(pickle_valid)\n",
    "\n",
    "X_train = np.array(trainX['xtrain'],dtype=np.float32)\n",
    "X_valid = np.array(validX['xvalid'],dtype=np.float32)\n",
    "\n",
    "y_train = np.array(trainX['ytrain'],dtype=np.int32)\n",
    "y_valid = np.array(validX['yvalid'],dtype=np.int32)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "    \n",
    "datagen.fit(X_train.reshape([-1,75,75,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Global Variables\n",
    "Create some global variables/parameters used by our network.\n",
    "* image_size - size of images, here 75 x 75\n",
    "* n_class - one hot encoding of iceberg or ship\n",
    "* batch_size - size of the batch which will be supplied to train our network. Using 32 as it create a medium sized tensors but if I use a bigger batch size (62, 128 etc), the size of tensors get large and problematic to train on my laptop.\n",
    "* epocs - No. of times the network sees the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size = 75\n",
    "n_class = 2\n",
    "batch_size = 32\n",
    "epochs = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Design the Graph\n",
    "----------------------------\n",
    "### 4.1 Methods\n",
    "Some helpful functions which will be repeatedly used while creating the graphs:\n",
    "* weight_variable: initializes weights. Using Xavier Initialization as provides better starting weights than initializing with other techniques and results in faster converging solutions.\n",
    "    * inputs shape of tensor and name for the variable\n",
    "* bias_variable: creates bias variable. Initialize it with small constant weight.\n",
    "    * inputs size based on weight and name of the bias\n",
    "* conv2d: creates a 2D convolutional layer with stride as [1,1,1,1] and padding as same\n",
    "    * inputs the tensor (original or from previous layer) and weights of the layer.\n",
    "* max_pool_3x3: max pooling layer with a 3 x 3 window size. Helpful in reducing the dimension.\n",
    "    * inputs the tensor (original or from previous layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape,nm):\n",
    "  initial = tf.contrib.layers.xavier_initializer()\n",
    "  return tf.get_variable(nm,shape=shape,initializer=initial)\n",
    "\n",
    "def bias_variable(shape,nm):\n",
    "  initial = tf.constant(0.1, shape=shape,name=nm)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_3x3(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 3, 3, 1],\n",
    "                        strides=[1, 3, 3, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Input Placeholders\n",
    "Create placeholders for passing input and output while processing the graph at runtime. 'None' helps in creating dynamic shapes which gets initialized during runtime. It will depend on the size of input. So, while training our batch is 32, x will be a tensor of shape [32,75,75,1] and [32,2] for y_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, image_size, image_size, 1])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, n_class])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Hidden Layer 1 \n",
    "In first layer a window of 5X5 is convoluted over the input image. This step gives us output of shape of [32,75,75,1] which is \n",
    "then passed into a relu activation function. After that we perform max pool across 3x3 window and this gives us [32,25,25,32]. \n",
    "h_pool1 is the output of the first convolution layer which will passed to another convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_conv1 = weight_variable([5, 5, 1, 32],\"W1\")\n",
    "b_conv1 = bias_variable([32],\"b1\")\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_3x3(h_conv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Hidden Layer 2 \n",
    "Again a window of 5X5 is convoluted over the input image. This step gives us output of shape of [32,25,25,64] which is then passed into a relu activation function. After that we perform max pool across 3x3 window and this gives us [32,9,9,64]. h_pool2 is flattened into [32,5184] (9x9x64=5184) passed to fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_conv2 = weight_variable([5,5,32,64],'W2')\n",
    "b_conv2 = bias_variable([64],'b2')\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_3x3(h_conv2)\n",
    "\n",
    "h_fc_flat = tf.reshape(h_pool2,[-1,9*9*64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Hidden Layer 3\n",
    "After the convolutional layers, we will add fully connected layers. Output is of size [32,2048]. Adding dropout for more roboust learning by the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_fc1 = weight_variable([9 * 9 * 64, 2048],\"W3\")\n",
    "b_fc1 = bias_variable([2048],\"b3\")\n",
    "\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_fc_flat, W_fc1) + b_fc1)\n",
    "\n",
    "drop_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1,drop_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Hidden Layer 4\n",
    "Another fully connected layer with droput. Output with shape [32, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_fc2 = weight_variable([2048,1024],'W4')\n",
    "b_fc2 = bias_variable([1024],'b4')\n",
    "\n",
    "h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop,W_fc2) + b_fc2)\n",
    "h_fc2_drop = tf.nn.dropout(h_fc2,drop_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Output Layer\n",
    "Final Layer which will provide the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_fc3 = weight_variable([1024,n_class],'W5')\n",
    "b_fc3 = bias_variable([n_class],'b5')\n",
    " \n",
    "\n",
    "y_out = tf.matmul(h_fc2_drop,W_fc3) + b_fc3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 Loss, Optimization and Accuracy\n",
    "* Cross entropy between true class y_ and value predict by our model y_out is calculated and then softmax operation is performed. Then mean of the cross entropy over the batch is calculated.  \n",
    "* Among the choices availabe for gradient descent, it's best to use adam optimizer which has momentum and decay function. Among different values I used for learning rate, values around 1e-4 gives the best results.\n",
    "* Accuracy is calculated to quantify the performance of the our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_,logits=y_out))\n",
    "\n",
    "train_optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(y_out, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization\n",
    "After designing the NN graph, we will plot some TF graphs for see how our TF network trains and performs over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.summary.scalar(name='Loss', tensor=cross_entropy)\n",
    "tf.summary.scalar(name='Accuracy',tensor=accuracy)\n",
    "graph_summary = tf.summary.merge_all()\n",
    "tf_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Trainig the NN Graph\n",
    "Intialize the weights, biases and variable for writing summaries into disk. Generate the training and validation data using the Keras image data generator and pass for training and validation.\n",
    "Lets train our network and find how it performs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(logdir='./Iceberg/CNN', graph=sess.graph)\n",
    "    for e in range(epochs):\n",
    "        n_batches = int(X_train.shape[0] / batch_size)\n",
    "        print(\"Epoch:{} \".format(e))\n",
    "        gen_train = datagen.flow(X_train, y_train,\n",
    "                                           batch_size=batch_size)\n",
    "        for batch in range(n_batches):\n",
    "            batch_data = gen_train.next()\n",
    "            sess.run(train_optimizer,feed_dict={x: batch_data[0], \n",
    "                                                y_: batch_data[1], \n",
    "                                                drop_prob: 0.5})\n",
    "            if(batch % 10 == 0):\n",
    "                train_acc = accuracy.eval(feed_dict={x: batch_data[0], \n",
    "                                                y_: batch_data[1], \n",
    "                                                drop_prob: 0.5})\n",
    "                batch_loss, summary = sess.run([train_optimizer,graph_summary],\n",
    "                                               feed_dict={x: batch_data[0], \n",
    "                                                            y_: batch_data[1], \n",
    "                                                            drop_prob: 0.5})\n",
    "                writer.add_summary(summary,global_step=global_step)\n",
    "                print(\" - Batch no: {}, train accuracy: {}\".format(batch,train_acc))\n",
    "            global_step += 1\n",
    "        valid_accuracy = accuracy.eval(feed_dict={x: X_valid, y_: y_valid, drop_prob: 1.0})\n",
    "        print(\"-Validation accuracy after epoc: {} \".format(valid_accuracy))\n",
    "    #Save your model\n",
    "    tf_saver.save(sess, save_path='./Iceberg/CNN/SavedModel')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
