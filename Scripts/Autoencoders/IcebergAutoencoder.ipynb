{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "* pickle - Used for importing created features\n",
    "* numpy - Used for working with arrays\n",
    "* TensorFlow - For creating deep neural network graphs and later processing them\n",
    "* Keras ImageDataGenerator - Used for randomly changing input data for more robust learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import the data\n",
    "The data is already pre-processed and we are just using mixture of horizontal and vertical data. This has provided me better classification result when compared against using only horizontal or vartical. Also, the performance is comparable (but not better :/) if we combine horizontal, verticle and summed horizontal and vertical images. But it reduces the size of our network. \n",
    "* provide the path of pickle files\n",
    "* Load and convert the data into numpy arrays/matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_train = open(\"train.pickle\",\"rb\")\n",
    "pickle_valid = open(\"valid.pickle\",\"rb\")\n",
    "\n",
    "trainX = pickle.load(pickle_train)\n",
    "validX = pickle.load(pickle_valid)\n",
    "\n",
    "X_train = np.array(trainX['xtrain'],dtype=np.float32)\n",
    "X_valid = np.array(validX['xvalid'],dtype=np.float32)\n",
    "\n",
    "y_train = np.array(trainX['ytrain'],dtype=np.int32)\n",
    "y_valid = np.array(validX['yvalid'],dtype=np.int32)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    horizontal_flip=True,  # randomly flip images\n",
    "    vertical_flip=False)  # randomly flip images\n",
    "    \n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Global Variables\n",
    "Create some global variables/parameters used by our network.\n",
    "* image_size - size of images, here 75 x 75\n",
    "* n_class - one hot encoding of iceberg or ship\n",
    "* batch_size - size of the batch which will be supplied to train our network. Using 32 as it create a medium sized tensors but if I use a bigger batch size (62, 128 etc), the size of tensors get large and problematic to train on my laptop.\n",
    "* epocs - No. of times the network sees the whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size = 75\n",
    "n_class = 2\n",
    "n_layer1 = 6144\n",
    "encoder_output = 32\n",
    "learning_rate = 9e-5\n",
    "batch_size = 32\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Design the Graph\n",
    "----------------------------\n",
    "### 4.1 Methods\n",
    "Some helpful functions which will be repeatedly used while creating the graphs:\n",
    "* weight_variable: initializes weights. Using Xavier Initialization as provides better starting weights than initializing with other techniques and results in faster converging solutions.\n",
    "    * inputs shape of tensor and name for the variable\n",
    "* bias_variable: creates bias variable. Initialize it with small constant weight.\n",
    "    * inputs size based on weight and name of the bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape,nm):\n",
    "  initial = tf.contrib.layers.xavier_initializer()\n",
    "  return tf.get_variable(nm,shape=shape,initializer=initial)\n",
    "\n",
    "def bias_variable(shape,nm):\n",
    "  initial = tf.constant(0.01, shape=shape,name=nm)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Placeholder\n",
    "Input and output placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, image_size * image_size])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, image_size * image_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Encoder\n",
    "2-fully connected layer and then an output layer with size 32. The encoder should reduce the dataset and find the best possible 32 features. The output of encoder is passed on to the decoder to recreate the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_e1 = weight_variable([image_size * image_size, n_layer1],\"W_e1\")\n",
    "b_e1 = bias_variable([n_layer1],\"b_e1\")\n",
    "\n",
    "e_layer1 = tf.nn.relu(tf.add(tf.matmul(x, W_e1), b_e1),name='e_l1')\n",
    "\n",
    "W_e2 = weight_variable([n_layer1, n_layer1],\"W_e2\")\n",
    "b_e2 = bias_variable([n_layer1],\"b_e2\")\n",
    "\n",
    "e_layer2 = tf.nn.relu(tf.add(tf.matmul(e_layer1, W_e2), b_e2),name='e_l2')\n",
    "\n",
    "W_e3 = weight_variable([n_layer1, encoder_output],\"W_e3\")\n",
    "b_e3 = bias_variable([encoder_output],\"b_e3\")\n",
    "\n",
    "e_out = tf.nn.relu(tf.add(tf.matmul(e_layer2, W_e3), b_e3),name='e_l3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Decoder\n",
    "It receives the output from encoder and then recreates the image. It consists of 2-layers of fully connected layers and then outputs the real image sized output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W_d1 = weight_variable([encoder_output, n_layer1],\"W_d1\")\n",
    "b_d1 = bias_variable([n_layer1],\"b_d1\")\n",
    "\n",
    "d_layer1 = tf.nn.relu(tf.add(tf.matmul(e_out, W_d1), b_d1),name='d_l1')\n",
    "\n",
    "W_d2 = weight_variable([n_layer1, n_layer1],\"W_d2\")\n",
    "b_d2 = bias_variable([n_layer1],\"b_d2\")\n",
    "\n",
    "d_layer2 = tf.nn.relu(tf.add(tf.matmul(d_layer1, W_d2), b_d2),name='d_l2')\n",
    "\n",
    "W_d3 = weight_variable([n_layer1, image_size * image_size],\"W_d3\")\n",
    "b_d3 = bias_variable([image_size * image_size],\"b_d3\")\n",
    "\n",
    "d_out = tf.nn.relu(tf.add(tf.matmul(d_layer2, W_d3), b_d3),name='d_l3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Loss and Optimization\n",
    "* loss - Mean of the square distance between input image and output image\n",
    "* train_optimizer - AdamOtimizer with learning rate 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.square(y_ - d_out))\n",
    "\n",
    "train_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization\n",
    "Plot the loss and encoder distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.summary.scalar(name='Loss', tensor=loss)\n",
    "tf.summary.histogram(name='Encoder_Distribution', values=e_out)\n",
    "summary_ae = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training\n",
    "Train the model on out data and find the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 \n",
      " - Batch no: 0, train loss: 1.201899766921997\n",
      " - Batch no: 8, train loss: 0.9124476909637451\n",
      " - Batch no: 16, train loss: 0.6679865121841431\n",
      " - Batch no: 24, train loss: 0.7741681933403015\n",
      "-Validation loss after epoc: 0.6996757984161377 \n",
      "Epoch:1 \n",
      " - Batch no: 0, train loss: 0.7932273149490356\n",
      " - Batch no: 8, train loss: 0.7835976481437683\n",
      " - Batch no: 16, train loss: 0.773699164390564\n",
      " - Batch no: 24, train loss: 1.0435341596603394\n",
      "-Validation loss after epoc: 0.6788281798362732 \n",
      "Epoch:2 \n",
      " - Batch no: 0, train loss: 0.6922950744628906\n",
      " - Batch no: 8, train loss: 0.9551734924316406\n",
      " - Batch no: 16, train loss: 0.6900110840797424\n",
      " - Batch no: 24, train loss: 0.6741445660591125\n",
      "-Validation loss after epoc: 0.6700567007064819 \n",
      "Epoch:3 \n",
      " - Batch no: 0, train loss: 0.8280214071273804\n",
      " - Batch no: 8, train loss: 0.7751855850219727\n",
      " - Batch no: 16, train loss: 0.6818187832832336\n",
      " - Batch no: 24, train loss: 0.6612483859062195\n",
      "-Validation loss after epoc: 0.6622953414916992 \n",
      "Epoch:4 \n",
      " - Batch no: 0, train loss: 0.5873791575431824\n",
      " - Batch no: 8, train loss: 0.6723609566688538\n",
      " - Batch no: 16, train loss: 0.572475254535675\n",
      " - Batch no: 24, train loss: 0.8364993333816528\n",
      "-Validation loss after epoc: 0.6542667150497437 \n",
      "Epoch:5 \n",
      " - Batch no: 0, train loss: 0.8279969692230225\n",
      " - Batch no: 8, train loss: 0.7788286209106445\n",
      " - Batch no: 16, train loss: 0.8243222236633301\n",
      " - Batch no: 24, train loss: 0.5583954453468323\n",
      "-Validation loss after epoc: 0.6447819471359253 \n",
      "Epoch:6 \n",
      " - Batch no: 0, train loss: 0.7681961059570312\n",
      " - Batch no: 8, train loss: 0.617983877658844\n",
      " - Batch no: 16, train loss: 0.6265498399734497\n",
      " - Batch no: 24, train loss: 0.868213951587677\n",
      "-Validation loss after epoc: 0.6419106125831604 \n",
      "Epoch:7 \n",
      " - Batch no: 0, train loss: 0.49490153789520264\n",
      " - Batch no: 8, train loss: 0.8709389567375183\n",
      " - Batch no: 16, train loss: 0.8111108541488647\n",
      " - Batch no: 24, train loss: 0.7548002004623413\n",
      "-Validation loss after epoc: 0.6336852312088013 \n",
      "Epoch:8 \n",
      " - Batch no: 0, train loss: 0.7788117527961731\n",
      " - Batch no: 8, train loss: 0.6733973622322083\n",
      " - Batch no: 16, train loss: 0.7997763156890869\n",
      " - Batch no: 24, train loss: 0.5558338165283203\n",
      "-Validation loss after epoc: 0.6291645169258118 \n",
      "Epoch:9 \n",
      " - Batch no: 0, train loss: 0.4119125306606293\n",
      " - Batch no: 8, train loss: 0.6191149353981018\n",
      " - Batch no: 16, train loss: 0.762392520904541\n",
      " - Batch no: 24, train loss: 0.7728566527366638\n",
      "-Validation loss after epoc: 0.6238003969192505 \n",
      "Epoch:10 \n",
      " - Batch no: 0, train loss: 0.6573991179466248\n",
      " - Batch no: 8, train loss: 0.807043731212616\n",
      " - Batch no: 16, train loss: 0.6818628311157227\n",
      " - Batch no: 24, train loss: 0.6292563676834106\n",
      "-Validation loss after epoc: 0.6230723857879639 \n",
      "Epoch:11 \n",
      " - Batch no: 0, train loss: 0.6498614549636841\n",
      " - Batch no: 8, train loss: 0.7459659576416016\n",
      " - Batch no: 16, train loss: 0.6476245522499084\n",
      " - Batch no: 24, train loss: 0.7247112989425659\n",
      "-Validation loss after epoc: 0.6196228861808777 \n",
      "Epoch:12 \n",
      " - Batch no: 0, train loss: 0.6783092617988586\n",
      " - Batch no: 8, train loss: 0.5900519490242004\n",
      " - Batch no: 16, train loss: 0.6991659998893738\n",
      " - Batch no: 24, train loss: 0.49965786933898926\n",
      "-Validation loss after epoc: 0.6168255805969238 \n",
      "Epoch:13 \n",
      " - Batch no: 0, train loss: 0.638272225856781\n",
      " - Batch no: 8, train loss: 0.7258481979370117\n",
      " - Batch no: 16, train loss: 0.6263836622238159\n",
      " - Batch no: 24, train loss: 0.7846295237541199\n",
      "-Validation loss after epoc: 0.6111610531806946 \n",
      "Epoch:14 \n",
      " - Batch no: 0, train loss: 0.6002715826034546\n",
      " - Batch no: 8, train loss: 0.6825295686721802\n",
      " - Batch no: 16, train loss: 0.6825018525123596\n",
      " - Batch no: 24, train loss: 0.6921906471252441\n",
      "-Validation loss after epoc: 0.6102675199508667 \n",
      "Epoch:15 \n",
      " - Batch no: 0, train loss: 0.6115114092826843\n",
      " - Batch no: 8, train loss: 0.8091441988945007\n",
      " - Batch no: 16, train loss: 0.6580035090446472\n",
      " - Batch no: 24, train loss: 0.6143836379051208\n",
      "-Validation loss after epoc: 0.6067067384719849 \n",
      "Epoch:16 \n",
      " - Batch no: 0, train loss: 0.5861685276031494\n",
      " - Batch no: 8, train loss: 0.6595203280448914\n",
      " - Batch no: 16, train loss: 0.691075325012207\n",
      " - Batch no: 24, train loss: 0.7191821932792664\n",
      "-Validation loss after epoc: 0.6062357425689697 \n",
      "Epoch:17 \n",
      " - Batch no: 0, train loss: 0.800075888633728\n",
      " - Batch no: 8, train loss: 0.5935099124908447\n",
      " - Batch no: 16, train loss: 0.7563095688819885\n",
      " - Batch no: 24, train loss: 0.6074039340019226\n",
      "-Validation loss after epoc: 0.6022021174430847 \n",
      "Epoch:18 \n",
      " - Batch no: 0, train loss: 0.615594208240509\n",
      " - Batch no: 8, train loss: 0.6676410436630249\n",
      " - Batch no: 16, train loss: 0.7069354057312012\n",
      " - Batch no: 24, train loss: 0.5384173393249512\n",
      "-Validation loss after epoc: 0.5977275371551514 \n",
      "Epoch:19 \n",
      " - Batch no: 0, train loss: 0.6310287117958069\n",
      " - Batch no: 8, train loss: 0.6424639225006104\n",
      " - Batch no: 16, train loss: 0.5557049512863159\n",
      " - Batch no: 24, train loss: 0.8176962733268738\n",
      "-Validation loss after epoc: 0.5967574119567871 \n",
      "Epoch:20 \n",
      " - Batch no: 0, train loss: 0.6284559369087219\n",
      " - Batch no: 8, train loss: 0.7418187260627747\n",
      " - Batch no: 16, train loss: 0.5443310737609863\n",
      " - Batch no: 24, train loss: 0.5176653265953064\n",
      "-Validation loss after epoc: 0.5945041179656982 \n",
      "Epoch:21 \n",
      " - Batch no: 0, train loss: 0.7790571451187134\n",
      " - Batch no: 8, train loss: 0.7945911288261414\n",
      " - Batch no: 16, train loss: 0.6193496584892273\n",
      " - Batch no: 24, train loss: 0.6404311656951904\n",
      "-Validation loss after epoc: 0.5941769480705261 \n",
      "Epoch:22 \n",
      " - Batch no: 0, train loss: 0.557917594909668\n",
      " - Batch no: 8, train loss: 0.5978213548660278\n",
      " - Batch no: 16, train loss: 0.7367140650749207\n",
      " - Batch no: 24, train loss: 0.6388724446296692\n",
      "-Validation loss after epoc: 0.5952784419059753 \n",
      "Epoch:23 \n",
      " - Batch no: 0, train loss: 0.5317684412002563\n",
      " - Batch no: 8, train loss: 0.7387548685073853\n",
      " - Batch no: 16, train loss: 0.6687588095664978\n",
      " - Batch no: 24, train loss: 0.6822562217712402\n",
      "-Validation loss after epoc: 0.5906426906585693 \n",
      "Epoch:24 \n",
      " - Batch no: 0, train loss: 0.5237635374069214\n",
      " - Batch no: 8, train loss: 0.7594398856163025\n",
      " - Batch no: 16, train loss: 0.5903734564781189\n",
      " - Batch no: 24, train loss: 0.6362110376358032\n",
      "-Validation loss after epoc: 0.5904881358146667 \n",
      "Epoch:25 \n",
      " - Batch no: 0, train loss: 0.5505470037460327\n",
      " - Batch no: 8, train loss: 0.6840702891349792\n",
      " - Batch no: 16, train loss: 0.6837098598480225\n",
      " - Batch no: 24, train loss: 0.6924170255661011\n",
      "-Validation loss after epoc: 0.5873919129371643 \n",
      "Epoch:26 \n",
      " - Batch no: 0, train loss: 0.632170557975769\n",
      " - Batch no: 8, train loss: 0.5550022721290588\n",
      " - Batch no: 16, train loss: 0.7282077074050903\n",
      " - Batch no: 24, train loss: 0.5593445301055908\n",
      "-Validation loss after epoc: 0.5865220427513123 \n",
      "Epoch:27 \n",
      " - Batch no: 0, train loss: 0.6033589243888855\n",
      " - Batch no: 8, train loss: 0.6416229009628296\n",
      " - Batch no: 16, train loss: 0.5458886027336121\n",
      " - Batch no: 24, train loss: 0.6098427772521973\n",
      "-Validation loss after epoc: 0.5848786234855652 \n",
      "Epoch:28 \n",
      " - Batch no: 0, train loss: 0.8936196565628052\n",
      " - Batch no: 8, train loss: 0.5989084243774414\n",
      " - Batch no: 16, train loss: 0.6897448301315308\n",
      " - Batch no: 24, train loss: 0.5083574652671814\n",
      "-Validation loss after epoc: 0.583396315574646 \n",
      "Epoch:29 \n",
      " - Batch no: 0, train loss: 0.4608090817928314\n",
      " - Batch no: 8, train loss: 0.6844759583473206\n",
      " - Batch no: 16, train loss: 0.4770887494087219\n",
      " - Batch no: 24, train loss: 0.6173118948936462\n",
      "-Validation loss after epoc: 0.5829961895942688 \n",
      "Epoch:30 \n",
      " - Batch no: 0, train loss: 0.6264585852622986\n",
      " - Batch no: 8, train loss: 0.6821218132972717\n",
      " - Batch no: 16, train loss: 0.5372642278671265\n",
      " - Batch no: 24, train loss: 0.6748416423797607\n",
      "-Validation loss after epoc: 0.5809022784233093 \n",
      "Epoch:31 \n",
      " - Batch no: 0, train loss: 0.5659391283988953\n",
      " - Batch no: 8, train loss: 0.448195219039917\n",
      " - Batch no: 16, train loss: 0.6223663091659546\n",
      " - Batch no: 24, train loss: 0.6353952884674072\n",
      "-Validation loss after epoc: 0.5800046920776367 \n",
      "Epoch:32 \n",
      " - Batch no: 0, train loss: 0.5598163604736328\n",
      " - Batch no: 8, train loss: 0.43101197481155396\n",
      " - Batch no: 16, train loss: 0.8938760757446289\n",
      " - Batch no: 24, train loss: 0.6460134387016296\n",
      "-Validation loss after epoc: 0.5797398090362549 \n",
      "Epoch:33 \n",
      " - Batch no: 0, train loss: 0.7855498194694519\n",
      " - Batch no: 8, train loss: 0.5358432531356812\n",
      " - Batch no: 16, train loss: 0.5871835350990295\n",
      " - Batch no: 24, train loss: 0.6896623969078064\n",
      "-Validation loss after epoc: 0.579258918762207 \n",
      "Epoch:34 \n",
      " - Batch no: 0, train loss: 0.631385862827301\n",
      " - Batch no: 8, train loss: 0.5371475219726562\n",
      " - Batch no: 16, train loss: 0.47880950570106506\n",
      " - Batch no: 24, train loss: 0.6227712631225586\n",
      "-Validation loss after epoc: 0.5788107514381409 \n",
      "Epoch:35 \n",
      " - Batch no: 0, train loss: 0.6011673212051392\n",
      " - Batch no: 8, train loss: 0.5317685008049011\n",
      " - Batch no: 16, train loss: 0.6009486317634583\n",
      " - Batch no: 24, train loss: 0.6471900939941406\n",
      "-Validation loss after epoc: 0.5786269903182983 \n",
      "Epoch:36 \n",
      " - Batch no: 0, train loss: 0.6649924516677856\n",
      " - Batch no: 8, train loss: 0.5343836545944214\n",
      " - Batch no: 16, train loss: 0.530726432800293\n",
      " - Batch no: 24, train loss: 0.5823377370834351\n",
      "-Validation loss after epoc: 0.5764199495315552 \n",
      "Epoch:37 \n",
      " - Batch no: 0, train loss: 0.7110918164253235\n",
      " - Batch no: 8, train loss: 0.6046921014785767\n",
      " - Batch no: 16, train loss: 0.555304229259491\n",
      " - Batch no: 24, train loss: 0.6013105511665344\n",
      "-Validation loss after epoc: 0.5754736661911011 \n",
      "Epoch:38 \n",
      " - Batch no: 0, train loss: 0.7131508588790894\n",
      " - Batch no: 8, train loss: 0.6074322462081909\n",
      " - Batch no: 16, train loss: 0.7135658860206604\n",
      " - Batch no: 24, train loss: 0.7135264873504639\n",
      "-Validation loss after epoc: 0.5764586329460144 \n",
      "Epoch:39 \n",
      " - Batch no: 0, train loss: 0.5249852538108826\n",
      " - Batch no: 8, train loss: 0.606220006942749\n",
      " - Batch no: 16, train loss: 0.5862584710121155\n",
      " - Batch no: 24, train loss: 0.5810391902923584\n",
      "-Validation loss after epoc: 0.5733962655067444 \n",
      "Epoch:40 \n",
      " - Batch no: 0, train loss: 0.7211763262748718\n",
      " - Batch no: 8, train loss: 0.5029418468475342\n",
      " - Batch no: 16, train loss: 0.5793452262878418\n",
      " - Batch no: 24, train loss: 0.5926564931869507\n",
      "-Validation loss after epoc: 0.5730053782463074 \n",
      "Epoch:41 \n",
      " - Batch no: 0, train loss: 0.504784107208252\n",
      " - Batch no: 8, train loss: 0.797153890132904\n",
      " - Batch no: 16, train loss: 0.6508062481880188\n",
      " - Batch no: 24, train loss: 0.4522905945777893\n",
      "-Validation loss after epoc: 0.5725778341293335 \n",
      "Epoch:42 \n",
      " - Batch no: 0, train loss: 0.8387901186943054\n",
      " - Batch no: 8, train loss: 0.5619401335716248\n",
      " - Batch no: 16, train loss: 0.5847698450088501\n",
      " - Batch no: 24, train loss: 0.6590341329574585\n",
      "-Validation loss after epoc: 0.5704026222229004 \n",
      "Epoch:43 \n",
      " - Batch no: 0, train loss: 0.8207557797431946\n",
      " - Batch no: 8, train loss: 0.5751184821128845\n",
      " - Batch no: 16, train loss: 0.741205096244812\n",
      " - Batch no: 24, train loss: 0.7477812170982361\n",
      "-Validation loss after epoc: 0.5704095363616943 \n",
      "Epoch:44 \n",
      " - Batch no: 0, train loss: 0.5916357636451721\n",
      " - Batch no: 8, train loss: 0.7723808288574219\n",
      " - Batch no: 16, train loss: 0.5764074921607971\n",
      " - Batch no: 24, train loss: 0.5999888181686401\n",
      "-Validation loss after epoc: 0.569921612739563 \n",
      "Epoch:45 \n",
      " - Batch no: 0, train loss: 0.4881264567375183\n",
      " - Batch no: 8, train loss: 0.6841845512390137\n",
      " - Batch no: 16, train loss: 0.5985804796218872\n",
      " - Batch no: 24, train loss: 0.7782680988311768\n",
      "-Validation loss after epoc: 0.5698046088218689 \n",
      "Epoch:46 \n",
      " - Batch no: 0, train loss: 0.6112027168273926\n",
      " - Batch no: 8, train loss: 0.6037554144859314\n",
      " - Batch no: 16, train loss: 0.6271594762802124\n",
      " - Batch no: 24, train loss: 0.4851904809474945\n",
      "-Validation loss after epoc: 0.5682263374328613 \n",
      "Epoch:47 \n",
      " - Batch no: 0, train loss: 0.7225511074066162\n",
      " - Batch no: 8, train loss: 0.797972559928894\n",
      " - Batch no: 16, train loss: 0.6276900172233582\n",
      " - Batch no: 24, train loss: 0.5687791705131531\n",
      "-Validation loss after epoc: 0.5680667161941528 \n",
      "Epoch:48 \n",
      " - Batch no: 0, train loss: 0.6285158395767212\n",
      " - Batch no: 8, train loss: 0.7182744145393372\n",
      " - Batch no: 16, train loss: 0.5952171683311462\n",
      " - Batch no: 24, train loss: 0.6407322287559509\n",
      "-Validation loss after epoc: 0.5682651996612549 \n",
      "Epoch:49 \n",
      " - Batch no: 0, train loss: 0.6827865839004517\n",
      " - Batch no: 8, train loss: 0.6670618653297424\n",
      " - Batch no: 16, train loss: 0.419209748506546\n",
      " - Batch no: 24, train loss: 0.749664843082428\n",
      "-Validation loss after epoc: 0.5688870549201965 \n",
      "Epoch:50 \n",
      " - Batch no: 0, train loss: 0.5965656638145447\n",
      " - Batch no: 8, train loss: 0.806152880191803\n",
      " - Batch no: 16, train loss: 0.5961337089538574\n",
      " - Batch no: 24, train loss: 0.5089650750160217\n",
      "-Validation loss after epoc: 0.5677295923233032 \n",
      "Epoch:51 \n",
      " - Batch no: 0, train loss: 0.4371214210987091\n",
      " - Batch no: 8, train loss: 0.6482925415039062\n",
      " - Batch no: 16, train loss: 0.7175450325012207\n",
      " - Batch no: 24, train loss: 0.7111304998397827\n",
      "-Validation loss after epoc: 0.5669950842857361 \n",
      "Epoch:52 \n",
      " - Batch no: 0, train loss: 0.604320764541626\n",
      " - Batch no: 8, train loss: 0.7445070743560791\n",
      " - Batch no: 16, train loss: 0.5786471962928772\n",
      " - Batch no: 24, train loss: 0.6171011924743652\n",
      "-Validation loss after epoc: 0.5668755173683167 \n",
      "Epoch:53 \n",
      " - Batch no: 0, train loss: 0.4950173497200012\n",
      " - Batch no: 8, train loss: 0.6930568218231201\n",
      " - Batch no: 16, train loss: 0.7297910451889038\n",
      " - Batch no: 24, train loss: 0.7386764287948608\n",
      "-Validation loss after epoc: 0.5648976564407349 \n",
      "Epoch:54 \n",
      " - Batch no: 0, train loss: 0.5986422896385193\n",
      " - Batch no: 8, train loss: 0.6410300135612488\n",
      " - Batch no: 16, train loss: 0.5893793106079102\n",
      " - Batch no: 24, train loss: 0.6841720938682556\n",
      "-Validation loss after epoc: 0.5635197162628174 \n",
      "Epoch:55 \n",
      " - Batch no: 0, train loss: 0.4824520945549011\n",
      " - Batch no: 8, train loss: 0.6222735047340393\n",
      " - Batch no: 16, train loss: 0.5034897327423096\n",
      " - Batch no: 24, train loss: 0.7015072703361511\n",
      "-Validation loss after epoc: 0.5647231340408325 \n",
      "Epoch:56 \n",
      " - Batch no: 0, train loss: 0.6240151524543762\n",
      " - Batch no: 8, train loss: 0.5944473743438721\n",
      " - Batch no: 16, train loss: 0.559197187423706\n",
      " - Batch no: 24, train loss: 0.5991098284721375\n",
      "-Validation loss after epoc: 0.563000500202179 \n",
      "Epoch:57 \n",
      " - Batch no: 0, train loss: 0.5132052898406982\n",
      " - Batch no: 8, train loss: 0.5423092842102051\n",
      " - Batch no: 16, train loss: 0.5569754838943481\n",
      " - Batch no: 24, train loss: 0.8244786262512207\n",
      "-Validation loss after epoc: 0.5614351630210876 \n",
      "Epoch:58 \n",
      " - Batch no: 0, train loss: 0.7004567384719849\n",
      " - Batch no: 8, train loss: 0.6593111753463745\n",
      " - Batch no: 16, train loss: 0.6896376609802246\n",
      " - Batch no: 24, train loss: 0.6480538249015808\n",
      "-Validation loss after epoc: 0.5609387755393982 \n",
      "Epoch:59 \n",
      " - Batch no: 0, train loss: 0.875374972820282\n",
      " - Batch no: 8, train loss: 0.7696512937545776\n",
      " - Batch no: 16, train loss: 0.6528846621513367\n",
      " - Batch no: 24, train loss: 0.8403112888336182\n",
      "-Validation loss after epoc: 0.5617657899856567 \n",
      "Epoch:60 \n",
      " - Batch no: 0, train loss: 0.6925441026687622\n",
      " - Batch no: 8, train loss: 0.6633223295211792\n",
      " - Batch no: 16, train loss: 0.5448496341705322\n",
      " - Batch no: 24, train loss: 0.7427012920379639\n",
      "-Validation loss after epoc: 0.5609215497970581 \n",
      "Epoch:61 \n",
      " - Batch no: 0, train loss: 0.760576605796814\n",
      " - Batch no: 8, train loss: 0.7844631671905518\n",
      " - Batch no: 16, train loss: 0.583827018737793\n",
      " - Batch no: 24, train loss: 0.6240650415420532\n",
      "-Validation loss after epoc: 0.5605366230010986 \n",
      "Epoch:62 \n",
      " - Batch no: 0, train loss: 0.5772398114204407\n",
      " - Batch no: 8, train loss: 0.6211282014846802\n",
      " - Batch no: 16, train loss: 0.5139181613922119\n",
      " - Batch no: 24, train loss: 0.4726734459400177\n",
      "-Validation loss after epoc: 0.5608428716659546 \n",
      "Epoch:63 \n",
      " - Batch no: 0, train loss: 0.5949026346206665\n",
      " - Batch no: 8, train loss: 0.3368431031703949\n",
      " - Batch no: 16, train loss: 0.6375625729560852\n",
      " - Batch no: 24, train loss: 0.6903005242347717\n",
      "-Validation loss after epoc: 0.5598394274711609 \n",
      "Epoch:64 \n",
      " - Batch no: 0, train loss: 0.6337608098983765\n",
      " - Batch no: 8, train loss: 0.6122339367866516\n",
      " - Batch no: 16, train loss: 0.5581534504890442\n",
      " - Batch no: 24, train loss: 0.6222472786903381\n",
      "-Validation loss after epoc: 0.5588437914848328 \n",
      "Epoch:65 \n",
      " - Batch no: 0, train loss: 0.7335753440856934\n",
      " - Batch no: 8, train loss: 0.5539731383323669\n",
      " - Batch no: 16, train loss: 0.7571315765380859\n",
      " - Batch no: 24, train loss: 0.4508277177810669\n",
      "-Validation loss after epoc: 0.5582654476165771 \n",
      "Epoch:66 \n",
      " - Batch no: 0, train loss: 0.6839831471443176\n",
      " - Batch no: 8, train loss: 0.499521940946579\n",
      " - Batch no: 16, train loss: 0.45818808674812317\n",
      " - Batch no: 24, train loss: 0.6624811887741089\n",
      "-Validation loss after epoc: 0.5579755902290344 \n",
      "Epoch:67 \n",
      " - Batch no: 0, train loss: 0.5452276468276978\n",
      " - Batch no: 8, train loss: 0.5716084837913513\n",
      " - Batch no: 16, train loss: 0.7285842895507812\n",
      " - Batch no: 24, train loss: 0.6774229407310486\n",
      "-Validation loss after epoc: 0.5572752356529236 \n",
      "Epoch:68 \n",
      " - Batch no: 0, train loss: 0.5104766488075256\n",
      " - Batch no: 8, train loss: 0.641435980796814\n",
      " - Batch no: 16, train loss: 0.5481122136116028\n",
      " - Batch no: 24, train loss: 0.5667234063148499\n",
      "-Validation loss after epoc: 0.5577859282493591 \n",
      "Epoch:69 \n",
      " - Batch no: 0, train loss: 0.6435498595237732\n",
      " - Batch no: 8, train loss: 0.6133996248245239\n",
      " - Batch no: 16, train loss: 0.5445587038993835\n",
      " - Batch no: 24, train loss: 0.7655004858970642\n",
      "-Validation loss after epoc: 0.5575704574584961 \n",
      "Epoch:70 \n",
      " - Batch no: 0, train loss: 0.6252965331077576\n",
      " - Batch no: 8, train loss: 0.6212102174758911\n",
      " - Batch no: 16, train loss: 0.8393632173538208\n",
      " - Batch no: 24, train loss: 0.5812056064605713\n",
      "-Validation loss after epoc: 0.5578867793083191 \n",
      "Epoch:71 \n",
      " - Batch no: 0, train loss: 0.5082199573516846\n",
      " - Batch no: 8, train loss: 0.5691906213760376\n",
      " - Batch no: 16, train loss: 0.6051665544509888\n",
      " - Batch no: 24, train loss: 0.5172291994094849\n",
      "-Validation loss after epoc: 0.5614737272262573 \n",
      "Epoch:72 \n",
      " - Batch no: 0, train loss: 0.6166719794273376\n",
      " - Batch no: 8, train loss: 0.5427638292312622\n",
      " - Batch no: 16, train loss: 0.5857533812522888\n",
      " - Batch no: 24, train loss: 0.6464179158210754\n",
      "-Validation loss after epoc: 0.5587318539619446 \n",
      "Epoch:73 \n",
      " - Batch no: 0, train loss: 0.40453019738197327\n",
      " - Batch no: 8, train loss: 0.5394676923751831\n",
      " - Batch no: 16, train loss: 0.6477593779563904\n",
      " - Batch no: 24, train loss: 0.6418718695640564\n",
      "-Validation loss after epoc: 0.5561200380325317 \n",
      "Epoch:74 \n",
      " - Batch no: 0, train loss: 0.6147158741950989\n",
      " - Batch no: 8, train loss: 0.48035362362861633\n",
      " - Batch no: 16, train loss: 0.4732736051082611\n",
      " - Batch no: 24, train loss: 0.6215611696243286\n",
      "-Validation loss after epoc: 0.5570317506790161 \n",
      "Epoch:75 \n",
      " - Batch no: 0, train loss: 0.5491759777069092\n",
      " - Batch no: 8, train loss: 0.6670972108840942\n",
      " - Batch no: 16, train loss: 0.5736337304115295\n",
      " - Batch no: 24, train loss: 0.532824695110321\n",
      "-Validation loss after epoc: 0.5554618239402771 \n",
      "Epoch:76 \n",
      " - Batch no: 0, train loss: 0.6079694032669067\n",
      " - Batch no: 8, train loss: 0.6513914465904236\n",
      " - Batch no: 16, train loss: 0.5560144782066345\n",
      " - Batch no: 24, train loss: 0.7708554267883301\n",
      "-Validation loss after epoc: 0.5551978349685669 \n",
      "Epoch:77 \n",
      " - Batch no: 0, train loss: 0.4698410928249359\n",
      " - Batch no: 8, train loss: 0.6332264542579651\n",
      " - Batch no: 16, train loss: 0.5865085124969482\n",
      " - Batch no: 24, train loss: 0.6475822329521179\n",
      "-Validation loss after epoc: 0.5540796518325806 \n",
      "Epoch:78 \n",
      " - Batch no: 0, train loss: 0.7616562247276306\n",
      " - Batch no: 8, train loss: 0.716580867767334\n",
      " - Batch no: 16, train loss: 0.5174137353897095\n",
      " - Batch no: 24, train loss: 0.7043623328208923\n",
      "-Validation loss after epoc: 0.5536724328994751 \n",
      "Epoch:79 \n",
      " - Batch no: 0, train loss: 0.8309256434440613\n",
      " - Batch no: 8, train loss: 0.6907334327697754\n",
      " - Batch no: 16, train loss: 0.5502819418907166\n",
      " - Batch no: 24, train loss: 0.4986799657344818\n",
      "-Validation loss after epoc: 0.5531811714172363 \n",
      "Epoch:80 \n",
      " - Batch no: 0, train loss: 0.6438876986503601\n",
      " - Batch no: 8, train loss: 0.6588360667228699\n",
      " - Batch no: 16, train loss: 0.3997949957847595\n",
      " - Batch no: 24, train loss: 0.6912911534309387\n",
      "-Validation loss after epoc: 0.55229252576828 \n",
      "Epoch:81 \n",
      " - Batch no: 0, train loss: 0.4429756999015808\n",
      " - Batch no: 8, train loss: 0.6999804377555847\n",
      " - Batch no: 16, train loss: 0.649974524974823\n",
      " - Batch no: 24, train loss: 0.6933016180992126\n",
      "-Validation loss after epoc: 0.5523377060890198 \n",
      "Epoch:82 \n",
      " - Batch no: 0, train loss: 0.47398799657821655\n",
      " - Batch no: 8, train loss: 0.5631092190742493\n",
      " - Batch no: 16, train loss: 0.5598992109298706\n",
      " - Batch no: 24, train loss: 0.707255482673645\n",
      "-Validation loss after epoc: 0.5525720119476318 \n",
      "Epoch:83 \n",
      " - Batch no: 0, train loss: 0.4888264536857605\n",
      " - Batch no: 8, train loss: 0.49763306975364685\n",
      " - Batch no: 16, train loss: 0.6164042949676514\n",
      " - Batch no: 24, train loss: 0.6575879454612732\n",
      "-Validation loss after epoc: 0.5505263805389404 \n",
      "Epoch:84 \n",
      " - Batch no: 0, train loss: 0.6244750022888184\n",
      " - Batch no: 8, train loss: 0.6944791674613953\n",
      " - Batch no: 16, train loss: 0.7832863330841064\n",
      " - Batch no: 24, train loss: 0.7549431920051575\n",
      "-Validation loss after epoc: 0.5492662787437439 \n",
      "Epoch:85 \n",
      " - Batch no: 0, train loss: 0.748749315738678\n",
      " - Batch no: 8, train loss: 0.6832546591758728\n",
      " - Batch no: 16, train loss: 0.6434279084205627\n",
      " - Batch no: 24, train loss: 0.5798048973083496\n",
      "-Validation loss after epoc: 0.5494502186775208 \n",
      "Epoch:86 \n",
      " - Batch no: 0, train loss: 0.6848238110542297\n",
      " - Batch no: 8, train loss: 0.5921327471733093\n",
      " - Batch no: 16, train loss: 0.4825398623943329\n",
      " - Batch no: 24, train loss: 0.6578612327575684\n",
      "-Validation loss after epoc: 0.5498947501182556 \n",
      "Epoch:87 \n",
      " - Batch no: 0, train loss: 0.660358190536499\n",
      " - Batch no: 8, train loss: 0.5717244744300842\n",
      " - Batch no: 16, train loss: 0.680812656879425\n",
      " - Batch no: 24, train loss: 0.5102293491363525\n",
      "-Validation loss after epoc: 0.5492879748344421 \n",
      "Epoch:88 \n",
      " - Batch no: 0, train loss: 0.5908241868019104\n",
      " - Batch no: 8, train loss: 0.5612642168998718\n",
      " - Batch no: 16, train loss: 0.5864629745483398\n",
      " - Batch no: 24, train loss: 0.6172842383384705\n",
      "-Validation loss after epoc: 0.548301100730896 \n",
      "Epoch:89 \n",
      " - Batch no: 0, train loss: 0.7395598292350769\n",
      " - Batch no: 8, train loss: 0.5650075674057007\n",
      " - Batch no: 16, train loss: 0.5167798399925232\n",
      " - Batch no: 24, train loss: 0.5034706592559814\n",
      "-Validation loss after epoc: 0.5484986305236816 \n",
      "Epoch:90 \n",
      " - Batch no: 0, train loss: 0.644283652305603\n",
      " - Batch no: 8, train loss: 0.5921595692634583\n",
      " - Batch no: 16, train loss: 0.5409541130065918\n",
      " - Batch no: 24, train loss: 0.6046337485313416\n",
      "-Validation loss after epoc: 0.5475340485572815 \n",
      "Epoch:91 \n",
      " - Batch no: 0, train loss: 0.6277413368225098\n",
      " - Batch no: 8, train loss: 0.5498167276382446\n",
      " - Batch no: 16, train loss: 0.779738187789917\n",
      " - Batch no: 24, train loss: 0.4234170913696289\n",
      "-Validation loss after epoc: 0.5472632050514221 \n",
      "Epoch:92 \n",
      " - Batch no: 0, train loss: 0.5008464455604553\n",
      " - Batch no: 8, train loss: 0.6461284756660461\n",
      " - Batch no: 16, train loss: 0.7667996883392334\n",
      " - Batch no: 24, train loss: 0.4512041211128235\n",
      "-Validation loss after epoc: 0.5485720038414001 \n",
      "Epoch:93 \n",
      " - Batch no: 0, train loss: 0.4627293646335602\n",
      " - Batch no: 8, train loss: 0.7329720854759216\n",
      " - Batch no: 16, train loss: 0.7118332982063293\n",
      " - Batch no: 24, train loss: 0.5584951043128967\n",
      "-Validation loss after epoc: 0.5469919443130493 \n",
      "Epoch:94 \n",
      " - Batch no: 0, train loss: 0.5997198224067688\n",
      " - Batch no: 8, train loss: 0.635931134223938\n",
      " - Batch no: 16, train loss: 0.6759417057037354\n",
      " - Batch no: 24, train loss: 0.565954327583313\n",
      "-Validation loss after epoc: 0.5460418462753296 \n",
      "Epoch:95 \n",
      " - Batch no: 0, train loss: 0.6954184174537659\n",
      " - Batch no: 8, train loss: 0.7747315168380737\n",
      " - Batch no: 16, train loss: 0.5642346739768982\n",
      " - Batch no: 24, train loss: 0.7149327993392944\n",
      "-Validation loss after epoc: 0.5454203486442566 \n",
      "Epoch:96 \n",
      " - Batch no: 0, train loss: 0.5320027470588684\n",
      " - Batch no: 8, train loss: 0.3713891804218292\n",
      " - Batch no: 16, train loss: 0.5170086622238159\n",
      " - Batch no: 24, train loss: 0.6465840339660645\n",
      "-Validation loss after epoc: 0.5454409122467041 \n",
      "Epoch:97 \n",
      " - Batch no: 0, train loss: 0.5106169581413269\n",
      " - Batch no: 8, train loss: 0.6204451322555542\n",
      " - Batch no: 16, train loss: 0.6431394815444946\n",
      " - Batch no: 24, train loss: 0.5615411996841431\n",
      "-Validation loss after epoc: 0.5449787974357605 \n",
      "Epoch:98 \n",
      " - Batch no: 0, train loss: 0.46050792932510376\n",
      " - Batch no: 8, train loss: 0.5098894238471985\n",
      " - Batch no: 16, train loss: 0.6032679677009583\n",
      " - Batch no: 24, train loss: 0.7766874432563782\n",
      "-Validation loss after epoc: 0.5442712903022766 \n",
      "Epoch:99 \n",
      " - Batch no: 0, train loss: 0.5318468809127808\n",
      " - Batch no: 8, train loss: 0.5154646039009094\n",
      " - Batch no: 16, train loss: 0.6799570322036743\n",
      " - Batch no: 24, train loss: 0.7823443412780762\n",
      "-Validation loss after epoc: 0.5441246032714844 \n",
      "Epoch:100 \n",
      " - Batch no: 0, train loss: 0.6571606397628784\n",
      " - Batch no: 8, train loss: 0.4326260983943939\n",
      " - Batch no: 16, train loss: 0.7810326814651489\n",
      " - Batch no: 24, train loss: 0.6183627843856812\n",
      "-Validation loss after epoc: 0.5441759824752808 \n",
      "Epoch:101 \n",
      " - Batch no: 0, train loss: 0.49149876832962036\n",
      " - Batch no: 8, train loss: 0.5877036452293396\n",
      " - Batch no: 16, train loss: 0.626330554485321\n",
      " - Batch no: 24, train loss: 0.458724707365036\n",
      "-Validation loss after epoc: 0.5429695844650269 \n",
      "Epoch:102 \n",
      " - Batch no: 0, train loss: 0.676539957523346\n",
      " - Batch no: 8, train loss: 0.5826090574264526\n",
      " - Batch no: 16, train loss: 0.6504504680633545\n",
      " - Batch no: 24, train loss: 0.5056766271591187\n",
      "-Validation loss after epoc: 0.5417711138725281 \n",
      "Epoch:103 \n",
      " - Batch no: 0, train loss: 0.41858160495758057\n",
      " - Batch no: 8, train loss: 0.5614291429519653\n",
      " - Batch no: 16, train loss: 0.5612479448318481\n",
      " - Batch no: 24, train loss: 0.5190994739532471\n",
      "-Validation loss after epoc: 0.5425394773483276 \n",
      "Epoch:104 \n",
      " - Batch no: 0, train loss: 0.7616869211196899\n",
      " - Batch no: 8, train loss: 0.6792382597923279\n",
      " - Batch no: 16, train loss: 0.6700197458267212\n",
      " - Batch no: 24, train loss: 0.4902741611003876\n",
      "-Validation loss after epoc: 0.541703999042511 \n",
      "Epoch:105 \n",
      " - Batch no: 0, train loss: 0.438896119594574\n",
      " - Batch no: 8, train loss: 0.6259034872055054\n",
      " - Batch no: 16, train loss: 0.6265747547149658\n",
      " - Batch no: 24, train loss: 0.648158073425293\n",
      "-Validation loss after epoc: 0.5427195429801941 \n",
      "Epoch:106 \n",
      " - Batch no: 0, train loss: 0.531119704246521\n",
      " - Batch no: 8, train loss: 0.5990312695503235\n",
      " - Batch no: 16, train loss: 0.9352017641067505\n",
      " - Batch no: 24, train loss: 0.5368161201477051\n",
      "-Validation loss after epoc: 0.5411250591278076 \n",
      "Epoch:107 \n",
      " - Batch no: 0, train loss: 0.6725710034370422\n",
      " - Batch no: 8, train loss: 0.7751438617706299\n",
      " - Batch no: 16, train loss: 0.5304397940635681\n",
      " - Batch no: 24, train loss: 0.4432012140750885\n",
      "-Validation loss after epoc: 0.541735827922821 \n",
      "Epoch:108 \n",
      " - Batch no: 0, train loss: 0.6284377574920654\n",
      " - Batch no: 8, train loss: 0.6293889284133911\n",
      " - Batch no: 16, train loss: 0.45497480034828186\n",
      " - Batch no: 24, train loss: 0.5371362566947937\n",
      "-Validation loss after epoc: 0.5404627323150635 \n",
      "Epoch:109 \n",
      " - Batch no: 0, train loss: 0.5639379024505615\n",
      " - Batch no: 8, train loss: 0.6509785652160645\n",
      " - Batch no: 16, train loss: 0.7872406244277954\n",
      " - Batch no: 24, train loss: 0.601672351360321\n",
      "-Validation loss after epoc: 0.5410993695259094 \n",
      "Epoch:110 \n",
      " - Batch no: 0, train loss: 0.709324836730957\n",
      " - Batch no: 8, train loss: 0.5669312477111816\n",
      " - Batch no: 16, train loss: 0.5559727549552917\n",
      " - Batch no: 24, train loss: 0.6416333913803101\n",
      "-Validation loss after epoc: 0.5408921837806702 \n",
      "Epoch:111 \n",
      " - Batch no: 0, train loss: 0.6426764130592346\n",
      " - Batch no: 8, train loss: 0.7480912804603577\n",
      " - Batch no: 16, train loss: 0.44804704189300537\n",
      " - Batch no: 24, train loss: 0.6406911611557007\n",
      "-Validation loss after epoc: 0.5410317182540894 \n",
      "Epoch:112 \n",
      " - Batch no: 0, train loss: 0.6032760739326477\n",
      " - Batch no: 8, train loss: 0.8100948929786682\n",
      " - Batch no: 16, train loss: 0.6056598424911499\n",
      " - Batch no: 24, train loss: 0.599993884563446\n",
      "-Validation loss after epoc: 0.539941668510437 \n",
      "Epoch:113 \n",
      " - Batch no: 0, train loss: 0.5018302202224731\n",
      " - Batch no: 8, train loss: 0.7875270247459412\n",
      " - Batch no: 16, train loss: 0.6112117171287537\n",
      " - Batch no: 24, train loss: 0.5586264133453369\n",
      "-Validation loss after epoc: 0.5407571792602539 \n",
      "Epoch:114 \n",
      " - Batch no: 0, train loss: 0.5034903883934021\n",
      " - Batch no: 8, train loss: 0.6179301142692566\n",
      " - Batch no: 16, train loss: 0.5315353274345398\n",
      " - Batch no: 24, train loss: 0.737310528755188\n",
      "-Validation loss after epoc: 0.5408148765563965 \n",
      "Epoch:115 \n",
      " - Batch no: 0, train loss: 0.6918872594833374\n",
      " - Batch no: 8, train loss: 0.4900255501270294\n",
      " - Batch no: 16, train loss: 0.6675871014595032\n",
      " - Batch no: 24, train loss: 0.6372916102409363\n",
      "-Validation loss after epoc: 0.5417001843452454 \n",
      "Epoch:116 \n",
      " - Batch no: 0, train loss: 0.598716676235199\n",
      " - Batch no: 8, train loss: 0.7021865844726562\n",
      " - Batch no: 16, train loss: 0.5187532305717468\n",
      " - Batch no: 24, train loss: 0.46759411692619324\n",
      "-Validation loss after epoc: 0.5448696613311768 \n",
      "Epoch:117 \n",
      " - Batch no: 0, train loss: 0.6213498115539551\n",
      " - Batch no: 8, train loss: 0.5461836457252502\n",
      " - Batch no: 16, train loss: 0.5798097252845764\n",
      " - Batch no: 24, train loss: 0.5144013166427612\n",
      "-Validation loss after epoc: 0.5407085418701172 \n",
      "Epoch:118 \n",
      " - Batch no: 0, train loss: 0.48766717314720154\n",
      " - Batch no: 8, train loss: 0.5098723769187927\n",
      " - Batch no: 16, train loss: 0.5755234360694885\n",
      " - Batch no: 24, train loss: 0.6053181290626526\n",
      "-Validation loss after epoc: 0.5394901633262634 \n",
      "Epoch:119 \n",
      " - Batch no: 0, train loss: 0.9613156914710999\n",
      " - Batch no: 8, train loss: 0.48096612095832825\n",
      " - Batch no: 16, train loss: 0.647735595703125\n",
      " - Batch no: 24, train loss: 0.6708749532699585\n",
      "-Validation loss after epoc: 0.5401114821434021 \n",
      "Epoch:120 \n",
      " - Batch no: 0, train loss: 0.5193731784820557\n",
      " - Batch no: 8, train loss: 0.5844470858573914\n",
      " - Batch no: 16, train loss: 0.5812700390815735\n",
      " - Batch no: 24, train loss: 0.47685202956199646\n",
      "-Validation loss after epoc: 0.5391249656677246 \n",
      "Epoch:121 \n",
      " - Batch no: 0, train loss: 0.5381397008895874\n",
      " - Batch no: 8, train loss: 0.6517460346221924\n",
      " - Batch no: 16, train loss: 0.5223958492279053\n",
      " - Batch no: 24, train loss: 0.5321798920631409\n",
      "-Validation loss after epoc: 0.5388197898864746 \n",
      "Epoch:122 \n",
      " - Batch no: 0, train loss: 0.44822800159454346\n",
      " - Batch no: 8, train loss: 0.533828616142273\n",
      " - Batch no: 16, train loss: 0.6775650978088379\n",
      " - Batch no: 24, train loss: 0.83537358045578\n",
      "-Validation loss after epoc: 0.5388778448104858 \n",
      "Epoch:123 \n",
      " - Batch no: 0, train loss: 0.7094169855117798\n",
      " - Batch no: 8, train loss: 0.6881891489028931\n",
      " - Batch no: 16, train loss: 0.49664127826690674\n",
      " - Batch no: 24, train loss: 0.5731551647186279\n",
      "-Validation loss after epoc: 0.5386362671852112 \n",
      "Epoch:124 \n",
      " - Batch no: 0, train loss: 0.5588400959968567\n",
      " - Batch no: 8, train loss: 0.526454746723175\n",
      " - Batch no: 16, train loss: 0.5478401184082031\n",
      " - Batch no: 24, train loss: 0.8212513327598572\n",
      "-Validation loss after epoc: 0.5386988520622253 \n",
      "Epoch:125 \n",
      " - Batch no: 0, train loss: 0.6940602660179138\n",
      " - Batch no: 8, train loss: 0.5642096996307373\n",
      " - Batch no: 16, train loss: 0.5172488689422607\n",
      " - Batch no: 24, train loss: 0.6746707558631897\n",
      "-Validation loss after epoc: 0.5381282567977905 \n",
      "Epoch:126 \n",
      " - Batch no: 0, train loss: 0.6098639369010925\n",
      " - Batch no: 8, train loss: 0.485106885433197\n",
      " - Batch no: 16, train loss: 0.6771586537361145\n",
      " - Batch no: 24, train loss: 0.4801166355609894\n",
      "-Validation loss after epoc: 0.5379127860069275 \n",
      "Epoch:127 \n",
      " - Batch no: 0, train loss: 0.6809753775596619\n",
      " - Batch no: 8, train loss: 0.506855309009552\n",
      " - Batch no: 16, train loss: 0.6600819826126099\n",
      " - Batch no: 24, train loss: 0.619085431098938\n",
      "-Validation loss after epoc: 0.5378957986831665 \n",
      "Epoch:128 \n",
      " - Batch no: 0, train loss: 0.5854876041412354\n",
      " - Batch no: 8, train loss: 0.724044680595398\n",
      " - Batch no: 16, train loss: 0.651602566242218\n",
      " - Batch no: 24, train loss: 0.5300710201263428\n",
      "-Validation loss after epoc: 0.5380857586860657 \n",
      "Epoch:129 \n",
      " - Batch no: 0, train loss: 0.7129696607589722\n",
      " - Batch no: 8, train loss: 0.525993824005127\n",
      " - Batch no: 16, train loss: 0.518924355506897\n",
      " - Batch no: 24, train loss: 0.5512012839317322\n",
      "-Validation loss after epoc: 0.5378679037094116 \n",
      "Epoch:130 \n",
      " - Batch no: 0, train loss: 0.6608232855796814\n",
      " - Batch no: 8, train loss: 0.5424591302871704\n",
      " - Batch no: 16, train loss: 0.7784318923950195\n",
      " - Batch no: 24, train loss: 0.5463396906852722\n",
      "-Validation loss after epoc: 0.5377888083457947 \n",
      "Epoch:131 \n",
      " - Batch no: 0, train loss: 0.8182552456855774\n",
      " - Batch no: 8, train loss: 0.713554859161377\n",
      " - Batch no: 16, train loss: 0.4502367079257965\n",
      " - Batch no: 24, train loss: 0.5803724527359009\n",
      "-Validation loss after epoc: 0.5387399792671204 \n",
      "Epoch:132 \n",
      " - Batch no: 0, train loss: 0.6961560249328613\n",
      " - Batch no: 8, train loss: 0.4545923173427582\n",
      " - Batch no: 16, train loss: 0.554947555065155\n",
      " - Batch no: 24, train loss: 0.636286199092865\n",
      "-Validation loss after epoc: 0.5366504192352295 \n",
      "Epoch:133 \n",
      " - Batch no: 0, train loss: 0.9170708656311035\n",
      " - Batch no: 8, train loss: 0.6613401770591736\n",
      " - Batch no: 16, train loss: 0.511180579662323\n",
      " - Batch no: 24, train loss: 0.6277350187301636\n",
      "-Validation loss after epoc: 0.537505030632019 \n",
      "Epoch:134 \n",
      " - Batch no: 0, train loss: 0.4837378263473511\n",
      " - Batch no: 8, train loss: 0.5039710998535156\n",
      " - Batch no: 16, train loss: 0.5006749033927917\n",
      " - Batch no: 24, train loss: 0.45070162415504456\n",
      "-Validation loss after epoc: 0.5369094610214233 \n",
      "Epoch:135 \n",
      " - Batch no: 0, train loss: 0.5316504836082458\n",
      " - Batch no: 8, train loss: 0.60893315076828\n",
      " - Batch no: 16, train loss: 0.9155829548835754\n",
      " - Batch no: 24, train loss: 0.7183473706245422\n",
      "-Validation loss after epoc: 0.5368357300758362 \n",
      "Epoch:136 \n",
      " - Batch no: 0, train loss: 0.577171802520752\n",
      " - Batch no: 8, train loss: 0.5393275022506714\n",
      " - Batch no: 16, train loss: 0.4769638776779175\n",
      " - Batch no: 24, train loss: 0.533882200717926\n",
      "-Validation loss after epoc: 0.5365375280380249 \n",
      "Epoch:137 \n",
      " - Batch no: 0, train loss: 0.6348751187324524\n",
      " - Batch no: 8, train loss: 0.5111384987831116\n",
      " - Batch no: 16, train loss: 0.6641970276832581\n",
      " - Batch no: 24, train loss: 0.48332083225250244\n",
      "-Validation loss after epoc: 0.5370939373970032 \n",
      "Epoch:138 \n",
      " - Batch no: 0, train loss: 0.5672935843467712\n",
      " - Batch no: 8, train loss: 0.614937424659729\n",
      " - Batch no: 16, train loss: 0.47392594814300537\n",
      " - Batch no: 24, train loss: 0.5743979215621948\n",
      "-Validation loss after epoc: 0.5365443825721741 \n",
      "Epoch:139 \n",
      " - Batch no: 0, train loss: 0.5266335010528564\n",
      " - Batch no: 8, train loss: 0.6133764386177063\n",
      " - Batch no: 16, train loss: 0.6842169761657715\n",
      " - Batch no: 24, train loss: 0.9059889912605286\n",
      "-Validation loss after epoc: 0.535703182220459 \n",
      "Epoch:140 \n",
      " - Batch no: 0, train loss: 0.7129972577095032\n",
      " - Batch no: 8, train loss: 0.48064330220222473\n",
      " - Batch no: 16, train loss: 0.7760277390480042\n",
      " - Batch no: 24, train loss: 0.7353194952011108\n",
      "-Validation loss after epoc: 0.5355557203292847 \n",
      "Epoch:141 \n",
      " - Batch no: 0, train loss: 0.6072553992271423\n",
      " - Batch no: 8, train loss: 0.7471744418144226\n",
      " - Batch no: 16, train loss: 0.6804426908493042\n",
      " - Batch no: 24, train loss: 0.5157763957977295\n",
      "-Validation loss after epoc: 0.5355660319328308 \n",
      "Epoch:142 \n",
      " - Batch no: 0, train loss: 0.7160600423812866\n",
      " - Batch no: 8, train loss: 0.42275115847587585\n",
      " - Batch no: 16, train loss: 0.7151626348495483\n",
      " - Batch no: 24, train loss: 0.5103519558906555\n",
      "-Validation loss after epoc: 0.5351902842521667 \n",
      "Epoch:143 \n",
      " - Batch no: 0, train loss: 0.8154441714286804\n",
      " - Batch no: 8, train loss: 0.5790108442306519\n",
      " - Batch no: 16, train loss: 0.4528607726097107\n",
      " - Batch no: 24, train loss: 0.6429893970489502\n",
      "-Validation loss after epoc: 0.5342680215835571 \n",
      "Epoch:144 \n",
      " - Batch no: 0, train loss: 0.6276555061340332\n",
      " - Batch no: 8, train loss: 0.7546234130859375\n",
      " - Batch no: 16, train loss: 0.6615791916847229\n",
      " - Batch no: 24, train loss: 0.5131255388259888\n",
      "-Validation loss after epoc: 0.533739447593689 \n",
      "Epoch:145 \n",
      " - Batch no: 0, train loss: 0.4821348488330841\n",
      " - Batch no: 8, train loss: 0.5997481346130371\n",
      " - Batch no: 16, train loss: 0.5563002824783325\n",
      " - Batch no: 24, train loss: 0.4576994478702545\n",
      "-Validation loss after epoc: 0.5330685973167419 \n",
      "Epoch:146 \n",
      " - Batch no: 0, train loss: 0.5971589684486389\n",
      " - Batch no: 8, train loss: 0.6990985870361328\n",
      " - Batch no: 16, train loss: 0.5941002368927002\n",
      " - Batch no: 24, train loss: 0.4549086093902588\n",
      "-Validation loss after epoc: 0.5334776043891907 \n",
      "Epoch:147 \n",
      " - Batch no: 0, train loss: 0.5140306353569031\n",
      " - Batch no: 8, train loss: 0.6645193696022034\n",
      " - Batch no: 16, train loss: 0.7567393183708191\n",
      " - Batch no: 24, train loss: 0.5659310221672058\n",
      "-Validation loss after epoc: 0.5328698754310608 \n",
      "Epoch:148 \n",
      " - Batch no: 0, train loss: 0.5481759309768677\n",
      " - Batch no: 8, train loss: 0.5975680947303772\n",
      " - Batch no: 16, train loss: 0.7095597982406616\n",
      " - Batch no: 24, train loss: 0.4639338552951813\n",
      "-Validation loss after epoc: 0.5335202217102051 \n",
      "Epoch:149 \n",
      " - Batch no: 0, train loss: 0.6807947754859924\n",
      " - Batch no: 8, train loss: 0.562018096446991\n",
      " - Batch no: 16, train loss: 0.4472692906856537\n",
      " - Batch no: 24, train loss: 0.677317202091217\n",
      "-Validation loss after epoc: 0.5331416726112366 \n",
      "Epoch:150 \n",
      " - Batch no: 0, train loss: 0.527984619140625\n",
      " - Batch no: 8, train loss: 0.5515932440757751\n",
      " - Batch no: 16, train loss: 0.4825461804866791\n",
      " - Batch no: 24, train loss: 0.5806167125701904\n",
      "-Validation loss after epoc: 0.5324777364730835 \n",
      "Epoch:151 \n",
      " - Batch no: 0, train loss: 0.4513097405433655\n",
      " - Batch no: 8, train loss: 0.6862677931785583\n",
      " - Batch no: 16, train loss: 0.5736145377159119\n",
      " - Batch no: 24, train loss: 0.5087090730667114\n",
      "-Validation loss after epoc: 0.5325294733047485 \n",
      "Epoch:152 \n",
      " - Batch no: 0, train loss: 0.633899986743927\n",
      " - Batch no: 8, train loss: 0.8891758918762207\n",
      " - Batch no: 16, train loss: 0.39077383279800415\n",
      " - Batch no: 24, train loss: 0.6013339757919312\n",
      "-Validation loss after epoc: 0.5329890251159668 \n",
      "Epoch:153 \n",
      " - Batch no: 0, train loss: 0.5700737833976746\n",
      " - Batch no: 8, train loss: 0.5264797806739807\n",
      " - Batch no: 16, train loss: 0.6885802745819092\n",
      " - Batch no: 24, train loss: 0.6637711524963379\n",
      "-Validation loss after epoc: 0.5319414138793945 \n",
      "Epoch:154 \n",
      " - Batch no: 0, train loss: 0.5174170136451721\n",
      " - Batch no: 8, train loss: 0.5702176690101624\n",
      " - Batch no: 16, train loss: 0.8622791767120361\n",
      " - Batch no: 24, train loss: 0.5964846014976501\n",
      "-Validation loss after epoc: 0.5325253009796143 \n",
      "Epoch:155 \n",
      " - Batch no: 0, train loss: 0.59664386510849\n",
      " - Batch no: 8, train loss: 0.43048009276390076\n",
      " - Batch no: 16, train loss: 0.47580793499946594\n",
      " - Batch no: 24, train loss: 0.7060883045196533\n",
      "-Validation loss after epoc: 0.5335713028907776 \n",
      "Epoch:156 \n",
      " - Batch no: 0, train loss: 0.590104341506958\n",
      " - Batch no: 8, train loss: 0.6587973237037659\n",
      " - Batch no: 16, train loss: 0.5939996838569641\n",
      " - Batch no: 24, train loss: 0.5577312707901001\n",
      "-Validation loss after epoc: 0.5320054888725281 \n",
      "Epoch:157 \n",
      " - Batch no: 0, train loss: 0.593245267868042\n",
      " - Batch no: 8, train loss: 0.652920126914978\n",
      " - Batch no: 16, train loss: 0.57786625623703\n",
      " - Batch no: 24, train loss: 0.5260120630264282\n",
      "-Validation loss after epoc: 0.5321480631828308 \n",
      "Epoch:158 \n",
      " - Batch no: 0, train loss: 0.4762413799762726\n",
      " - Batch no: 8, train loss: 0.6535854935646057\n",
      " - Batch no: 16, train loss: 0.6991018652915955\n",
      " - Batch no: 24, train loss: 0.49598070979118347\n",
      "-Validation loss after epoc: 0.5319622755050659 \n",
      "Epoch:159 \n",
      " - Batch no: 0, train loss: 0.5946875810623169\n",
      " - Batch no: 8, train loss: 0.5860570669174194\n",
      " - Batch no: 16, train loss: 0.6643256545066833\n",
      " - Batch no: 24, train loss: 0.4401533603668213\n",
      "-Validation loss after epoc: 0.5326827764511108 \n",
      "Epoch:160 \n",
      " - Batch no: 0, train loss: 0.7022399306297302\n",
      " - Batch no: 8, train loss: 0.5938360691070557\n",
      " - Batch no: 16, train loss: 0.6697192192077637\n",
      " - Batch no: 24, train loss: 0.7389224767684937\n",
      "-Validation loss after epoc: 0.5303636193275452 \n",
      "Epoch:161 \n",
      " - Batch no: 0, train loss: 0.5461575984954834\n",
      " - Batch no: 8, train loss: 0.5766065716743469\n",
      " - Batch no: 16, train loss: 0.595129132270813\n",
      " - Batch no: 24, train loss: 0.4798973798751831\n",
      "-Validation loss after epoc: 0.5306992530822754 \n",
      "Epoch:162 \n",
      " - Batch no: 0, train loss: 0.754135251045227\n",
      " - Batch no: 8, train loss: 0.6106328368186951\n",
      " - Batch no: 16, train loss: 0.4053891897201538\n",
      " - Batch no: 24, train loss: 0.6467355489730835\n",
      "-Validation loss after epoc: 0.5308848023414612 \n",
      "Epoch:163 \n",
      " - Batch no: 0, train loss: 0.5857967734336853\n",
      " - Batch no: 8, train loss: 0.6662529110908508\n",
      " - Batch no: 16, train loss: 0.5543813705444336\n",
      " - Batch no: 24, train loss: 0.5815444588661194\n",
      "-Validation loss after epoc: 0.5309655070304871 \n",
      "Epoch:164 \n",
      " - Batch no: 0, train loss: 0.6834800243377686\n",
      " - Batch no: 8, train loss: 0.5102768540382385\n",
      " - Batch no: 16, train loss: 0.7346311807632446\n",
      " - Batch no: 24, train loss: 0.43568217754364014\n",
      "-Validation loss after epoc: 0.5298502445220947 \n",
      "Epoch:165 \n",
      " - Batch no: 0, train loss: 0.5495396256446838\n",
      " - Batch no: 8, train loss: 0.6119316220283508\n",
      " - Batch no: 16, train loss: 0.5881044268608093\n",
      " - Batch no: 24, train loss: 0.5011459589004517\n",
      "-Validation loss after epoc: 0.5293645262718201 \n",
      "Epoch:166 \n",
      " - Batch no: 0, train loss: 0.502094030380249\n",
      " - Batch no: 8, train loss: 0.752417266368866\n",
      " - Batch no: 16, train loss: 0.658126950263977\n",
      " - Batch no: 24, train loss: 0.6767730712890625\n",
      "-Validation loss after epoc: 0.5307703614234924 \n",
      "Epoch:167 \n",
      " - Batch no: 0, train loss: 0.5859060287475586\n",
      " - Batch no: 8, train loss: 0.4865529537200928\n",
      " - Batch no: 16, train loss: 0.651974081993103\n",
      " - Batch no: 24, train loss: 0.8161651492118835\n",
      "-Validation loss after epoc: 0.5294482707977295 \n",
      "Epoch:168 \n",
      " - Batch no: 0, train loss: 0.6235786080360413\n",
      " - Batch no: 8, train loss: 0.6375012993812561\n",
      " - Batch no: 16, train loss: 0.6467286348342896\n",
      " - Batch no: 24, train loss: 0.53923499584198\n",
      "-Validation loss after epoc: 0.5294182896614075 \n",
      "Epoch:169 \n",
      " - Batch no: 0, train loss: 0.8471733331680298\n",
      " - Batch no: 8, train loss: 0.4986204504966736\n",
      " - Batch no: 16, train loss: 0.6860140562057495\n",
      " - Batch no: 24, train loss: 0.7356733083724976\n",
      "-Validation loss after epoc: 0.529399037361145 \n",
      "Epoch:170 \n",
      " - Batch no: 0, train loss: 0.614497721195221\n",
      " - Batch no: 8, train loss: 0.649668276309967\n",
      " - Batch no: 16, train loss: 0.613801121711731\n",
      " - Batch no: 24, train loss: 0.5073874592781067\n",
      "-Validation loss after epoc: 0.5292414426803589 \n",
      "Epoch:171 \n",
      " - Batch no: 0, train loss: 0.595198929309845\n",
      " - Batch no: 8, train loss: 0.5191759467124939\n",
      " - Batch no: 16, train loss: 0.5030410885810852\n",
      " - Batch no: 24, train loss: 0.6817918419837952\n",
      "-Validation loss after epoc: 0.5290830135345459 \n",
      "Epoch:172 \n",
      " - Batch no: 0, train loss: 0.6807265877723694\n",
      " - Batch no: 8, train loss: 0.526813268661499\n",
      " - Batch no: 16, train loss: 0.5222406983375549\n",
      " - Batch no: 24, train loss: 0.4646378457546234\n",
      "-Validation loss after epoc: 0.5289819240570068 \n",
      "Epoch:173 \n",
      " - Batch no: 0, train loss: 0.7565491795539856\n",
      " - Batch no: 8, train loss: 0.5329446196556091\n",
      " - Batch no: 16, train loss: 0.5870250463485718\n",
      " - Batch no: 24, train loss: 0.47535285353660583\n",
      "-Validation loss after epoc: 0.5289718508720398 \n",
      "Epoch:174 \n",
      " - Batch no: 0, train loss: 0.6752523183822632\n",
      " - Batch no: 8, train loss: 0.6834220886230469\n",
      " - Batch no: 16, train loss: 0.535391628742218\n",
      " - Batch no: 24, train loss: 0.5771368145942688\n",
      "-Validation loss after epoc: 0.5294798016548157 \n",
      "Epoch:175 \n",
      " - Batch no: 0, train loss: 0.6177017688751221\n",
      " - Batch no: 8, train loss: 0.6787893176078796\n",
      " - Batch no: 16, train loss: 0.4941006600856781\n",
      " - Batch no: 24, train loss: 0.7978914380073547\n",
      "-Validation loss after epoc: 0.530474841594696 \n",
      "Epoch:176 \n",
      " - Batch no: 0, train loss: 0.7175499200820923\n",
      " - Batch no: 8, train loss: 0.6375786662101746\n",
      " - Batch no: 16, train loss: 0.5549432039260864\n",
      " - Batch no: 24, train loss: 0.5057910680770874\n",
      "-Validation loss after epoc: 0.5295934081077576 \n",
      "Epoch:177 \n",
      " - Batch no: 0, train loss: 0.4943183660507202\n",
      " - Batch no: 8, train loss: 0.6278907656669617\n",
      " - Batch no: 16, train loss: 0.621238648891449\n",
      " - Batch no: 24, train loss: 0.5959464311599731\n",
      "-Validation loss after epoc: 0.5291780233383179 \n",
      "Epoch:178 \n",
      " - Batch no: 0, train loss: 0.5581392645835876\n",
      " - Batch no: 8, train loss: 0.6536195278167725\n",
      " - Batch no: 16, train loss: 0.6322963833808899\n",
      " - Batch no: 24, train loss: 0.6017585396766663\n",
      "-Validation loss after epoc: 0.529227077960968 \n",
      "Epoch:179 \n",
      " - Batch no: 0, train loss: 0.522125780582428\n",
      " - Batch no: 8, train loss: 0.7366188764572144\n",
      " - Batch no: 16, train loss: 0.46294552087783813\n",
      " - Batch no: 24, train loss: 0.6798360347747803\n",
      "-Validation loss after epoc: 0.5282279253005981 \n",
      "Epoch:180 \n",
      " - Batch no: 0, train loss: 0.5538477301597595\n",
      " - Batch no: 8, train loss: 0.6230458617210388\n",
      " - Batch no: 16, train loss: 0.7174301147460938\n",
      " - Batch no: 24, train loss: 0.6258700489997864\n",
      "-Validation loss after epoc: 0.5275242924690247 \n",
      "Epoch:181 \n",
      " - Batch no: 0, train loss: 0.5055224299430847\n",
      " - Batch no: 8, train loss: 0.5602371692657471\n",
      " - Batch no: 16, train loss: 0.49960511922836304\n",
      " - Batch no: 24, train loss: 0.7802683711051941\n",
      "-Validation loss after epoc: 0.5277125239372253 \n",
      "Epoch:182 \n",
      " - Batch no: 0, train loss: 0.4775455892086029\n",
      " - Batch no: 8, train loss: 0.47446778416633606\n",
      " - Batch no: 16, train loss: 0.6036370396614075\n",
      " - Batch no: 24, train loss: 0.5864048600196838\n",
      "-Validation loss after epoc: 0.5275232195854187 \n",
      "Epoch:183 \n",
      " - Batch no: 0, train loss: 0.6544123291969299\n",
      " - Batch no: 8, train loss: 0.6646679043769836\n",
      " - Batch no: 16, train loss: 0.6214810609817505\n",
      " - Batch no: 24, train loss: 0.577948808670044\n",
      "-Validation loss after epoc: 0.5275407433509827 \n",
      "Epoch:184 \n",
      " - Batch no: 0, train loss: 0.46866703033447266\n",
      " - Batch no: 8, train loss: 0.7216061949729919\n",
      " - Batch no: 16, train loss: 0.517249345779419\n",
      " - Batch no: 24, train loss: 0.587772786617279\n",
      "-Validation loss after epoc: 0.5274465084075928 \n",
      "Epoch:185 \n",
      " - Batch no: 0, train loss: 0.620995819568634\n",
      " - Batch no: 8, train loss: 0.684417724609375\n",
      " - Batch no: 16, train loss: 0.625053882598877\n",
      " - Batch no: 24, train loss: 0.6567767858505249\n",
      "-Validation loss after epoc: 0.5265764594078064 \n",
      "Epoch:186 \n",
      " - Batch no: 0, train loss: 0.6510552167892456\n",
      " - Batch no: 8, train loss: 0.5805894732475281\n",
      " - Batch no: 16, train loss: 0.6713599562644958\n",
      " - Batch no: 24, train loss: 0.4847519099712372\n",
      "-Validation loss after epoc: 0.5268465876579285 \n",
      "Epoch:187 \n",
      " - Batch no: 0, train loss: 0.7047170400619507\n",
      " - Batch no: 8, train loss: 0.7117657661437988\n",
      " - Batch no: 16, train loss: 0.5802385210990906\n",
      " - Batch no: 24, train loss: 0.8225282430648804\n",
      "-Validation loss after epoc: 0.526944637298584 \n",
      "Epoch:188 \n",
      " - Batch no: 0, train loss: 0.5544848442077637\n",
      " - Batch no: 8, train loss: 0.5601838827133179\n",
      " - Batch no: 16, train loss: 0.4937012791633606\n",
      " - Batch no: 24, train loss: 0.6313455104827881\n",
      "-Validation loss after epoc: 0.5274765491485596 \n",
      "Epoch:189 \n",
      " - Batch no: 0, train loss: 0.6064716577529907\n",
      " - Batch no: 8, train loss: 0.703566312789917\n",
      " - Batch no: 16, train loss: 0.410177081823349\n",
      " - Batch no: 24, train loss: 0.6572697162628174\n",
      "-Validation loss after epoc: 0.52629554271698 \n",
      "Epoch:190 \n",
      " - Batch no: 0, train loss: 0.5876123905181885\n",
      " - Batch no: 8, train loss: 0.5220872759819031\n",
      " - Batch no: 16, train loss: 0.5737771391868591\n",
      " - Batch no: 24, train loss: 0.47712576389312744\n",
      "-Validation loss after epoc: 0.5271831154823303 \n",
      "Epoch:191 \n",
      " - Batch no: 0, train loss: 0.7113078832626343\n",
      " - Batch no: 8, train loss: 0.5714284777641296\n",
      " - Batch no: 16, train loss: 0.5609267950057983\n",
      " - Batch no: 24, train loss: 0.5264016389846802\n",
      "-Validation loss after epoc: 0.5266751050949097 \n",
      "Epoch:192 \n",
      " - Batch no: 0, train loss: 0.5260213613510132\n",
      " - Batch no: 8, train loss: 0.6601787805557251\n",
      " - Batch no: 16, train loss: 0.418055921792984\n",
      " - Batch no: 24, train loss: 0.9182909727096558\n",
      "-Validation loss after epoc: 0.5263665914535522 \n",
      "Epoch:193 \n",
      " - Batch no: 0, train loss: 0.4661446809768677\n",
      " - Batch no: 8, train loss: 0.5233215689659119\n",
      " - Batch no: 16, train loss: 0.535078763961792\n",
      " - Batch no: 24, train loss: 0.8099830150604248\n",
      "-Validation loss after epoc: 0.5268481969833374 \n",
      "Epoch:194 \n",
      " - Batch no: 0, train loss: 0.49831604957580566\n",
      " - Batch no: 8, train loss: 0.5612119436264038\n",
      " - Batch no: 16, train loss: 0.7240266799926758\n",
      " - Batch no: 24, train loss: 0.6831092238426208\n",
      "-Validation loss after epoc: 0.5253296494483948 \n",
      "Epoch:195 \n",
      " - Batch no: 0, train loss: 0.6989391446113586\n",
      " - Batch no: 8, train loss: 0.7821570634841919\n",
      " - Batch no: 16, train loss: 0.5882567763328552\n",
      " - Batch no: 24, train loss: 0.4845885932445526\n",
      "-Validation loss after epoc: 0.5254659056663513 \n",
      "Epoch:196 \n",
      " - Batch no: 0, train loss: 0.5986049771308899\n",
      " - Batch no: 8, train loss: 0.38398313522338867\n",
      " - Batch no: 16, train loss: 0.49876323342323303\n",
      " - Batch no: 24, train loss: 0.4259631931781769\n",
      "-Validation loss after epoc: 0.5249499082565308 \n",
      "Epoch:197 \n",
      " - Batch no: 0, train loss: 0.6180539131164551\n",
      " - Batch no: 8, train loss: 0.6500075459480286\n",
      " - Batch no: 16, train loss: 0.5338146686553955\n",
      " - Batch no: 24, train loss: 0.5346585512161255\n",
      "-Validation loss after epoc: 0.5255886912345886 \n",
      "Epoch:198 \n",
      " - Batch no: 0, train loss: 0.585186779499054\n",
      " - Batch no: 8, train loss: 0.7621110677719116\n",
      " - Batch no: 16, train loss: 0.44147592782974243\n",
      " - Batch no: 24, train loss: 0.5989389419555664\n",
      "-Validation loss after epoc: 0.5253427624702454 \n",
      "Epoch:199 \n",
      " - Batch no: 0, train loss: 0.5517809987068176\n",
      " - Batch no: 8, train loss: 0.7317277789115906\n",
      " - Batch no: 16, train loss: 0.714772641658783\n",
      " - Batch no: 24, train loss: 0.48810362815856934\n",
      "-Validation loss after epoc: 0.5254802107810974 \n",
      "Epoch:200 \n",
      " - Batch no: 0, train loss: 0.5273491144180298\n",
      " - Batch no: 8, train loss: 0.487783282995224\n",
      " - Batch no: 16, train loss: 0.49559542536735535\n",
      " - Batch no: 24, train loss: 0.5747551918029785\n",
      "-Validation loss after epoc: 0.5248783826828003 \n",
      "Epoch:201 \n",
      " - Batch no: 0, train loss: 0.48289263248443604\n",
      " - Batch no: 8, train loss: 0.5681129097938538\n",
      " - Batch no: 16, train loss: 0.5111559629440308\n",
      " - Batch no: 24, train loss: 0.6987252831459045\n",
      "-Validation loss after epoc: 0.5247595906257629 \n",
      "Epoch:202 \n",
      " - Batch no: 0, train loss: 0.4820310175418854\n",
      " - Batch no: 8, train loss: 0.6728223562240601\n",
      " - Batch no: 16, train loss: 0.4488012492656708\n",
      " - Batch no: 24, train loss: 0.5218673944473267\n",
      "-Validation loss after epoc: 0.5248634815216064 \n",
      "Epoch:203 \n",
      " - Batch no: 0, train loss: 0.5485671162605286\n",
      " - Batch no: 8, train loss: 0.6764883399009705\n",
      " - Batch no: 16, train loss: 0.5158043503761292\n",
      " - Batch no: 24, train loss: 0.5306830406188965\n",
      "-Validation loss after epoc: 0.5246334075927734 \n",
      "Epoch:204 \n",
      " - Batch no: 0, train loss: 0.8010530471801758\n",
      " - Batch no: 8, train loss: 0.5660631656646729\n",
      " - Batch no: 16, train loss: 0.5397979021072388\n",
      " - Batch no: 24, train loss: 0.6093562841415405\n",
      "-Validation loss after epoc: 0.5245389342308044 \n",
      "Epoch:205 \n",
      " - Batch no: 0, train loss: 0.5741923451423645\n",
      " - Batch no: 8, train loss: 0.6012439131736755\n",
      " - Batch no: 16, train loss: 0.5116899609565735\n",
      " - Batch no: 24, train loss: 0.572985827922821\n",
      "-Validation loss after epoc: 0.5256174206733704 \n",
      "Epoch:206 \n",
      " - Batch no: 0, train loss: 0.5307073593139648\n",
      " - Batch no: 8, train loss: 0.6093273162841797\n",
      " - Batch no: 16, train loss: 0.5182555317878723\n",
      " - Batch no: 24, train loss: 0.4907858073711395\n",
      "-Validation loss after epoc: 0.5243531465530396 \n",
      "Epoch:207 \n",
      " - Batch no: 0, train loss: 0.5098692178726196\n",
      " - Batch no: 8, train loss: 0.5728381276130676\n",
      " - Batch no: 16, train loss: 0.5053380727767944\n",
      " - Batch no: 24, train loss: 0.5769926309585571\n",
      "-Validation loss after epoc: 0.5236617922782898 \n",
      "Epoch:208 \n",
      " - Batch no: 0, train loss: 0.5620311498641968\n",
      " - Batch no: 8, train loss: 0.5043182373046875\n",
      " - Batch no: 16, train loss: 0.7035151720046997\n",
      " - Batch no: 24, train loss: 0.5152104496955872\n",
      "-Validation loss after epoc: 0.5234289765357971 \n",
      "Epoch:209 \n",
      " - Batch no: 0, train loss: 0.5479211211204529\n",
      " - Batch no: 8, train loss: 0.569465696811676\n",
      " - Batch no: 16, train loss: 0.5682350397109985\n",
      " - Batch no: 24, train loss: 0.5269993543624878\n",
      "-Validation loss after epoc: 0.5246953964233398 \n",
      "Epoch:210 \n",
      " - Batch no: 0, train loss: 0.6785624623298645\n",
      " - Batch no: 8, train loss: 0.5187517404556274\n",
      " - Batch no: 16, train loss: 0.8662799596786499\n",
      " - Batch no: 24, train loss: 0.612494170665741\n",
      "-Validation loss after epoc: 0.5241031646728516 \n",
      "Epoch:211 \n",
      " - Batch no: 0, train loss: 0.4296223223209381\n",
      " - Batch no: 8, train loss: 0.4081358015537262\n",
      " - Batch no: 16, train loss: 0.5669496655464172\n",
      " - Batch no: 24, train loss: 0.5630030035972595\n",
      "-Validation loss after epoc: 0.523551881313324 \n",
      "Epoch:212 \n",
      " - Batch no: 0, train loss: 0.605055034160614\n",
      " - Batch no: 8, train loss: 0.7312646508216858\n",
      " - Batch no: 16, train loss: 0.4997880756855011\n",
      " - Batch no: 24, train loss: 0.5226595401763916\n",
      "-Validation loss after epoc: 0.5242934823036194 \n",
      "Epoch:213 \n",
      " - Batch no: 0, train loss: 0.7016417980194092\n",
      " - Batch no: 8, train loss: 0.7294073700904846\n",
      " - Batch no: 16, train loss: 0.6219967007637024\n",
      " - Batch no: 24, train loss: 0.7774470448493958\n",
      "-Validation loss after epoc: 0.5230634212493896 \n",
      "Epoch:214 \n",
      " - Batch no: 0, train loss: 0.5349816679954529\n",
      " - Batch no: 8, train loss: 0.48670414090156555\n",
      " - Batch no: 16, train loss: 0.5878062844276428\n",
      " - Batch no: 24, train loss: 0.49151933193206787\n",
      "-Validation loss after epoc: 0.5230702757835388 \n",
      "Epoch:215 \n",
      " - Batch no: 0, train loss: 0.5497673153877258\n",
      " - Batch no: 8, train loss: 0.7488594651222229\n",
      " - Batch no: 16, train loss: 0.6570447683334351\n",
      " - Batch no: 24, train loss: 0.7208839654922485\n",
      "-Validation loss after epoc: 0.5226890444755554 \n",
      "Epoch:216 \n",
      " - Batch no: 0, train loss: 0.45557382702827454\n",
      " - Batch no: 8, train loss: 0.5925692319869995\n",
      " - Batch no: 16, train loss: 0.8081257343292236\n",
      " - Batch no: 24, train loss: 0.4942796230316162\n",
      "-Validation loss after epoc: 0.5233250260353088 \n",
      "Epoch:217 \n",
      " - Batch no: 0, train loss: 0.6388276219367981\n",
      " - Batch no: 8, train loss: 0.5320072174072266\n",
      " - Batch no: 16, train loss: 0.7101867198944092\n",
      " - Batch no: 24, train loss: 0.6490392684936523\n",
      "-Validation loss after epoc: 0.5229896306991577 \n",
      "Epoch:218 \n",
      " - Batch no: 0, train loss: 0.6860411763191223\n",
      " - Batch no: 8, train loss: 0.4823462963104248\n",
      " - Batch no: 16, train loss: 0.7614861726760864\n",
      " - Batch no: 24, train loss: 0.5906088352203369\n",
      "-Validation loss after epoc: 0.523564875125885 \n",
      "Epoch:219 \n",
      " - Batch no: 0, train loss: 0.4633970260620117\n",
      " - Batch no: 8, train loss: 0.5493955016136169\n",
      " - Batch no: 16, train loss: 0.5350850224494934\n",
      " - Batch no: 24, train loss: 0.6496282815933228\n",
      "-Validation loss after epoc: 0.5222986340522766 \n",
      "Epoch:220 \n",
      " - Batch no: 0, train loss: 0.4875586926937103\n",
      " - Batch no: 8, train loss: 0.5150852799415588\n",
      " - Batch no: 16, train loss: 0.3476217985153198\n",
      " - Batch no: 24, train loss: 0.6799303889274597\n",
      "-Validation loss after epoc: 0.5221174955368042 \n",
      "Epoch:221 \n",
      " - Batch no: 0, train loss: 0.5687540173530579\n",
      " - Batch no: 8, train loss: 0.4393613338470459\n",
      " - Batch no: 16, train loss: 0.47417035698890686\n",
      " - Batch no: 24, train loss: 0.5862897634506226\n",
      "-Validation loss after epoc: 0.522649884223938 \n",
      "Epoch:222 \n",
      " - Batch no: 0, train loss: 0.6456615328788757\n",
      " - Batch no: 8, train loss: 0.5437837243080139\n",
      " - Batch no: 16, train loss: 0.6051164269447327\n",
      " - Batch no: 24, train loss: 0.4869800806045532\n",
      "-Validation loss after epoc: 0.5224173665046692 \n",
      "Epoch:223 \n",
      " - Batch no: 0, train loss: 0.6273819208145142\n",
      " - Batch no: 8, train loss: 0.4086834490299225\n",
      " - Batch no: 16, train loss: 0.6691247820854187\n",
      " - Batch no: 24, train loss: 0.518069326877594\n",
      "-Validation loss after epoc: 0.521897554397583 \n",
      "Epoch:224 \n",
      " - Batch no: 0, train loss: 0.7437039613723755\n",
      " - Batch no: 8, train loss: 0.5096613764762878\n",
      " - Batch no: 16, train loss: 0.5535745620727539\n",
      " - Batch no: 24, train loss: 0.6029990911483765\n",
      "-Validation loss after epoc: 0.5218824148178101 \n",
      "Epoch:225 \n",
      " - Batch no: 0, train loss: 0.7183696031570435\n",
      " - Batch no: 8, train loss: 0.44429972767829895\n",
      " - Batch no: 16, train loss: 0.43758949637413025\n",
      " - Batch no: 24, train loss: 0.6550657749176025\n",
      "-Validation loss after epoc: 0.5220717787742615 \n",
      "Epoch:226 \n",
      " - Batch no: 0, train loss: 0.7194531559944153\n",
      " - Batch no: 8, train loss: 0.6106870770454407\n",
      " - Batch no: 16, train loss: 0.4892522990703583\n",
      " - Batch no: 24, train loss: 0.6052848696708679\n",
      "-Validation loss after epoc: 0.5222269892692566 \n",
      "Epoch:227 \n",
      " - Batch no: 0, train loss: 0.4849466383457184\n",
      " - Batch no: 8, train loss: 0.48868778347969055\n",
      " - Batch no: 16, train loss: 0.7326897978782654\n",
      " - Batch no: 24, train loss: 0.6396055817604065\n",
      "-Validation loss after epoc: 0.5219502449035645 \n",
      "Epoch:228 \n",
      " - Batch no: 0, train loss: 0.6700218915939331\n",
      " - Batch no: 8, train loss: 0.6468256115913391\n",
      " - Batch no: 16, train loss: 0.5722965598106384\n",
      " - Batch no: 24, train loss: 0.5877879858016968\n",
      "-Validation loss after epoc: 0.5216705799102783 \n",
      "Epoch:229 \n",
      " - Batch no: 0, train loss: 0.48139750957489014\n",
      " - Batch no: 8, train loss: 0.7027790546417236\n",
      " - Batch no: 16, train loss: 0.6699081659317017\n",
      " - Batch no: 24, train loss: 0.4846254587173462\n",
      "-Validation loss after epoc: 0.522053062915802 \n",
      "Epoch:230 \n",
      " - Batch no: 0, train loss: 0.44373029470443726\n",
      " - Batch no: 8, train loss: 0.5027909874916077\n",
      " - Batch no: 16, train loss: 0.6341657042503357\n",
      " - Batch no: 24, train loss: 0.6196132302284241\n",
      "-Validation loss after epoc: 0.5216071009635925 \n",
      "Epoch:231 \n",
      " - Batch no: 0, train loss: 0.5363290905952454\n",
      " - Batch no: 8, train loss: 0.5123595595359802\n",
      " - Batch no: 16, train loss: 0.5162131190299988\n",
      " - Batch no: 24, train loss: 0.4423138499259949\n",
      "-Validation loss after epoc: 0.5217805504798889 \n",
      "Epoch:232 \n",
      " - Batch no: 0, train loss: 0.5743347406387329\n",
      " - Batch no: 8, train loss: 0.5454867482185364\n",
      " - Batch no: 16, train loss: 0.5917282104492188\n",
      " - Batch no: 24, train loss: 0.5763439536094666\n",
      "-Validation loss after epoc: 0.5220823884010315 \n",
      "Epoch:233 \n",
      " - Batch no: 0, train loss: 0.6421738862991333\n",
      " - Batch no: 8, train loss: 0.5378284454345703\n",
      " - Batch no: 16, train loss: 0.40866991877555847\n",
      " - Batch no: 24, train loss: 0.6034821271896362\n",
      "-Validation loss after epoc: 0.5207536220550537 \n",
      "Epoch:234 \n",
      " - Batch no: 0, train loss: 0.6036078929901123\n",
      " - Batch no: 8, train loss: 0.4872537851333618\n",
      " - Batch no: 16, train loss: 0.4889226257801056\n",
      " - Batch no: 24, train loss: 0.5992633700370789\n",
      "-Validation loss after epoc: 0.5202230215072632 \n",
      "Epoch:235 \n",
      " - Batch no: 0, train loss: 0.5061236023902893\n",
      " - Batch no: 8, train loss: 0.5907161831855774\n",
      " - Batch no: 16, train loss: 0.5439295768737793\n",
      " - Batch no: 24, train loss: 0.6041380167007446\n",
      "-Validation loss after epoc: 0.5197481513023376 \n",
      "Epoch:236 \n",
      " - Batch no: 0, train loss: 0.545602560043335\n",
      " - Batch no: 8, train loss: 0.5076852440834045\n",
      " - Batch no: 16, train loss: 0.6583170890808105\n",
      " - Batch no: 24, train loss: 0.45226380228996277\n",
      "-Validation loss after epoc: 0.5201148390769958 \n",
      "Epoch:237 \n",
      " - Batch no: 0, train loss: 0.4355519115924835\n",
      " - Batch no: 8, train loss: 0.5289488434791565\n",
      " - Batch no: 16, train loss: 0.461718887090683\n",
      " - Batch no: 24, train loss: 0.7115495800971985\n",
      "-Validation loss after epoc: 0.5195621848106384 \n",
      "Epoch:238 \n",
      " - Batch no: 0, train loss: 0.43759816884994507\n",
      " - Batch no: 8, train loss: 0.6965216994285583\n",
      " - Batch no: 16, train loss: 0.789369523525238\n",
      " - Batch no: 24, train loss: 0.41333654522895813\n",
      "-Validation loss after epoc: 0.5194841623306274 \n",
      "Epoch:239 \n",
      " - Batch no: 0, train loss: 0.5436908602714539\n",
      " - Batch no: 8, train loss: 0.5882936120033264\n",
      " - Batch no: 16, train loss: 0.46580955386161804\n",
      " - Batch no: 24, train loss: 0.42261722683906555\n",
      "-Validation loss after epoc: 0.5194797515869141 \n",
      "Epoch:240 \n",
      " - Batch no: 0, train loss: 0.5727728605270386\n",
      " - Batch no: 8, train loss: 0.7123326063156128\n",
      " - Batch no: 16, train loss: 0.45022091269493103\n",
      " - Batch no: 24, train loss: 0.665973961353302\n",
      "-Validation loss after epoc: 0.520540177822113 \n",
      "Epoch:241 \n",
      " - Batch no: 0, train loss: 0.5589452385902405\n",
      " - Batch no: 8, train loss: 0.6273831129074097\n",
      " - Batch no: 16, train loss: 0.6612118482589722\n",
      " - Batch no: 24, train loss: 0.48740312457084656\n",
      "-Validation loss after epoc: 0.5194169878959656 \n",
      "Epoch:242 \n",
      " - Batch no: 0, train loss: 0.4073428809642792\n",
      " - Batch no: 8, train loss: 0.5486933588981628\n",
      " - Batch no: 16, train loss: 0.5225075483322144\n",
      " - Batch no: 24, train loss: 0.7049510478973389\n",
      "-Validation loss after epoc: 0.520625650882721 \n",
      "Epoch:243 \n",
      " - Batch no: 0, train loss: 0.6388999223709106\n",
      " - Batch no: 8, train loss: 0.6403504014015198\n",
      " - Batch no: 16, train loss: 0.6507113575935364\n",
      " - Batch no: 24, train loss: 0.6201292872428894\n",
      "-Validation loss after epoc: 0.5195997357368469 \n",
      "Epoch:244 \n",
      " - Batch no: 0, train loss: 0.630653440952301\n",
      " - Batch no: 8, train loss: 0.6243683099746704\n",
      " - Batch no: 16, train loss: 0.666030764579773\n",
      " - Batch no: 24, train loss: 0.3476380407810211\n",
      "-Validation loss after epoc: 0.5192369222640991 \n",
      "Epoch:245 \n",
      " - Batch no: 0, train loss: 0.6070929169654846\n",
      " - Batch no: 8, train loss: 0.5725685358047485\n",
      " - Batch no: 16, train loss: 0.4449489712715149\n",
      " - Batch no: 24, train loss: 0.527937650680542\n",
      "-Validation loss after epoc: 0.519655704498291 \n",
      "Epoch:246 \n",
      " - Batch no: 0, train loss: 0.4305461347103119\n",
      " - Batch no: 8, train loss: 0.6267207264900208\n",
      " - Batch no: 16, train loss: 0.5352702140808105\n",
      " - Batch no: 24, train loss: 0.8049483299255371\n",
      "-Validation loss after epoc: 0.5197732448577881 \n",
      "Epoch:247 \n",
      " - Batch no: 0, train loss: 0.7828489542007446\n",
      " - Batch no: 8, train loss: 0.5550743341445923\n",
      " - Batch no: 16, train loss: 0.7216636538505554\n",
      " - Batch no: 24, train loss: 0.44644591212272644\n",
      "-Validation loss after epoc: 0.5204382538795471 \n",
      "Epoch:248 \n",
      " - Batch no: 0, train loss: 0.39602383971214294\n",
      " - Batch no: 8, train loss: 0.5897409915924072\n",
      " - Batch no: 16, train loss: 0.5183936953544617\n",
      " - Batch no: 24, train loss: 0.4878085255622864\n",
      "-Validation loss after epoc: 0.5186955332756042 \n",
      "Epoch:249 \n",
      " - Batch no: 0, train loss: 0.6721262335777283\n",
      " - Batch no: 8, train loss: 0.4516914486885071\n",
      " - Batch no: 16, train loss: 0.5653769373893738\n",
      " - Batch no: 24, train loss: 0.6355413794517517\n",
      "-Validation loss after epoc: 0.5199762582778931 \n",
      "Epoch:250 \n",
      " - Batch no: 0, train loss: 0.6173298358917236\n",
      " - Batch no: 8, train loss: 0.6392378211021423\n",
      " - Batch no: 16, train loss: 0.6245359182357788\n",
      " - Batch no: 24, train loss: 0.5460551977157593\n",
      "-Validation loss after epoc: 0.5188612937927246 \n",
      "Epoch:251 \n",
      " - Batch no: 0, train loss: 0.714784562587738\n",
      " - Batch no: 8, train loss: 0.5012931823730469\n",
      " - Batch no: 16, train loss: 0.5303094387054443\n",
      " - Batch no: 24, train loss: 0.5943722128868103\n",
      "-Validation loss after epoc: 0.5191124677658081 \n",
      "Epoch:252 \n",
      " - Batch no: 0, train loss: 0.5638152360916138\n",
      " - Batch no: 8, train loss: 0.42915767431259155\n",
      " - Batch no: 16, train loss: 0.5547794103622437\n",
      " - Batch no: 24, train loss: 0.6198952794075012\n",
      "-Validation loss after epoc: 0.5194312930107117 \n",
      "Epoch:253 \n",
      " - Batch no: 0, train loss: 0.7764134407043457\n",
      " - Batch no: 8, train loss: 0.5511038899421692\n",
      " - Batch no: 16, train loss: 0.511543333530426\n",
      " - Batch no: 24, train loss: 0.7401383519172668\n",
      "-Validation loss after epoc: 0.5183773636817932 \n",
      "Epoch:254 \n",
      " - Batch no: 0, train loss: 0.4733099043369293\n",
      " - Batch no: 8, train loss: 0.541126012802124\n",
      " - Batch no: 16, train loss: 0.7430071234703064\n",
      " - Batch no: 24, train loss: 0.5686138272285461\n",
      "-Validation loss after epoc: 0.5191078186035156 \n",
      "Epoch:255 \n",
      " - Batch no: 0, train loss: 0.5388880372047424\n",
      " - Batch no: 8, train loss: 0.6170769929885864\n",
      " - Batch no: 16, train loss: 0.5923042893409729\n",
      " - Batch no: 24, train loss: 0.4653962552547455\n",
      "-Validation loss after epoc: 0.5179497599601746 \n",
      "Epoch:256 \n",
      " - Batch no: 0, train loss: 0.677607536315918\n",
      " - Batch no: 8, train loss: 0.7365031242370605\n",
      " - Batch no: 16, train loss: 0.5733558535575867\n",
      " - Batch no: 24, train loss: 0.6849501729011536\n",
      "-Validation loss after epoc: 0.5181660056114197 \n",
      "Epoch:257 \n",
      " - Batch no: 0, train loss: 0.6058351397514343\n",
      " - Batch no: 8, train loss: 0.5358904600143433\n",
      " - Batch no: 16, train loss: 0.3914952278137207\n",
      " - Batch no: 24, train loss: 0.45864495635032654\n",
      "-Validation loss after epoc: 0.5184142589569092 \n",
      "Epoch:258 \n",
      " - Batch no: 0, train loss: 0.6438242197036743\n",
      " - Batch no: 8, train loss: 0.4451410472393036\n",
      " - Batch no: 16, train loss: 0.7958508729934692\n",
      " - Batch no: 24, train loss: 0.5275614857673645\n",
      "-Validation loss after epoc: 0.5181950926780701 \n",
      "Epoch:259 \n",
      " - Batch no: 0, train loss: 0.6650192737579346\n",
      " - Batch no: 8, train loss: 0.6775367856025696\n",
      " - Batch no: 16, train loss: 0.5165367126464844\n",
      " - Batch no: 24, train loss: 0.47385311126708984\n",
      "-Validation loss after epoc: 0.51739901304245 \n",
      "Epoch:260 \n",
      " - Batch no: 0, train loss: 0.45522788166999817\n",
      " - Batch no: 8, train loss: 0.5108022093772888\n",
      " - Batch no: 16, train loss: 0.7475121021270752\n",
      " - Batch no: 24, train loss: 0.742788553237915\n",
      "-Validation loss after epoc: 0.5174105167388916 \n",
      "Epoch:261 \n",
      " - Batch no: 0, train loss: 0.6250492930412292\n",
      " - Batch no: 8, train loss: 0.7396187782287598\n",
      " - Batch no: 16, train loss: 0.46663862466812134\n",
      " - Batch no: 24, train loss: 0.6761800050735474\n",
      "-Validation loss after epoc: 0.5172851085662842 \n",
      "Epoch:262 \n",
      " - Batch no: 0, train loss: 0.6062489748001099\n",
      " - Batch no: 8, train loss: 0.5407165884971619\n",
      " - Batch no: 16, train loss: 0.5606804490089417\n",
      " - Batch no: 24, train loss: 0.4042063057422638\n",
      "-Validation loss after epoc: 0.5171069502830505 \n",
      "Epoch:263 \n",
      " - Batch no: 0, train loss: 0.6181589365005493\n",
      " - Batch no: 8, train loss: 0.4734925627708435\n",
      " - Batch no: 16, train loss: 0.7432883381843567\n",
      " - Batch no: 24, train loss: 0.38262706995010376\n",
      "-Validation loss after epoc: 0.5172704458236694 \n",
      "Epoch:264 \n",
      " - Batch no: 0, train loss: 0.6721919178962708\n",
      " - Batch no: 8, train loss: 0.5267558693885803\n",
      " - Batch no: 16, train loss: 0.5655133128166199\n",
      " - Batch no: 24, train loss: 0.605456531047821\n",
      "-Validation loss after epoc: 0.5168632864952087 \n",
      "Epoch:265 \n",
      " - Batch no: 0, train loss: 0.44252246618270874\n",
      " - Batch no: 8, train loss: 0.6081543564796448\n",
      " - Batch no: 16, train loss: 0.6379799842834473\n",
      " - Batch no: 24, train loss: 0.5297127366065979\n",
      "-Validation loss after epoc: 0.5171592235565186 \n",
      "Epoch:266 \n",
      " - Batch no: 0, train loss: 0.6896247267723083\n",
      " - Batch no: 8, train loss: 0.4992121756076813\n",
      " - Batch no: 16, train loss: 0.7338731288909912\n",
      " - Batch no: 24, train loss: 0.5123100280761719\n",
      "-Validation loss after epoc: 0.5164777636528015 \n",
      "Epoch:267 \n",
      " - Batch no: 0, train loss: 0.6070742011070251\n",
      " - Batch no: 8, train loss: 0.6605402827262878\n",
      " - Batch no: 16, train loss: 0.5403569340705872\n",
      " - Batch no: 24, train loss: 0.4919062852859497\n",
      "-Validation loss after epoc: 0.5163739323616028 \n",
      "Epoch:268 \n",
      " - Batch no: 0, train loss: 0.38650429248809814\n",
      " - Batch no: 8, train loss: 0.7562156319618225\n",
      " - Batch no: 16, train loss: 0.7447839379310608\n",
      " - Batch no: 24, train loss: 0.7889088988304138\n",
      "-Validation loss after epoc: 0.5165907144546509 \n",
      "Epoch:269 \n",
      " - Batch no: 0, train loss: 0.6194388270378113\n",
      " - Batch no: 8, train loss: 0.5638160109519958\n",
      " - Batch no: 16, train loss: 0.5528767704963684\n",
      " - Batch no: 24, train loss: 0.6246805787086487\n",
      "-Validation loss after epoc: 0.5163000226020813 \n",
      "Epoch:270 \n",
      " - Batch no: 0, train loss: 0.8474698662757874\n",
      " - Batch no: 8, train loss: 0.6237383484840393\n",
      " - Batch no: 16, train loss: 0.41349247097969055\n",
      " - Batch no: 24, train loss: 0.5318494439125061\n",
      "-Validation loss after epoc: 0.5165254473686218 \n",
      "Epoch:271 \n",
      " - Batch no: 0, train loss: 0.49937376379966736\n",
      " - Batch no: 8, train loss: 0.5471178293228149\n",
      " - Batch no: 16, train loss: 0.7902613878250122\n",
      " - Batch no: 24, train loss: 0.4142228960990906\n",
      "-Validation loss after epoc: 0.516535222530365 \n",
      "Epoch:272 \n",
      " - Batch no: 0, train loss: 0.5077561736106873\n",
      " - Batch no: 8, train loss: 0.4848672151565552\n",
      " - Batch no: 16, train loss: 0.6557996869087219\n",
      " - Batch no: 24, train loss: 0.6658177971839905\n",
      "-Validation loss after epoc: 0.5171133875846863 \n",
      "Epoch:273 \n",
      " - Batch no: 0, train loss: 0.5136981010437012\n",
      " - Batch no: 8, train loss: 0.7897956371307373\n",
      " - Batch no: 16, train loss: 0.7143279314041138\n",
      " - Batch no: 24, train loss: 0.490653395652771\n",
      "-Validation loss after epoc: 0.5161483883857727 \n",
      "Epoch:274 \n",
      " - Batch no: 0, train loss: 0.5989205837249756\n",
      " - Batch no: 8, train loss: 0.49648749828338623\n",
      " - Batch no: 16, train loss: 0.7229539155960083\n",
      " - Batch no: 24, train loss: 0.768125593662262\n",
      "-Validation loss after epoc: 0.5167856812477112 \n",
      "Epoch:275 \n",
      " - Batch no: 0, train loss: 0.6571522355079651\n",
      " - Batch no: 8, train loss: 0.8133038282394409\n",
      " - Batch no: 16, train loss: 0.5528237819671631\n",
      " - Batch no: 24, train loss: 0.510711133480072\n",
      "-Validation loss after epoc: 0.5177526473999023 \n",
      "Epoch:276 \n",
      " - Batch no: 0, train loss: 0.7417963743209839\n",
      " - Batch no: 8, train loss: 0.4621411859989166\n",
      " - Batch no: 16, train loss: 0.5074319839477539\n",
      " - Batch no: 24, train loss: 0.6325161457061768\n",
      "-Validation loss after epoc: 0.5156119465827942 \n",
      "Epoch:277 \n",
      " - Batch no: 0, train loss: 0.4058540463447571\n",
      " - Batch no: 8, train loss: 0.5635806322097778\n",
      " - Batch no: 16, train loss: 0.5612544417381287\n",
      " - Batch no: 24, train loss: 0.757737398147583\n",
      "-Validation loss after epoc: 0.5150038003921509 \n",
      "Epoch:278 \n",
      " - Batch no: 0, train loss: 0.7041159272193909\n",
      " - Batch no: 8, train loss: 0.6691721081733704\n",
      " - Batch no: 16, train loss: 0.6820549368858337\n",
      " - Batch no: 24, train loss: 0.6031826734542847\n",
      "-Validation loss after epoc: 0.5151835083961487 \n",
      "Epoch:279 \n",
      " - Batch no: 0, train loss: 0.5924291014671326\n",
      " - Batch no: 8, train loss: 0.42228174209594727\n",
      " - Batch no: 16, train loss: 0.4686576724052429\n",
      " - Batch no: 24, train loss: 0.5557022094726562\n",
      "-Validation loss after epoc: 0.5156811475753784 \n",
      "Epoch:280 \n",
      " - Batch no: 0, train loss: 0.6434917449951172\n",
      " - Batch no: 8, train loss: 0.567360520362854\n",
      " - Batch no: 16, train loss: 0.4160183370113373\n",
      " - Batch no: 24, train loss: 0.4425655007362366\n",
      "-Validation loss after epoc: 0.5157707333564758 \n",
      "Epoch:281 \n",
      " - Batch no: 0, train loss: 0.6625364422798157\n",
      " - Batch no: 8, train loss: 0.5031297206878662\n",
      " - Batch no: 16, train loss: 0.4762994945049286\n",
      " - Batch no: 24, train loss: 0.38397181034088135\n",
      "-Validation loss after epoc: 0.5150536298751831 \n",
      "Epoch:282 \n",
      " - Batch no: 0, train loss: 0.4237799644470215\n",
      " - Batch no: 8, train loss: 0.45868274569511414\n",
      " - Batch no: 16, train loss: 0.5112716555595398\n",
      " - Batch no: 24, train loss: 0.5048877000808716\n",
      "-Validation loss after epoc: 0.5169075727462769 \n",
      "Epoch:283 \n",
      " - Batch no: 0, train loss: 0.479808509349823\n",
      " - Batch no: 8, train loss: 0.6926565766334534\n",
      " - Batch no: 16, train loss: 0.6866156458854675\n",
      " - Batch no: 24, train loss: 0.443824827671051\n",
      "-Validation loss after epoc: 0.515826404094696 \n",
      "Epoch:284 \n",
      " - Batch no: 0, train loss: 0.5029504895210266\n",
      " - Batch no: 8, train loss: 0.8330215215682983\n",
      " - Batch no: 16, train loss: 0.3671320080757141\n",
      " - Batch no: 24, train loss: 0.36689621210098267\n",
      "-Validation loss after epoc: 0.5157970786094666 \n",
      "Epoch:285 \n",
      " - Batch no: 0, train loss: 0.6605336666107178\n",
      " - Batch no: 8, train loss: 0.5123090147972107\n",
      " - Batch no: 16, train loss: 0.6292231678962708\n",
      " - Batch no: 24, train loss: 0.5649891495704651\n",
      "-Validation loss after epoc: 0.5153641104698181 \n",
      "Epoch:286 \n",
      " - Batch no: 0, train loss: 0.5974100828170776\n",
      " - Batch no: 8, train loss: 0.5385856032371521\n",
      " - Batch no: 16, train loss: 0.587929368019104\n",
      " - Batch no: 24, train loss: 0.9099028706550598\n",
      "-Validation loss after epoc: 0.5150762796401978 \n",
      "Epoch:287 \n",
      " - Batch no: 0, train loss: 0.4700615108013153\n",
      " - Batch no: 8, train loss: 0.6578330993652344\n",
      " - Batch no: 16, train loss: 0.4207322299480438\n",
      " - Batch no: 24, train loss: 0.4679180383682251\n",
      "-Validation loss after epoc: 0.5150370597839355 \n",
      "Epoch:288 \n",
      " - Batch no: 0, train loss: 0.5067593455314636\n",
      " - Batch no: 8, train loss: 0.4714661240577698\n",
      " - Batch no: 16, train loss: 0.7876006960868835\n",
      " - Batch no: 24, train loss: 0.8125123977661133\n",
      "-Validation loss after epoc: 0.5148712992668152 \n",
      "Epoch:289 \n",
      " - Batch no: 0, train loss: 0.6196689009666443\n",
      " - Batch no: 8, train loss: 0.5814221501350403\n",
      " - Batch no: 16, train loss: 0.5026761293411255\n",
      " - Batch no: 24, train loss: 0.6699127554893494\n",
      "-Validation loss after epoc: 0.5153188109397888 \n",
      "Epoch:290 \n",
      " - Batch no: 0, train loss: 0.49250882863998413\n",
      " - Batch no: 8, train loss: 0.701641321182251\n",
      " - Batch no: 16, train loss: 0.5512745380401611\n",
      " - Batch no: 24, train loss: 0.4719870388507843\n",
      "-Validation loss after epoc: 0.5155957937240601 \n",
      "Epoch:291 \n",
      " - Batch no: 0, train loss: 0.6434023380279541\n",
      " - Batch no: 8, train loss: 0.5978461503982544\n",
      " - Batch no: 16, train loss: 0.556489109992981\n",
      " - Batch no: 24, train loss: 0.6173524856567383\n",
      "-Validation loss after epoc: 0.5144132971763611 \n",
      "Epoch:292 \n",
      " - Batch no: 0, train loss: 0.4064050018787384\n",
      " - Batch no: 8, train loss: 1.0289195775985718\n",
      " - Batch no: 16, train loss: 0.5287147164344788\n",
      " - Batch no: 24, train loss: 0.6723939180374146\n",
      "-Validation loss after epoc: 0.5139257311820984 \n",
      "Epoch:293 \n",
      " - Batch no: 0, train loss: 0.5547100901603699\n",
      " - Batch no: 8, train loss: 0.6583462953567505\n",
      " - Batch no: 16, train loss: 0.45353734493255615\n",
      " - Batch no: 24, train loss: 0.6060281991958618\n",
      "-Validation loss after epoc: 0.5142440795898438 \n",
      "Epoch:294 \n",
      " - Batch no: 0, train loss: 0.4931334853172302\n",
      " - Batch no: 8, train loss: 0.5885505080223083\n",
      " - Batch no: 16, train loss: 0.45867955684661865\n",
      " - Batch no: 24, train loss: 0.43296122550964355\n",
      "-Validation loss after epoc: 0.514563262462616 \n",
      "Epoch:295 \n",
      " - Batch no: 0, train loss: 0.5715226531028748\n",
      " - Batch no: 8, train loss: 0.3713173270225525\n",
      " - Batch no: 16, train loss: 0.4085378646850586\n",
      " - Batch no: 24, train loss: 0.4349462389945984\n",
      "-Validation loss after epoc: 0.5134320855140686 \n",
      "Epoch:296 \n",
      " - Batch no: 0, train loss: 0.9243065118789673\n",
      " - Batch no: 8, train loss: 0.4463333189487457\n",
      " - Batch no: 16, train loss: 0.6067237854003906\n",
      " - Batch no: 24, train loss: 0.526696503162384\n",
      "-Validation loss after epoc: 0.5133026242256165 \n",
      "Epoch:297 \n",
      " - Batch no: 0, train loss: 0.4482102394104004\n",
      " - Batch no: 8, train loss: 0.47995781898498535\n",
      " - Batch no: 16, train loss: 0.6784164905548096\n",
      " - Batch no: 24, train loss: 0.5950069427490234\n",
      "-Validation loss after epoc: 0.514671266078949 \n",
      "Epoch:298 \n",
      " - Batch no: 0, train loss: 0.5814616084098816\n",
      " - Batch no: 8, train loss: 0.5359873175621033\n",
      " - Batch no: 16, train loss: 0.7228075265884399\n",
      " - Batch no: 24, train loss: 0.4762237071990967\n",
      "-Validation loss after epoc: 0.5145412087440491 \n",
      "Epoch:299 \n",
      " - Batch no: 0, train loss: 0.49562132358551025\n",
      " - Batch no: 8, train loss: 0.629632830619812\n",
      " - Batch no: 16, train loss: 0.46877166628837585\n",
      " - Batch no: 24, train loss: 0.6408036947250366\n",
      "-Validation loss after epoc: 0.5143295526504517 \n",
      "Epoch:300 \n",
      " - Batch no: 0, train loss: 0.5436370968818665\n",
      " - Batch no: 8, train loss: 0.5035116076469421\n",
      " - Batch no: 16, train loss: 0.4766949415206909\n",
      " - Batch no: 24, train loss: 0.44268596172332764\n",
      "-Validation loss after epoc: 0.5156629681587219 \n",
      "Epoch:301 \n",
      " - Batch no: 0, train loss: 0.37788963317871094\n",
      " - Batch no: 8, train loss: 0.6164783835411072\n",
      " - Batch no: 16, train loss: 0.633765697479248\n",
      " - Batch no: 24, train loss: 0.6629618406295776\n",
      "-Validation loss after epoc: 0.5133711099624634 \n",
      "Epoch:302 \n",
      " - Batch no: 0, train loss: 0.5050758123397827\n",
      " - Batch no: 8, train loss: 0.5531380772590637\n",
      " - Batch no: 16, train loss: 0.46769997477531433\n",
      " - Batch no: 24, train loss: 0.7182364463806152\n",
      "-Validation loss after epoc: 0.512809693813324 \n",
      "Epoch:303 \n",
      " - Batch no: 0, train loss: 0.4552445709705353\n",
      " - Batch no: 8, train loss: 0.5562096238136292\n",
      " - Batch no: 16, train loss: 0.6497443914413452\n",
      " - Batch no: 24, train loss: 0.47151052951812744\n",
      "-Validation loss after epoc: 0.5127447843551636 \n",
      "Epoch:304 \n",
      " - Batch no: 0, train loss: 0.6334848999977112\n",
      " - Batch no: 8, train loss: 0.4338720142841339\n",
      " - Batch no: 16, train loss: 0.5141867399215698\n",
      " - Batch no: 24, train loss: 0.5858358144760132\n",
      "-Validation loss after epoc: 0.5129342079162598 \n",
      "Epoch:305 \n",
      " - Batch no: 0, train loss: 0.6437328457832336\n",
      " - Batch no: 8, train loss: 0.7295435070991516\n",
      " - Batch no: 16, train loss: 0.4756188690662384\n",
      " - Batch no: 24, train loss: 0.5804809331893921\n",
      "-Validation loss after epoc: 0.513169527053833 \n",
      "Epoch:306 \n",
      " - Batch no: 0, train loss: 0.723915696144104\n",
      " - Batch no: 8, train loss: 0.5322476625442505\n",
      " - Batch no: 16, train loss: 0.55037522315979\n",
      " - Batch no: 24, train loss: 0.5720607042312622\n",
      "-Validation loss after epoc: 0.5122098922729492 \n",
      "Epoch:307 \n",
      " - Batch no: 0, train loss: 0.4711742699146271\n",
      " - Batch no: 8, train loss: 0.8131586909294128\n",
      " - Batch no: 16, train loss: 0.418478399515152\n",
      " - Batch no: 24, train loss: 0.6707461476325989\n",
      "-Validation loss after epoc: 0.5122541785240173 \n",
      "Epoch:308 \n",
      " - Batch no: 0, train loss: 0.8634665012359619\n",
      " - Batch no: 8, train loss: 0.4507414996623993\n",
      " - Batch no: 16, train loss: 0.438195139169693\n",
      " - Batch no: 24, train loss: 0.5410812497138977\n",
      "-Validation loss after epoc: 0.5123174786567688 \n",
      "Epoch:309 \n",
      " - Batch no: 0, train loss: 0.35860010981559753\n",
      " - Batch no: 8, train loss: 0.5914459228515625\n",
      " - Batch no: 16, train loss: 0.8149331212043762\n",
      " - Batch no: 24, train loss: 0.5138946175575256\n",
      "-Validation loss after epoc: 0.5124749541282654 \n",
      "Epoch:310 \n",
      " - Batch no: 0, train loss: 0.903248131275177\n",
      " - Batch no: 8, train loss: 0.41024550795555115\n",
      " - Batch no: 16, train loss: 0.44739627838134766\n",
      " - Batch no: 24, train loss: 0.548570990562439\n",
      "-Validation loss after epoc: 0.5128379464149475 \n",
      "Epoch:311 \n",
      " - Batch no: 0, train loss: 0.4872586727142334\n",
      " - Batch no: 8, train loss: 0.5802854299545288\n",
      " - Batch no: 16, train loss: 0.4714369475841522\n",
      " - Batch no: 24, train loss: 0.6548944115638733\n",
      "-Validation loss after epoc: 0.5121208429336548 \n",
      "Epoch:312 \n",
      " - Batch no: 0, train loss: 0.5753690004348755\n",
      " - Batch no: 8, train loss: 0.670552134513855\n",
      " - Batch no: 16, train loss: 0.5742947459220886\n",
      " - Batch no: 24, train loss: 0.4644622206687927\n",
      "-Validation loss after epoc: 0.5121663808822632 \n",
      "Epoch:313 \n",
      " - Batch no: 0, train loss: 0.6015274524688721\n",
      " - Batch no: 8, train loss: 0.6185237765312195\n",
      " - Batch no: 16, train loss: 0.29046630859375\n",
      " - Batch no: 24, train loss: 0.5902314186096191\n",
      "-Validation loss after epoc: 0.5118895769119263 \n",
      "Epoch:314 \n",
      " - Batch no: 0, train loss: 0.47117841243743896\n",
      " - Batch no: 8, train loss: 0.40639713406562805\n",
      " - Batch no: 16, train loss: 0.5022173523902893\n",
      " - Batch no: 24, train loss: 0.650268018245697\n",
      "-Validation loss after epoc: 0.5117250084877014 \n",
      "Epoch:315 \n",
      " - Batch no: 0, train loss: 0.5230151414871216\n",
      " - Batch no: 8, train loss: 0.7311640381813049\n",
      " - Batch no: 16, train loss: 0.5625467300415039\n",
      " - Batch no: 24, train loss: 0.4970017969608307\n",
      "-Validation loss after epoc: 0.5116747617721558 \n",
      "Epoch:316 \n",
      " - Batch no: 0, train loss: 0.5295009016990662\n",
      " - Batch no: 8, train loss: 0.5280331373214722\n",
      " - Batch no: 16, train loss: 0.6011010408401489\n",
      " - Batch no: 24, train loss: 0.8495579361915588\n",
      "-Validation loss after epoc: 0.511621356010437 \n",
      "Epoch:317 \n",
      " - Batch no: 0, train loss: 0.5209844708442688\n",
      " - Batch no: 8, train loss: 0.6269529461860657\n",
      " - Batch no: 16, train loss: 0.611305296421051\n",
      " - Batch no: 24, train loss: 0.7043507099151611\n",
      "-Validation loss after epoc: 0.5110026597976685 \n",
      "Epoch:318 \n",
      " - Batch no: 0, train loss: 0.37613025307655334\n",
      " - Batch no: 8, train loss: 0.6266871094703674\n",
      " - Batch no: 16, train loss: 0.6700946688652039\n",
      " - Batch no: 24, train loss: 0.45461320877075195\n",
      "-Validation loss after epoc: 0.51125168800354 \n",
      "Epoch:319 \n",
      " - Batch no: 0, train loss: 0.6893535852432251\n",
      " - Batch no: 8, train loss: 0.6281189322471619\n",
      " - Batch no: 16, train loss: 0.5288801193237305\n",
      " - Batch no: 24, train loss: 0.5444543361663818\n",
      "-Validation loss after epoc: 0.5106450915336609 \n",
      "Epoch:320 \n",
      " - Batch no: 0, train loss: 0.46759089827537537\n",
      " - Batch no: 8, train loss: 0.5739417672157288\n",
      " - Batch no: 16, train loss: 0.5615724325180054\n",
      " - Batch no: 24, train loss: 0.5224707722663879\n",
      "-Validation loss after epoc: 0.5107640624046326 \n",
      "Epoch:321 \n",
      " - Batch no: 0, train loss: 0.5077661275863647\n",
      " - Batch no: 8, train loss: 0.5946733951568604\n",
      " - Batch no: 16, train loss: 0.4318919777870178\n",
      " - Batch no: 24, train loss: 0.7310116291046143\n",
      "-Validation loss after epoc: 0.5117778182029724 \n",
      "Epoch:322 \n",
      " - Batch no: 0, train loss: 0.4635637700557709\n",
      " - Batch no: 8, train loss: 0.6476540565490723\n",
      " - Batch no: 16, train loss: 0.8247689604759216\n",
      " - Batch no: 24, train loss: 0.6196591854095459\n",
      "-Validation loss after epoc: 0.5102939605712891 \n",
      "Epoch:323 \n",
      " - Batch no: 0, train loss: 0.463926762342453\n",
      " - Batch no: 8, train loss: 0.4190678298473358\n",
      " - Batch no: 16, train loss: 0.5612422227859497\n",
      " - Batch no: 24, train loss: 0.7704824805259705\n",
      "-Validation loss after epoc: 0.5104715824127197 \n",
      "Epoch:324 \n",
      " - Batch no: 0, train loss: 0.4813024699687958\n",
      " - Batch no: 8, train loss: 0.5120953321456909\n",
      " - Batch no: 16, train loss: 0.5067944526672363\n",
      " - Batch no: 24, train loss: 0.3844214975833893\n",
      "-Validation loss after epoc: 0.5105684399604797 \n",
      "Epoch:325 \n",
      " - Batch no: 0, train loss: 0.7354158163070679\n",
      " - Batch no: 8, train loss: 0.557380199432373\n",
      " - Batch no: 16, train loss: 0.62703937292099\n",
      " - Batch no: 24, train loss: 0.5321072340011597\n",
      "-Validation loss after epoc: 0.5106502175331116 \n",
      "Epoch:326 \n",
      " - Batch no: 0, train loss: 0.4991643726825714\n",
      " - Batch no: 8, train loss: 0.5341914296150208\n",
      " - Batch no: 16, train loss: 0.7083143591880798\n",
      " - Batch no: 24, train loss: 0.6443701982498169\n",
      "-Validation loss after epoc: 0.5108858346939087 \n",
      "Epoch:327 \n",
      " - Batch no: 0, train loss: 0.590595006942749\n",
      " - Batch no: 8, train loss: 0.49251851439476013\n",
      " - Batch no: 16, train loss: 0.5794347524642944\n",
      " - Batch no: 24, train loss: 0.6145188212394714\n",
      "-Validation loss after epoc: 0.5102800726890564 \n",
      "Epoch:328 \n",
      " - Batch no: 0, train loss: 0.46409255266189575\n",
      " - Batch no: 8, train loss: 0.400420606136322\n",
      " - Batch no: 16, train loss: 0.6285832524299622\n",
      " - Batch no: 24, train loss: 0.6766268014907837\n",
      "-Validation loss after epoc: 0.510397732257843 \n",
      "Epoch:329 \n",
      " - Batch no: 0, train loss: 0.5929573178291321\n",
      " - Batch no: 8, train loss: 0.6493445038795471\n",
      " - Batch no: 16, train loss: 0.4878317415714264\n",
      " - Batch no: 24, train loss: 0.48343396186828613\n",
      "-Validation loss after epoc: 0.5099161863327026 \n",
      "Epoch:330 \n",
      " - Batch no: 0, train loss: 0.6812121272087097\n",
      " - Batch no: 8, train loss: 0.6850195527076721\n",
      " - Batch no: 16, train loss: 0.6483513116836548\n",
      " - Batch no: 24, train loss: 0.5130836963653564\n",
      "-Validation loss after epoc: 0.5101348757743835 \n",
      "Epoch:331 \n",
      " - Batch no: 0, train loss: 0.5739768743515015\n",
      " - Batch no: 8, train loss: 0.49005016684532166\n",
      " - Batch no: 16, train loss: 0.40599608421325684\n",
      " - Batch no: 24, train loss: 0.6087533235549927\n",
      "-Validation loss after epoc: 0.5102866888046265 \n",
      "Epoch:332 \n",
      " - Batch no: 0, train loss: 0.6113266944885254\n",
      " - Batch no: 8, train loss: 0.5400692224502563\n",
      " - Batch no: 16, train loss: 0.5903966426849365\n",
      " - Batch no: 24, train loss: 0.5686019659042358\n",
      "-Validation loss after epoc: 0.509721040725708 \n",
      "Epoch:333 \n",
      " - Batch no: 0, train loss: 0.3783595860004425\n",
      " - Batch no: 8, train loss: 0.7200562357902527\n",
      " - Batch no: 16, train loss: 0.5603825449943542\n",
      " - Batch no: 24, train loss: 0.5384884476661682\n",
      "-Validation loss after epoc: 0.5097290277481079 \n",
      "Epoch:334 \n",
      " - Batch no: 0, train loss: 0.38679802417755127\n",
      " - Batch no: 8, train loss: 0.6662361025810242\n",
      " - Batch no: 16, train loss: 0.8678914904594421\n",
      " - Batch no: 24, train loss: 0.6153171062469482\n",
      "-Validation loss after epoc: 0.509878396987915 \n",
      "Epoch:335 \n",
      " - Batch no: 0, train loss: 0.5150495171546936\n",
      " - Batch no: 8, train loss: 0.4134054183959961\n",
      " - Batch no: 16, train loss: 0.47999289631843567\n",
      " - Batch no: 24, train loss: 0.8611345291137695\n",
      "-Validation loss after epoc: 0.5095045566558838 \n",
      "Epoch:336 \n",
      " - Batch no: 0, train loss: 0.4584541618824005\n",
      " - Batch no: 8, train loss: 0.592875599861145\n",
      " - Batch no: 16, train loss: 0.5778512358665466\n",
      " - Batch no: 24, train loss: 0.7202804088592529\n",
      "-Validation loss after epoc: 0.5104572772979736 \n",
      "Epoch:337 \n",
      " - Batch no: 0, train loss: 0.5799689292907715\n",
      " - Batch no: 8, train loss: 0.6344739198684692\n",
      " - Batch no: 16, train loss: 0.5112245082855225\n",
      " - Batch no: 24, train loss: 0.6424935460090637\n",
      "-Validation loss after epoc: 0.5101566910743713 \n",
      "Epoch:338 \n",
      " - Batch no: 0, train loss: 0.6575496792793274\n",
      " - Batch no: 8, train loss: 0.6170833110809326\n",
      " - Batch no: 16, train loss: 0.540656566619873\n",
      " - Batch no: 24, train loss: 0.5262234807014465\n",
      "-Validation loss after epoc: 0.5098921060562134 \n",
      "Epoch:339 \n",
      " - Batch no: 0, train loss: 0.42278119921684265\n",
      " - Batch no: 8, train loss: 0.7135318517684937\n",
      " - Batch no: 16, train loss: 0.7437758445739746\n",
      " - Batch no: 24, train loss: 0.5063372850418091\n",
      "-Validation loss after epoc: 0.5089721083641052 \n",
      "Epoch:340 \n",
      " - Batch no: 0, train loss: 0.39897632598876953\n",
      " - Batch no: 8, train loss: 0.680594265460968\n",
      " - Batch no: 16, train loss: 0.682012140750885\n",
      " - Batch no: 24, train loss: 0.5044712424278259\n",
      "-Validation loss after epoc: 0.5092743039131165 \n",
      "Epoch:341 \n",
      " - Batch no: 0, train loss: 0.5083565711975098\n",
      " - Batch no: 8, train loss: 0.47019466757774353\n",
      " - Batch no: 16, train loss: 0.3628479242324829\n",
      " - Batch no: 24, train loss: 0.4749557077884674\n",
      "-Validation loss after epoc: 0.5093821883201599 \n",
      "Epoch:342 \n",
      " - Batch no: 0, train loss: 0.4459454119205475\n",
      " - Batch no: 8, train loss: 0.4787554144859314\n",
      " - Batch no: 16, train loss: 0.6398558616638184\n",
      " - Batch no: 24, train loss: 0.47278767824172974\n",
      "-Validation loss after epoc: 0.5088534951210022 \n",
      "Epoch:343 \n",
      " - Batch no: 0, train loss: 0.5050195455551147\n",
      " - Batch no: 8, train loss: 0.5497274398803711\n",
      " - Batch no: 16, train loss: 0.5944297313690186\n",
      " - Batch no: 24, train loss: 0.5615532398223877\n",
      "-Validation loss after epoc: 0.5088357925415039 \n",
      "Epoch:344 \n",
      " - Batch no: 0, train loss: 0.6810451745986938\n",
      " - Batch no: 8, train loss: 0.6869751214981079\n",
      " - Batch no: 16, train loss: 0.4588424563407898\n",
      " - Batch no: 24, train loss: 0.4782920479774475\n",
      "-Validation loss after epoc: 0.5089803338050842 \n",
      "Epoch:345 \n",
      " - Batch no: 0, train loss: 0.5668946504592896\n",
      " - Batch no: 8, train loss: 0.4801996946334839\n",
      " - Batch no: 16, train loss: 0.5881454944610596\n",
      " - Batch no: 24, train loss: 0.5111250877380371\n",
      "-Validation loss after epoc: 0.5099647045135498 \n",
      "Epoch:346 \n",
      " - Batch no: 0, train loss: 0.6344528794288635\n",
      " - Batch no: 8, train loss: 0.6191743016242981\n",
      " - Batch no: 16, train loss: 0.4758443534374237\n",
      " - Batch no: 24, train loss: 0.48283880949020386\n",
      "-Validation loss after epoc: 0.5091440677642822 \n",
      "Epoch:347 \n",
      " - Batch no: 0, train loss: 0.44874107837677\n",
      " - Batch no: 8, train loss: 0.6432551741600037\n",
      " - Batch no: 16, train loss: 0.6463468074798584\n",
      " - Batch no: 24, train loss: 0.37730830907821655\n",
      "-Validation loss after epoc: 0.5088512897491455 \n",
      "Epoch:348 \n",
      " - Batch no: 0, train loss: 0.5598794221878052\n",
      " - Batch no: 8, train loss: 0.5965433716773987\n",
      " - Batch no: 16, train loss: 0.5660316348075867\n",
      " - Batch no: 24, train loss: 0.5625503063201904\n",
      "-Validation loss after epoc: 0.5081815719604492 \n",
      "Epoch:349 \n",
      " - Batch no: 0, train loss: 0.5691038966178894\n",
      " - Batch no: 8, train loss: 0.5660414099693298\n",
      " - Batch no: 16, train loss: 0.7223767042160034\n",
      " - Batch no: 24, train loss: 0.6283875703811646\n",
      "-Validation loss after epoc: 0.5089111924171448 \n",
      "Epoch:350 \n",
      " - Batch no: 0, train loss: 0.5299574732780457\n",
      " - Batch no: 8, train loss: 0.5455743074417114\n",
      " - Batch no: 16, train loss: 0.44242870807647705\n",
      " - Batch no: 24, train loss: 0.671297013759613\n",
      "-Validation loss after epoc: 0.5088796019554138 \n",
      "Epoch:351 \n",
      " - Batch no: 0, train loss: 0.5550805926322937\n",
      " - Batch no: 8, train loss: 0.48511287569999695\n",
      " - Batch no: 16, train loss: 0.5631160140037537\n",
      " - Batch no: 24, train loss: 0.7007320523262024\n",
      "-Validation loss after epoc: 0.5087443590164185 \n",
      "Epoch:352 \n",
      " - Batch no: 0, train loss: 0.4959930181503296\n",
      " - Batch no: 8, train loss: 0.3721233010292053\n",
      " - Batch no: 16, train loss: 0.48996004462242126\n",
      " - Batch no: 24, train loss: 0.5498149394989014\n",
      "-Validation loss after epoc: 0.5088584423065186 \n",
      "Epoch:353 \n",
      " - Batch no: 0, train loss: 0.49443909525871277\n",
      " - Batch no: 8, train loss: 0.46716150641441345\n",
      " - Batch no: 16, train loss: 0.5813542604446411\n",
      " - Batch no: 24, train loss: 0.7890633344650269\n",
      "-Validation loss after epoc: 0.5089574456214905 \n",
      "Epoch:354 \n",
      " - Batch no: 0, train loss: 0.6750681400299072\n",
      " - Batch no: 8, train loss: 0.47002294659614563\n",
      " - Batch no: 16, train loss: 0.5483292937278748\n",
      " - Batch no: 24, train loss: 0.5239032506942749\n",
      "-Validation loss after epoc: 0.5090060830116272 \n",
      "Epoch:355 \n",
      " - Batch no: 0, train loss: 0.5343525409698486\n",
      " - Batch no: 8, train loss: 0.4995768070220947\n",
      " - Batch no: 16, train loss: 0.6335709095001221\n",
      " - Batch no: 24, train loss: 0.437845915555954\n",
      "-Validation loss after epoc: 0.5076566338539124 \n",
      "Epoch:356 \n",
      " - Batch no: 0, train loss: 0.5613695383071899\n",
      " - Batch no: 8, train loss: 0.5339295864105225\n",
      " - Batch no: 16, train loss: 0.7541727423667908\n",
      " - Batch no: 24, train loss: 0.4466923177242279\n",
      "-Validation loss after epoc: 0.5070666074752808 \n",
      "Epoch:357 \n",
      " - Batch no: 0, train loss: 0.6441050171852112\n",
      " - Batch no: 8, train loss: 0.4974750280380249\n",
      " - Batch no: 16, train loss: 0.43003758788108826\n",
      " - Batch no: 24, train loss: 0.5910974144935608\n",
      "-Validation loss after epoc: 0.5071250200271606 \n",
      "Epoch:358 \n",
      " - Batch no: 0, train loss: 0.498710960149765\n",
      " - Batch no: 8, train loss: 0.5035203099250793\n",
      " - Batch no: 16, train loss: 0.49712249636650085\n",
      " - Batch no: 24, train loss: 0.44041404128074646\n",
      "-Validation loss after epoc: 0.5068169236183167 \n",
      "Epoch:359 \n",
      " - Batch no: 0, train loss: 0.5997470617294312\n",
      " - Batch no: 8, train loss: 0.6623302102088928\n",
      " - Batch no: 16, train loss: 0.6670678853988647\n",
      " - Batch no: 24, train loss: 0.5893322229385376\n",
      "-Validation loss after epoc: 0.5071203112602234 \n",
      "Epoch:360 \n",
      " - Batch no: 0, train loss: 0.8221741318702698\n",
      " - Batch no: 8, train loss: 0.46871405839920044\n",
      " - Batch no: 16, train loss: 0.46037018299102783\n",
      " - Batch no: 24, train loss: 0.5243849754333496\n",
      "-Validation loss after epoc: 0.5069078803062439 \n",
      "Epoch:361 \n",
      " - Batch no: 0, train loss: 0.45817944407463074\n",
      " - Batch no: 8, train loss: 0.4713990390300751\n",
      " - Batch no: 16, train loss: 0.8321729302406311\n",
      " - Batch no: 24, train loss: 0.49871963262557983\n",
      "-Validation loss after epoc: 0.5069074034690857 \n",
      "Epoch:362 \n",
      " - Batch no: 0, train loss: 0.6345024108886719\n",
      " - Batch no: 8, train loss: 0.528012752532959\n",
      " - Batch no: 16, train loss: 0.48642075061798096\n",
      " - Batch no: 24, train loss: 0.7708937525749207\n",
      "-Validation loss after epoc: 0.5073068141937256 \n",
      "Epoch:363 \n",
      " - Batch no: 0, train loss: 0.6638190746307373\n",
      " - Batch no: 8, train loss: 0.5782225728034973\n",
      " - Batch no: 16, train loss: 0.5099744200706482\n",
      " - Batch no: 24, train loss: 0.6213893890380859\n",
      "-Validation loss after epoc: 0.5072028040885925 \n",
      "Epoch:364 \n",
      " - Batch no: 0, train loss: 0.7507992386817932\n",
      " - Batch no: 8, train loss: 0.5285432934761047\n",
      " - Batch no: 16, train loss: 0.563232421875\n",
      " - Batch no: 24, train loss: 0.438199907541275\n",
      "-Validation loss after epoc: 0.5071828365325928 \n",
      "Epoch:365 \n",
      " - Batch no: 0, train loss: 0.6458721160888672\n",
      " - Batch no: 8, train loss: 0.6906604170799255\n",
      " - Batch no: 16, train loss: 0.5646966695785522\n",
      " - Batch no: 24, train loss: 0.449278324842453\n",
      "-Validation loss after epoc: 0.506758987903595 \n",
      "Epoch:366 \n",
      " - Batch no: 0, train loss: 0.45401647686958313\n",
      " - Batch no: 8, train loss: 0.7355328798294067\n",
      " - Batch no: 16, train loss: 0.5247841477394104\n",
      " - Batch no: 24, train loss: 0.6153927445411682\n",
      "-Validation loss after epoc: 0.5063591003417969 \n",
      "Epoch:367 \n",
      " - Batch no: 0, train loss: 0.6253020763397217\n",
      " - Batch no: 8, train loss: 0.6427270174026489\n",
      " - Batch no: 16, train loss: 0.6632628440856934\n",
      " - Batch no: 24, train loss: 0.5676017999649048\n",
      "-Validation loss after epoc: 0.5064243674278259 \n",
      "Epoch:368 \n",
      " - Batch no: 0, train loss: 0.4225285053253174\n",
      " - Batch no: 8, train loss: 0.5121111869812012\n",
      " - Batch no: 16, train loss: 0.5591126084327698\n",
      " - Batch no: 24, train loss: 0.6795545816421509\n",
      "-Validation loss after epoc: 0.5064359903335571 \n",
      "Epoch:369 \n",
      " - Batch no: 0, train loss: 0.6076456904411316\n",
      " - Batch no: 8, train loss: 0.5889525413513184\n",
      " - Batch no: 16, train loss: 0.830625057220459\n",
      " - Batch no: 24, train loss: 0.7178927659988403\n",
      "-Validation loss after epoc: 0.5065327286720276 \n",
      "Epoch:370 \n",
      " - Batch no: 0, train loss: 0.4906405508518219\n",
      " - Batch no: 8, train loss: 0.4760366380214691\n",
      " - Batch no: 16, train loss: 0.616209864616394\n",
      " - Batch no: 24, train loss: 0.5245722532272339\n",
      "-Validation loss after epoc: 0.506641685962677 \n",
      "Epoch:371 \n",
      " - Batch no: 0, train loss: 0.5379890203475952\n",
      " - Batch no: 8, train loss: 0.6408659815788269\n",
      " - Batch no: 16, train loss: 0.7282317876815796\n",
      " - Batch no: 24, train loss: 0.5382118225097656\n",
      "-Validation loss after epoc: 0.5066052675247192 \n",
      "Epoch:372 \n",
      " - Batch no: 0, train loss: 0.5242710113525391\n",
      " - Batch no: 8, train loss: 0.5037747621536255\n",
      " - Batch no: 16, train loss: 0.5539971590042114\n",
      " - Batch no: 24, train loss: 0.43434950709342957\n",
      "-Validation loss after epoc: 0.5068551898002625 \n",
      "Epoch:373 \n",
      " - Batch no: 0, train loss: 0.4956645667552948\n",
      " - Batch no: 8, train loss: 0.6458605527877808\n",
      " - Batch no: 16, train loss: 0.5702347159385681\n",
      " - Batch no: 24, train loss: 0.5817862153053284\n",
      "-Validation loss after epoc: 0.5068424344062805 \n",
      "Epoch:374 \n",
      " - Batch no: 0, train loss: 0.5507897734642029\n",
      " - Batch no: 8, train loss: 0.5405203104019165\n",
      " - Batch no: 16, train loss: 0.608506441116333\n",
      " - Batch no: 24, train loss: 0.5996154546737671\n",
      "-Validation loss after epoc: 0.5063968300819397 \n",
      "Epoch:375 \n",
      " - Batch no: 0, train loss: 0.6050537824630737\n",
      " - Batch no: 8, train loss: 0.46560508012771606\n",
      " - Batch no: 16, train loss: 0.5753305554389954\n",
      " - Batch no: 24, train loss: 0.4846593141555786\n",
      "-Validation loss after epoc: 0.5060784220695496 \n",
      "Epoch:376 \n",
      " - Batch no: 0, train loss: 0.7956134676933289\n",
      " - Batch no: 8, train loss: 0.43355679512023926\n",
      " - Batch no: 16, train loss: 0.5312944054603577\n",
      " - Batch no: 24, train loss: 0.5877524614334106\n",
      "-Validation loss after epoc: 0.506259024143219 \n",
      "Epoch:377 \n",
      " - Batch no: 0, train loss: 0.5341998338699341\n",
      " - Batch no: 8, train loss: 0.6972888708114624\n",
      " - Batch no: 16, train loss: 0.7778148651123047\n",
      " - Batch no: 24, train loss: 0.5147784948348999\n",
      "-Validation loss after epoc: 0.506157398223877 \n",
      "Epoch:378 \n",
      " - Batch no: 0, train loss: 0.46100571751594543\n",
      " - Batch no: 8, train loss: 0.776512086391449\n",
      " - Batch no: 16, train loss: 0.4265674352645874\n",
      " - Batch no: 24, train loss: 0.6385270953178406\n",
      "-Validation loss after epoc: 0.5056921243667603 \n",
      "Epoch:379 \n",
      " - Batch no: 0, train loss: 0.7045389413833618\n",
      " - Batch no: 8, train loss: 0.531650960445404\n",
      " - Batch no: 16, train loss: 0.568786084651947\n",
      " - Batch no: 24, train loss: 0.5816128253936768\n",
      "-Validation loss after epoc: 0.5056477189064026 \n",
      "Epoch:380 \n",
      " - Batch no: 0, train loss: 0.41333091259002686\n",
      " - Batch no: 8, train loss: 0.752380907535553\n",
      " - Batch no: 16, train loss: 0.5820139050483704\n",
      " - Batch no: 24, train loss: 0.5052090883255005\n",
      "-Validation loss after epoc: 0.5055320262908936 \n",
      "Epoch:381 \n",
      " - Batch no: 0, train loss: 0.4853125512599945\n",
      " - Batch no: 8, train loss: 0.5882312059402466\n",
      " - Batch no: 16, train loss: 0.6672945618629456\n",
      " - Batch no: 24, train loss: 0.7890657186508179\n",
      "-Validation loss after epoc: 0.5062596797943115 \n",
      "Epoch:382 \n",
      " - Batch no: 0, train loss: 0.5428871512413025\n",
      " - Batch no: 8, train loss: 0.48719674348831177\n",
      " - Batch no: 16, train loss: 0.5079758167266846\n",
      " - Batch no: 24, train loss: 0.7088272571563721\n",
      "-Validation loss after epoc: 0.5055240392684937 \n",
      "Epoch:383 \n",
      " - Batch no: 0, train loss: 0.5866384506225586\n",
      " - Batch no: 8, train loss: 0.5142154693603516\n",
      " - Batch no: 16, train loss: 0.5380458831787109\n",
      " - Batch no: 24, train loss: 0.7639099955558777\n",
      "-Validation loss after epoc: 0.5062150955200195 \n",
      "Epoch:384 \n",
      " - Batch no: 0, train loss: 0.4603320062160492\n",
      " - Batch no: 8, train loss: 0.6875949501991272\n",
      " - Batch no: 16, train loss: 0.506996214389801\n",
      " - Batch no: 24, train loss: 0.47402799129486084\n",
      "-Validation loss after epoc: 0.5055204033851624 \n",
      "Epoch:385 \n",
      " - Batch no: 0, train loss: 0.6905792355537415\n",
      " - Batch no: 8, train loss: 0.7064369916915894\n",
      " - Batch no: 16, train loss: 0.5205752849578857\n",
      " - Batch no: 24, train loss: 0.5122869610786438\n",
      "-Validation loss after epoc: 0.5062822699546814 \n",
      "Epoch:386 \n",
      " - Batch no: 0, train loss: 0.4429706931114197\n",
      " - Batch no: 8, train loss: 0.431665301322937\n",
      " - Batch no: 16, train loss: 0.4448041021823883\n",
      " - Batch no: 24, train loss: 0.6325072646141052\n",
      "-Validation loss after epoc: 0.5050340890884399 \n",
      "Epoch:387 \n",
      " - Batch no: 0, train loss: 0.547290563583374\n",
      " - Batch no: 8, train loss: 0.8622838258743286\n",
      " - Batch no: 16, train loss: 0.4011813998222351\n",
      " - Batch no: 24, train loss: 0.41763344407081604\n",
      "-Validation loss after epoc: 0.5053218603134155 \n",
      "Epoch:388 \n",
      " - Batch no: 0, train loss: 0.6387124061584473\n",
      " - Batch no: 8, train loss: 0.4321558177471161\n",
      " - Batch no: 16, train loss: 0.4316365420818329\n",
      " - Batch no: 24, train loss: 0.6062629222869873\n",
      "-Validation loss after epoc: 0.5049751996994019 \n",
      "Epoch:389 \n",
      " - Batch no: 0, train loss: 0.5250325798988342\n",
      " - Batch no: 8, train loss: 0.39239776134490967\n",
      " - Batch no: 16, train loss: 0.6250319480895996\n",
      " - Batch no: 24, train loss: 0.6259725689888\n",
      "-Validation loss after epoc: 0.504781186580658 \n",
      "Epoch:390 \n",
      " - Batch no: 0, train loss: 0.44971704483032227\n",
      " - Batch no: 8, train loss: 0.5390051007270813\n",
      " - Batch no: 16, train loss: 0.620031476020813\n",
      " - Batch no: 24, train loss: 0.5153247714042664\n",
      "-Validation loss after epoc: 0.5047200322151184 \n",
      "Epoch:391 \n",
      " - Batch no: 0, train loss: 0.562201201915741\n",
      " - Batch no: 8, train loss: 0.581257700920105\n",
      " - Batch no: 16, train loss: 0.5833281874656677\n",
      " - Batch no: 24, train loss: 0.5680956244468689\n",
      "-Validation loss after epoc: 0.5047183632850647 \n",
      "Epoch:392 \n",
      " - Batch no: 0, train loss: 0.5663882493972778\n",
      " - Batch no: 8, train loss: 0.6140137910842896\n",
      " - Batch no: 16, train loss: 0.5415970683097839\n",
      " - Batch no: 24, train loss: 0.7213830351829529\n",
      "-Validation loss after epoc: 0.506044864654541 \n",
      "Epoch:393 \n",
      " - Batch no: 0, train loss: 0.7125732898712158\n",
      " - Batch no: 8, train loss: 0.5745567083358765\n",
      " - Batch no: 16, train loss: 0.5557522773742676\n",
      " - Batch no: 24, train loss: 0.4546150863170624\n",
      "-Validation loss after epoc: 0.5050492286682129 \n",
      "Epoch:394 \n",
      " - Batch no: 0, train loss: 0.5763196349143982\n",
      " - Batch no: 8, train loss: 0.5941782593727112\n",
      " - Batch no: 16, train loss: 0.6656053066253662\n",
      " - Batch no: 24, train loss: 0.5917959809303284\n",
      "-Validation loss after epoc: 0.5042413473129272 \n",
      "Epoch:395 \n",
      " - Batch no: 0, train loss: 0.5131624937057495\n",
      " - Batch no: 8, train loss: 0.4853309690952301\n",
      " - Batch no: 16, train loss: 0.5376406908035278\n",
      " - Batch no: 24, train loss: 0.4445135295391083\n",
      "-Validation loss after epoc: 0.5046271681785583 \n",
      "Epoch:396 \n",
      " - Batch no: 0, train loss: 0.5599493384361267\n",
      " - Batch no: 8, train loss: 0.6873584389686584\n",
      " - Batch no: 16, train loss: 0.462459921836853\n",
      " - Batch no: 24, train loss: 0.5489757657051086\n",
      "-Validation loss after epoc: 0.5045586228370667 \n",
      "Epoch:397 \n",
      " - Batch no: 0, train loss: 0.47699570655822754\n",
      " - Batch no: 8, train loss: 0.7749269604682922\n",
      " - Batch no: 16, train loss: 0.5269052982330322\n",
      " - Batch no: 24, train loss: 0.41003358364105225\n",
      "-Validation loss after epoc: 0.5046981573104858 \n",
      "Epoch:398 \n",
      " - Batch no: 0, train loss: 0.6568698883056641\n",
      " - Batch no: 8, train loss: 0.49633610248565674\n",
      " - Batch no: 16, train loss: 0.5339757204055786\n",
      " - Batch no: 24, train loss: 0.49027732014656067\n",
      "-Validation loss after epoc: 0.5041947960853577 \n",
      "Epoch:399 \n",
      " - Batch no: 0, train loss: 0.8225140571594238\n",
      " - Batch no: 8, train loss: 0.4474121630191803\n",
      " - Batch no: 16, train loss: 0.5325838923454285\n",
      " - Batch no: 24, train loss: 0.4544649124145508\n",
      "-Validation loss after epoc: 0.504280149936676 \n",
      "Epoch:400 \n",
      " - Batch no: 0, train loss: 0.6242368817329407\n",
      " - Batch no: 8, train loss: 0.565478503704071\n",
      " - Batch no: 16, train loss: 0.45006680488586426\n",
      " - Batch no: 24, train loss: 0.4089764654636383\n",
      "-Validation loss after epoc: 0.5037605166435242 \n",
      "Epoch:401 \n",
      " - Batch no: 0, train loss: 0.6134805679321289\n",
      " - Batch no: 8, train loss: 0.6858434677124023\n",
      " - Batch no: 16, train loss: 0.6462438106536865\n",
      " - Batch no: 24, train loss: 0.499692440032959\n",
      "-Validation loss after epoc: 0.5038145184516907 \n",
      "Epoch:402 \n",
      " - Batch no: 0, train loss: 0.37213683128356934\n",
      " - Batch no: 8, train loss: 0.5528170466423035\n",
      " - Batch no: 16, train loss: 0.5515584945678711\n",
      " - Batch no: 24, train loss: 0.47783899307250977\n",
      "-Validation loss after epoc: 0.5038846135139465 \n",
      "Epoch:403 \n",
      " - Batch no: 0, train loss: 0.5651438236236572\n",
      " - Batch no: 8, train loss: 0.6237541437149048\n",
      " - Batch no: 16, train loss: 0.5537497401237488\n",
      " - Batch no: 24, train loss: 0.5733359456062317\n",
      "-Validation loss after epoc: 0.5039767026901245 \n",
      "Epoch:404 \n",
      " - Batch no: 0, train loss: 0.565488874912262\n",
      " - Batch no: 8, train loss: 0.4217096269130707\n",
      " - Batch no: 16, train loss: 0.536300539970398\n",
      " - Batch no: 24, train loss: 0.6254289150238037\n",
      "-Validation loss after epoc: 0.5033532977104187 \n",
      "Epoch:405 \n",
      " - Batch no: 0, train loss: 0.5968917608261108\n",
      " - Batch no: 8, train loss: 0.5559431910514832\n",
      " - Batch no: 16, train loss: 0.5003471970558167\n",
      " - Batch no: 24, train loss: 0.5244592428207397\n",
      "-Validation loss after epoc: 0.5032882690429688 \n",
      "Epoch:406 \n",
      " - Batch no: 0, train loss: 0.5122079849243164\n",
      " - Batch no: 8, train loss: 0.642751157283783\n",
      " - Batch no: 16, train loss: 0.33213499188423157\n",
      " - Batch no: 24, train loss: 0.4561626613140106\n",
      "-Validation loss after epoc: 0.5030502676963806 \n",
      "Epoch:407 \n",
      " - Batch no: 0, train loss: 0.5281675457954407\n",
      " - Batch no: 8, train loss: 0.5142627358436584\n",
      " - Batch no: 16, train loss: 0.48471933603286743\n",
      " - Batch no: 24, train loss: 0.6976783275604248\n",
      "-Validation loss after epoc: 0.5032135248184204 \n",
      "Epoch:408 \n",
      " - Batch no: 0, train loss: 0.5559359788894653\n",
      " - Batch no: 8, train loss: 0.5686073303222656\n",
      " - Batch no: 16, train loss: 0.47046422958374023\n",
      " - Batch no: 24, train loss: 0.8039210438728333\n",
      "-Validation loss after epoc: 0.5035371780395508 \n",
      "Epoch:409 \n",
      " - Batch no: 0, train loss: 0.3774653971195221\n",
      " - Batch no: 8, train loss: 0.6430768966674805\n",
      " - Batch no: 16, train loss: 0.4388619363307953\n",
      " - Batch no: 24, train loss: 0.46605896949768066\n",
      "-Validation loss after epoc: 0.5050089359283447 \n",
      "Epoch:410 \n",
      " - Batch no: 0, train loss: 0.674863338470459\n",
      " - Batch no: 8, train loss: 0.6130977272987366\n",
      " - Batch no: 16, train loss: 0.5094966888427734\n",
      " - Batch no: 24, train loss: 0.563268780708313\n",
      "-Validation loss after epoc: 0.5033132433891296 \n",
      "Epoch:411 \n",
      " - Batch no: 0, train loss: 0.5182116031646729\n",
      " - Batch no: 8, train loss: 0.5555354356765747\n",
      " - Batch no: 16, train loss: 0.6001282930374146\n",
      " - Batch no: 24, train loss: 0.5036252737045288\n",
      "-Validation loss after epoc: 0.5029537677764893 \n",
      "Epoch:412 \n",
      " - Batch no: 0, train loss: 0.6037187576293945\n",
      " - Batch no: 8, train loss: 0.6167452335357666\n",
      " - Batch no: 16, train loss: 0.5797704458236694\n",
      " - Batch no: 24, train loss: 0.42065104842185974\n",
      "-Validation loss after epoc: 0.5029008388519287 \n",
      "Epoch:413 \n",
      " - Batch no: 0, train loss: 0.6887433528900146\n",
      " - Batch no: 8, train loss: 0.6144893765449524\n",
      " - Batch no: 16, train loss: 0.5775649547576904\n",
      " - Batch no: 24, train loss: 0.49425986409187317\n",
      "-Validation loss after epoc: 0.5037481188774109 \n",
      "Epoch:414 \n",
      " - Batch no: 0, train loss: 0.5598185658454895\n",
      " - Batch no: 8, train loss: 0.636685848236084\n",
      " - Batch no: 16, train loss: 0.6298054456710815\n",
      " - Batch no: 24, train loss: 0.4685533046722412\n",
      "-Validation loss after epoc: 0.5034074783325195 \n",
      "Epoch:415 \n",
      " - Batch no: 0, train loss: 0.4956398606300354\n",
      " - Batch no: 8, train loss: 0.5735145211219788\n",
      " - Batch no: 16, train loss: 0.5777599215507507\n",
      " - Batch no: 24, train loss: 0.48660561442375183\n",
      "-Validation loss after epoc: 0.5030291080474854 \n",
      "Epoch:416 \n",
      " - Batch no: 0, train loss: 0.4926019310951233\n",
      " - Batch no: 8, train loss: 0.5439472794532776\n",
      " - Batch no: 16, train loss: 0.4299081265926361\n",
      " - Batch no: 24, train loss: 0.5925976634025574\n",
      "-Validation loss after epoc: 0.5032111406326294 \n",
      "Epoch:417 \n",
      " - Batch no: 0, train loss: 0.6566411256790161\n",
      " - Batch no: 8, train loss: 0.6154816150665283\n",
      " - Batch no: 16, train loss: 0.5197367072105408\n",
      " - Batch no: 24, train loss: 0.49820414185523987\n",
      "-Validation loss after epoc: 0.5028995275497437 \n",
      "Epoch:418 \n",
      " - Batch no: 0, train loss: 0.7043964266777039\n",
      " - Batch no: 8, train loss: 0.3808230459690094\n",
      " - Batch no: 16, train loss: 0.574955403804779\n",
      " - Batch no: 24, train loss: 0.40459108352661133\n",
      "-Validation loss after epoc: 0.5033621191978455 \n",
      "Epoch:419 \n",
      " - Batch no: 0, train loss: 0.5898051857948303\n",
      " - Batch no: 8, train loss: 0.5092474222183228\n",
      " - Batch no: 16, train loss: 0.5730769038200378\n",
      " - Batch no: 24, train loss: 0.5095770955085754\n",
      "-Validation loss after epoc: 0.502571165561676 \n",
      "Epoch:420 \n",
      " - Batch no: 0, train loss: 0.6551166772842407\n",
      " - Batch no: 8, train loss: 0.8418298959732056\n",
      " - Batch no: 16, train loss: 0.4341890811920166\n",
      " - Batch no: 24, train loss: 0.557141900062561\n",
      "-Validation loss after epoc: 0.5026422739028931 \n",
      "Epoch:421 \n",
      " - Batch no: 0, train loss: 0.5077253580093384\n",
      " - Batch no: 8, train loss: 0.6763667464256287\n",
      " - Batch no: 16, train loss: 0.48991659283638\n",
      " - Batch no: 24, train loss: 0.5367572903633118\n",
      "-Validation loss after epoc: 0.5020440816879272 \n",
      "Epoch:422 \n",
      " - Batch no: 0, train loss: 0.4793683588504791\n",
      " - Batch no: 8, train loss: 0.3579447865486145\n",
      " - Batch no: 16, train loss: 0.6170386672019958\n",
      " - Batch no: 24, train loss: 0.6992089748382568\n",
      "-Validation loss after epoc: 0.5020521283149719 \n",
      "Epoch:423 \n",
      " - Batch no: 0, train loss: 0.4367592930793762\n",
      " - Batch no: 8, train loss: 0.5098127126693726\n",
      " - Batch no: 16, train loss: 0.458892285823822\n",
      " - Batch no: 24, train loss: 0.4635723829269409\n",
      "-Validation loss after epoc: 0.5026221871376038 \n",
      "Epoch:424 \n",
      " - Batch no: 0, train loss: 0.6514340043067932\n",
      " - Batch no: 8, train loss: 0.7614169120788574\n",
      " - Batch no: 16, train loss: 0.39652061462402344\n",
      " - Batch no: 24, train loss: 0.6022878885269165\n",
      "-Validation loss after epoc: 0.5018554329872131 \n",
      "Epoch:425 \n",
      " - Batch no: 0, train loss: 0.6291464567184448\n",
      " - Batch no: 8, train loss: 0.4482877850532532\n",
      " - Batch no: 16, train loss: 0.49343034625053406\n",
      " - Batch no: 24, train loss: 0.9394470453262329\n",
      "-Validation loss after epoc: 0.5016391277313232 \n",
      "Epoch:426 \n",
      " - Batch no: 0, train loss: 0.46899864077568054\n",
      " - Batch no: 8, train loss: 0.5493568778038025\n",
      " - Batch no: 16, train loss: 0.7082178592681885\n",
      " - Batch no: 24, train loss: 0.47366252541542053\n",
      "-Validation loss after epoc: 0.5022230744361877 \n",
      "Epoch:427 \n",
      " - Batch no: 0, train loss: 0.4636060297489166\n",
      " - Batch no: 8, train loss: 0.566527783870697\n",
      " - Batch no: 16, train loss: 0.6460301280021667\n",
      " - Batch no: 24, train loss: 0.7718273401260376\n",
      "-Validation loss after epoc: 0.5013813972473145 \n",
      "Epoch:428 \n",
      " - Batch no: 0, train loss: 0.5544394254684448\n",
      " - Batch no: 8, train loss: 0.44312557578086853\n",
      " - Batch no: 16, train loss: 0.40086010098457336\n",
      " - Batch no: 24, train loss: 0.5146628022193909\n",
      "-Validation loss after epoc: 0.5016602277755737 \n",
      "Epoch:429 \n",
      " - Batch no: 0, train loss: 0.6481183767318726\n",
      " - Batch no: 8, train loss: 0.5137483477592468\n",
      " - Batch no: 16, train loss: 0.5290032625198364\n",
      " - Batch no: 24, train loss: 0.5661219358444214\n",
      "-Validation loss after epoc: 0.5016763210296631 \n",
      "Epoch:430 \n",
      " - Batch no: 0, train loss: 0.5162563920021057\n",
      " - Batch no: 8, train loss: 0.7788213491439819\n",
      " - Batch no: 16, train loss: 0.39247167110443115\n",
      " - Batch no: 24, train loss: 0.6518038511276245\n",
      "-Validation loss after epoc: 0.5020473599433899 \n",
      "Epoch:431 \n",
      " - Batch no: 0, train loss: 0.5743979215621948\n",
      " - Batch no: 8, train loss: 0.6711223721504211\n",
      " - Batch no: 16, train loss: 0.35587626695632935\n",
      " - Batch no: 24, train loss: 0.49842971563339233\n",
      "-Validation loss after epoc: 0.5020745396614075 \n",
      "Epoch:432 \n",
      " - Batch no: 0, train loss: 0.6553487777709961\n",
      " - Batch no: 8, train loss: 0.6679171919822693\n",
      " - Batch no: 16, train loss: 0.4781147241592407\n",
      " - Batch no: 24, train loss: 0.4147471487522125\n",
      "-Validation loss after epoc: 0.5022781491279602 \n",
      "Epoch:433 \n",
      " - Batch no: 0, train loss: 0.4116726517677307\n",
      " - Batch no: 8, train loss: 0.5114626288414001\n",
      " - Batch no: 16, train loss: 0.5355973839759827\n",
      " - Batch no: 24, train loss: 0.6901601552963257\n",
      "-Validation loss after epoc: 0.5010019540786743 \n",
      "Epoch:434 \n",
      " - Batch no: 0, train loss: 0.5616552829742432\n",
      " - Batch no: 8, train loss: 0.6593373417854309\n",
      " - Batch no: 16, train loss: 0.4946940541267395\n",
      " - Batch no: 24, train loss: 0.3624284267425537\n",
      "-Validation loss after epoc: 0.5010334253311157 \n",
      "Epoch:435 \n",
      " - Batch no: 0, train loss: 0.5457404851913452\n",
      " - Batch no: 8, train loss: 0.5916479229927063\n",
      " - Batch no: 16, train loss: 0.5720973014831543\n",
      " - Batch no: 24, train loss: 0.515026330947876\n",
      "-Validation loss after epoc: 0.501708984375 \n",
      "Epoch:436 \n",
      " - Batch no: 0, train loss: 0.48392918705940247\n",
      " - Batch no: 8, train loss: 0.39309394359588623\n",
      " - Batch no: 16, train loss: 0.5382792353630066\n",
      " - Batch no: 24, train loss: 0.5603476166725159\n",
      "-Validation loss after epoc: 0.5010517835617065 \n",
      "Epoch:437 \n",
      " - Batch no: 0, train loss: 0.6190004348754883\n",
      " - Batch no: 8, train loss: 0.4733904004096985\n",
      " - Batch no: 16, train loss: 0.5776049494743347\n",
      " - Batch no: 24, train loss: 0.6163849234580994\n",
      "-Validation loss after epoc: 0.5016447901725769 \n",
      "Epoch:438 \n",
      " - Batch no: 0, train loss: 0.44263118505477905\n",
      " - Batch no: 8, train loss: 0.4021322429180145\n",
      " - Batch no: 16, train loss: 0.5865510702133179\n",
      " - Batch no: 24, train loss: 0.7071252465248108\n",
      "-Validation loss after epoc: 0.5011307001113892 \n",
      "Epoch:439 \n",
      " - Batch no: 0, train loss: 0.5050539374351501\n",
      " - Batch no: 8, train loss: 0.814786970615387\n",
      " - Batch no: 16, train loss: 0.5846609473228455\n",
      " - Batch no: 24, train loss: 0.3747009336948395\n",
      "-Validation loss after epoc: 0.5008016228675842 \n",
      "Epoch:440 \n",
      " - Batch no: 0, train loss: 0.5670064687728882\n",
      " - Batch no: 8, train loss: 0.4536415934562683\n",
      " - Batch no: 16, train loss: 0.47187498211860657\n",
      " - Batch no: 24, train loss: 0.5444298386573792\n",
      "-Validation loss after epoc: 0.5004910826683044 \n",
      "Epoch:441 \n",
      " - Batch no: 0, train loss: 0.5492610931396484\n",
      " - Batch no: 8, train loss: 0.5261327624320984\n",
      " - Batch no: 16, train loss: 0.4502008259296417\n",
      " - Batch no: 24, train loss: 0.7146841883659363\n",
      "-Validation loss after epoc: 0.5003141760826111 \n",
      "Epoch:442 \n",
      " - Batch no: 0, train loss: 0.6989004015922546\n",
      " - Batch no: 8, train loss: 0.5547922253608704\n",
      " - Batch no: 16, train loss: 0.5191090703010559\n",
      " - Batch no: 24, train loss: 0.4895053803920746\n",
      "-Validation loss after epoc: 0.5006749033927917 \n",
      "Epoch:443 \n",
      " - Batch no: 0, train loss: 0.43280136585235596\n",
      " - Batch no: 8, train loss: 0.5510119199752808\n",
      " - Batch no: 16, train loss: 0.5577870011329651\n",
      " - Batch no: 24, train loss: 0.5830957293510437\n",
      "-Validation loss after epoc: 0.500572144985199 \n",
      "Epoch:444 \n",
      " - Batch no: 0, train loss: 0.6495360136032104\n",
      " - Batch no: 8, train loss: 0.6544951796531677\n",
      " - Batch no: 16, train loss: 0.6060990691184998\n",
      " - Batch no: 24, train loss: 0.5126572847366333\n",
      "-Validation loss after epoc: 0.5009499788284302 \n",
      "Epoch:445 \n",
      " - Batch no: 0, train loss: 0.7604557275772095\n",
      " - Batch no: 8, train loss: 0.5545172691345215\n",
      " - Batch no: 16, train loss: 0.5797302722930908\n",
      " - Batch no: 24, train loss: 0.43938183784484863\n",
      "-Validation loss after epoc: 0.5006782412528992 \n",
      "Epoch:446 \n",
      " - Batch no: 0, train loss: 0.5010342597961426\n",
      " - Batch no: 8, train loss: 0.664861261844635\n",
      " - Batch no: 16, train loss: 0.5204389691352844\n",
      " - Batch no: 24, train loss: 0.5411586165428162\n",
      "-Validation loss after epoc: 0.5022900104522705 \n",
      "Epoch:447 \n",
      " - Batch no: 0, train loss: 0.3812612295150757\n",
      " - Batch no: 8, train loss: 0.4941004514694214\n",
      " - Batch no: 16, train loss: 0.7185779809951782\n",
      " - Batch no: 24, train loss: 0.4369347095489502\n",
      "-Validation loss after epoc: 0.5014525055885315 \n",
      "Epoch:448 \n",
      " - Batch no: 0, train loss: 0.5887366533279419\n",
      " - Batch no: 8, train loss: 0.6819595694541931\n",
      " - Batch no: 16, train loss: 0.6370458602905273\n",
      " - Batch no: 24, train loss: 0.5424384474754333\n",
      "-Validation loss after epoc: 0.5005365014076233 \n",
      "Epoch:449 \n",
      " - Batch no: 0, train loss: 0.5848109126091003\n",
      " - Batch no: 8, train loss: 0.7662371397018433\n",
      " - Batch no: 16, train loss: 0.54254150390625\n",
      " - Batch no: 24, train loss: 0.6128790378570557\n",
      "-Validation loss after epoc: 0.5005882382392883 \n",
      "Epoch:450 \n",
      " - Batch no: 0, train loss: 0.4910619556903839\n",
      " - Batch no: 8, train loss: 0.8222975730895996\n",
      " - Batch no: 16, train loss: 0.5466299653053284\n",
      " - Batch no: 24, train loss: 0.6796277761459351\n",
      "-Validation loss after epoc: 0.5010044574737549 \n",
      "Epoch:451 \n",
      " - Batch no: 0, train loss: 0.524365246295929\n",
      " - Batch no: 8, train loss: 0.6683330535888672\n",
      " - Batch no: 16, train loss: 0.44634538888931274\n",
      " - Batch no: 24, train loss: 0.5294436812400818\n",
      "-Validation loss after epoc: 0.5007303953170776 \n",
      "Epoch:452 \n",
      " - Batch no: 0, train loss: 0.626336932182312\n",
      " - Batch no: 8, train loss: 0.4592655897140503\n",
      " - Batch no: 16, train loss: 0.5629565119743347\n",
      " - Batch no: 24, train loss: 0.5339998006820679\n",
      "-Validation loss after epoc: 0.5010411739349365 \n",
      "Epoch:453 \n",
      " - Batch no: 0, train loss: 0.6796932220458984\n",
      " - Batch no: 8, train loss: 0.6243194341659546\n",
      " - Batch no: 16, train loss: 0.5411554574966431\n",
      " - Batch no: 24, train loss: 0.5589271783828735\n",
      "-Validation loss after epoc: 0.5001749396324158 \n",
      "Epoch:454 \n",
      " - Batch no: 0, train loss: 0.44944432377815247\n",
      " - Batch no: 8, train loss: 0.4330054521560669\n",
      " - Batch no: 16, train loss: 0.39739352464675903\n",
      " - Batch no: 24, train loss: 0.5622215270996094\n",
      "-Validation loss after epoc: 0.49968817830085754 \n",
      "Epoch:455 \n",
      " - Batch no: 0, train loss: 0.893190860748291\n",
      " - Batch no: 8, train loss: 0.7843603491783142\n",
      " - Batch no: 16, train loss: 0.5408431887626648\n",
      " - Batch no: 24, train loss: 0.5571939945220947\n",
      "-Validation loss after epoc: 0.5001206994056702 \n",
      "Epoch:456 \n",
      " - Batch no: 0, train loss: 0.6432598233222961\n",
      " - Batch no: 8, train loss: 0.6026471853256226\n",
      " - Batch no: 16, train loss: 0.40195736289024353\n",
      " - Batch no: 24, train loss: 0.5605648756027222\n",
      "-Validation loss after epoc: 0.5003566145896912 \n",
      "Epoch:457 \n",
      " - Batch no: 0, train loss: 0.46881482005119324\n",
      " - Batch no: 8, train loss: 0.6512052416801453\n",
      " - Batch no: 16, train loss: 0.6099706292152405\n",
      " - Batch no: 24, train loss: 0.5610858798027039\n",
      "-Validation loss after epoc: 0.5000455379486084 \n",
      "Epoch:458 \n",
      " - Batch no: 0, train loss: 0.5143933296203613\n",
      " - Batch no: 8, train loss: 0.37584665417671204\n",
      " - Batch no: 16, train loss: 0.5452984571456909\n",
      " - Batch no: 24, train loss: 0.8272958993911743\n",
      "-Validation loss after epoc: 0.4998416304588318 \n",
      "Epoch:459 \n",
      " - Batch no: 0, train loss: 0.4256274700164795\n",
      " - Batch no: 8, train loss: 0.4433126449584961\n",
      " - Batch no: 16, train loss: 0.5875651240348816\n",
      " - Batch no: 24, train loss: 0.831847608089447\n",
      "-Validation loss after epoc: 0.500365674495697 \n",
      "Epoch:460 \n",
      " - Batch no: 0, train loss: 0.41580086946487427\n",
      " - Batch no: 8, train loss: 0.6140627861022949\n",
      " - Batch no: 16, train loss: 0.5579097867012024\n",
      " - Batch no: 24, train loss: 0.5554138422012329\n",
      "-Validation loss after epoc: 0.4996945559978485 \n",
      "Epoch:461 \n",
      " - Batch no: 0, train loss: 0.570420503616333\n",
      " - Batch no: 8, train loss: 0.402835875749588\n",
      " - Batch no: 16, train loss: 0.5296909809112549\n",
      " - Batch no: 24, train loss: 0.4356020390987396\n",
      "-Validation loss after epoc: 0.4997158348560333 \n",
      "Epoch:462 \n",
      " - Batch no: 0, train loss: 0.5074512362480164\n",
      " - Batch no: 8, train loss: 0.5751512050628662\n",
      " - Batch no: 16, train loss: 0.4800056517124176\n",
      " - Batch no: 24, train loss: 0.5723357796669006\n",
      "-Validation loss after epoc: 0.49969741702079773 \n",
      "Epoch:463 \n",
      " - Batch no: 0, train loss: 0.4435262382030487\n",
      " - Batch no: 8, train loss: 0.4731232821941376\n",
      " - Batch no: 16, train loss: 0.38476693630218506\n",
      " - Batch no: 24, train loss: 0.3950420022010803\n",
      "-Validation loss after epoc: 0.49967241287231445 \n",
      "Epoch:464 \n",
      " - Batch no: 0, train loss: 0.5055221319198608\n",
      " - Batch no: 8, train loss: 0.6746369004249573\n",
      " - Batch no: 16, train loss: 0.5530431866645813\n",
      " - Batch no: 24, train loss: 0.7019528150558472\n",
      "-Validation loss after epoc: 0.4990609586238861 \n",
      "Epoch:465 \n",
      " - Batch no: 0, train loss: 0.7019457817077637\n",
      " - Batch no: 8, train loss: 0.5007078051567078\n",
      " - Batch no: 16, train loss: 0.5427854061126709\n",
      " - Batch no: 24, train loss: 0.5243931412696838\n",
      "-Validation loss after epoc: 0.49983930587768555 \n",
      "Epoch:466 \n",
      " - Batch no: 0, train loss: 0.567879319190979\n",
      " - Batch no: 8, train loss: 0.6465846300125122\n",
      " - Batch no: 16, train loss: 0.62947678565979\n",
      " - Batch no: 24, train loss: 0.4971417486667633\n",
      "-Validation loss after epoc: 0.49920716881752014 \n",
      "Epoch:467 \n",
      " - Batch no: 0, train loss: 0.5158286690711975\n",
      " - Batch no: 8, train loss: 0.4804810881614685\n",
      " - Batch no: 16, train loss: 0.4487166404724121\n",
      " - Batch no: 24, train loss: 0.7202164530754089\n",
      "-Validation loss after epoc: 0.4990560710430145 \n",
      "Epoch:468 \n",
      " - Batch no: 0, train loss: 0.35134589672088623\n",
      " - Batch no: 8, train loss: 0.6451917290687561\n",
      " - Batch no: 16, train loss: 0.4787256419658661\n",
      " - Batch no: 24, train loss: 0.40741246938705444\n",
      "-Validation loss after epoc: 0.49939215183258057 \n",
      "Epoch:469 \n",
      " - Batch no: 0, train loss: 0.6679478883743286\n",
      " - Batch no: 8, train loss: 0.3573358952999115\n",
      " - Batch no: 16, train loss: 0.6408547163009644\n",
      " - Batch no: 24, train loss: 0.6266530156135559\n",
      "-Validation loss after epoc: 0.4997689723968506 \n",
      "Epoch:470 \n",
      " - Batch no: 0, train loss: 0.5489912033081055\n",
      " - Batch no: 8, train loss: 0.4999394118785858\n",
      " - Batch no: 16, train loss: 0.5171269774436951\n",
      " - Batch no: 24, train loss: 0.38667187094688416\n",
      "-Validation loss after epoc: 0.49986138939857483 \n",
      "Epoch:471 \n",
      " - Batch no: 0, train loss: 0.6406536102294922\n",
      " - Batch no: 8, train loss: 0.6420788764953613\n",
      " - Batch no: 16, train loss: 0.49488964676856995\n",
      " - Batch no: 24, train loss: 0.5940996408462524\n",
      "-Validation loss after epoc: 0.4997628629207611 \n",
      "Epoch:472 \n",
      " - Batch no: 0, train loss: 0.4700467884540558\n",
      " - Batch no: 8, train loss: 0.6780880093574524\n",
      " - Batch no: 16, train loss: 0.5223014950752258\n",
      " - Batch no: 24, train loss: 0.5621947646141052\n",
      "-Validation loss after epoc: 0.4991768002510071 \n",
      "Epoch:473 \n",
      " - Batch no: 0, train loss: 0.5195953845977783\n",
      " - Batch no: 8, train loss: 0.41796401143074036\n",
      " - Batch no: 16, train loss: 0.6343313455581665\n",
      " - Batch no: 24, train loss: 0.7912276983261108\n",
      "-Validation loss after epoc: 0.4990738034248352 \n",
      "Epoch:474 \n",
      " - Batch no: 0, train loss: 0.7954428791999817\n",
      " - Batch no: 8, train loss: 0.7148237824440002\n",
      " - Batch no: 16, train loss: 0.49604594707489014\n",
      " - Batch no: 24, train loss: 0.4871926009654999\n",
      "-Validation loss after epoc: 0.49978476762771606 \n",
      "Epoch:475 \n",
      " - Batch no: 0, train loss: 0.45456424355506897\n",
      " - Batch no: 8, train loss: 0.48934274911880493\n",
      " - Batch no: 16, train loss: 0.48008525371551514\n",
      " - Batch no: 24, train loss: 0.6498143076896667\n",
      "-Validation loss after epoc: 0.4984643757343292 \n",
      "Epoch:476 \n",
      " - Batch no: 0, train loss: 0.630870521068573\n",
      " - Batch no: 8, train loss: 0.5298690795898438\n",
      " - Batch no: 16, train loss: 0.5802083611488342\n",
      " - Batch no: 24, train loss: 0.6529868841171265\n",
      "-Validation loss after epoc: 0.49850016832351685 \n",
      "Epoch:477 \n",
      " - Batch no: 0, train loss: 0.5877010226249695\n",
      " - Batch no: 8, train loss: 0.5267795324325562\n",
      " - Batch no: 16, train loss: 0.3712959587574005\n",
      " - Batch no: 24, train loss: 0.7430317997932434\n",
      "-Validation loss after epoc: 0.49854183197021484 \n",
      "Epoch:478 \n",
      " - Batch no: 0, train loss: 0.770010232925415\n",
      " - Batch no: 8, train loss: 0.4741622507572174\n",
      " - Batch no: 16, train loss: 0.44853469729423523\n",
      " - Batch no: 24, train loss: 0.810752272605896\n",
      "-Validation loss after epoc: 0.4986390769481659 \n",
      "Epoch:479 \n",
      " - Batch no: 0, train loss: 0.5692973732948303\n",
      " - Batch no: 8, train loss: 0.6255365014076233\n",
      " - Batch no: 16, train loss: 0.5131798982620239\n",
      " - Batch no: 24, train loss: 0.5566291809082031\n",
      "-Validation loss after epoc: 0.49848389625549316 \n",
      "Epoch:480 \n",
      " - Batch no: 0, train loss: 0.34660446643829346\n",
      " - Batch no: 8, train loss: 0.5058537125587463\n",
      " - Batch no: 16, train loss: 0.62278813123703\n",
      " - Batch no: 24, train loss: 0.5624016523361206\n",
      "-Validation loss after epoc: 0.49854326248168945 \n",
      "Epoch:481 \n",
      " - Batch no: 0, train loss: 0.3829152584075928\n",
      " - Batch no: 8, train loss: 0.48498988151550293\n",
      " - Batch no: 16, train loss: 0.7451695203781128\n",
      " - Batch no: 24, train loss: 0.6497650146484375\n",
      "-Validation loss after epoc: 0.49768102169036865 \n",
      "Epoch:482 \n",
      " - Batch no: 0, train loss: 0.6015581488609314\n",
      " - Batch no: 8, train loss: 0.7595019340515137\n",
      " - Batch no: 16, train loss: 0.4811117649078369\n",
      " - Batch no: 24, train loss: 0.5714691281318665\n",
      "-Validation loss after epoc: 0.49824151396751404 \n",
      "Epoch:483 \n",
      " - Batch no: 0, train loss: 0.5556183457374573\n",
      " - Batch no: 8, train loss: 0.5122510194778442\n",
      " - Batch no: 16, train loss: 0.7731001377105713\n",
      " - Batch no: 24, train loss: 0.366750031709671\n",
      "-Validation loss after epoc: 0.498525470495224 \n",
      "Epoch:484 \n",
      " - Batch no: 0, train loss: 0.5633074641227722\n",
      " - Batch no: 8, train loss: 0.3866575062274933\n",
      " - Batch no: 16, train loss: 0.7493138909339905\n",
      " - Batch no: 24, train loss: 0.6113653182983398\n",
      "-Validation loss after epoc: 0.4983119070529938 \n",
      "Epoch:485 \n",
      " - Batch no: 0, train loss: 0.5233862996101379\n",
      " - Batch no: 8, train loss: 0.6055389046669006\n",
      " - Batch no: 16, train loss: 0.621131956577301\n",
      " - Batch no: 24, train loss: 0.44119909405708313\n",
      "-Validation loss after epoc: 0.4981919229030609 \n",
      "Epoch:486 \n",
      " - Batch no: 0, train loss: 0.5283604264259338\n",
      " - Batch no: 8, train loss: 0.5691742897033691\n",
      " - Batch no: 16, train loss: 0.682984471321106\n",
      " - Batch no: 24, train loss: 0.5756382942199707\n",
      "-Validation loss after epoc: 0.4978087842464447 \n",
      "Epoch:487 \n",
      " - Batch no: 0, train loss: 0.46434101462364197\n",
      " - Batch no: 8, train loss: 0.4317105710506439\n",
      " - Batch no: 16, train loss: 0.5264208912849426\n",
      " - Batch no: 24, train loss: 0.5636305809020996\n",
      "-Validation loss after epoc: 0.49767500162124634 \n",
      "Epoch:488 \n",
      " - Batch no: 0, train loss: 0.5368316173553467\n",
      " - Batch no: 8, train loss: 0.7118715047836304\n",
      " - Batch no: 16, train loss: 0.6702424883842468\n",
      " - Batch no: 24, train loss: 0.48606032133102417\n",
      "-Validation loss after epoc: 0.4976266622543335 \n",
      "Epoch:489 \n",
      " - Batch no: 0, train loss: 0.5866659283638\n",
      " - Batch no: 8, train loss: 0.7109621167182922\n",
      " - Batch no: 16, train loss: 0.6676453351974487\n",
      " - Batch no: 24, train loss: 0.5666452050209045\n",
      "-Validation loss after epoc: 0.497639924287796 \n",
      "Epoch:490 \n",
      " - Batch no: 0, train loss: 0.4840872585773468\n",
      " - Batch no: 8, train loss: 0.4492429196834564\n",
      " - Batch no: 16, train loss: 0.5121065378189087\n",
      " - Batch no: 24, train loss: 0.5760554671287537\n",
      "-Validation loss after epoc: 0.49764254689216614 \n",
      "Epoch:491 \n",
      " - Batch no: 0, train loss: 0.5611166954040527\n",
      " - Batch no: 8, train loss: 0.49616166949272156\n",
      " - Batch no: 16, train loss: 0.6022536754608154\n",
      " - Batch no: 24, train loss: 0.5378567576408386\n",
      "-Validation loss after epoc: 0.4974062144756317 \n",
      "Epoch:492 \n",
      " - Batch no: 0, train loss: 0.5562967658042908\n",
      " - Batch no: 8, train loss: 0.5907553434371948\n",
      " - Batch no: 16, train loss: 0.5499176383018494\n",
      " - Batch no: 24, train loss: 0.5505730509757996\n",
      "-Validation loss after epoc: 0.4976239800453186 \n",
      "Epoch:493 \n",
      " - Batch no: 0, train loss: 0.5364845395088196\n",
      " - Batch no: 8, train loss: 0.5299561619758606\n",
      " - Batch no: 16, train loss: 0.628883421421051\n",
      " - Batch no: 24, train loss: 0.5915123820304871\n",
      "-Validation loss after epoc: 0.49800631403923035 \n",
      "Epoch:494 \n",
      " - Batch no: 0, train loss: 0.5494382977485657\n",
      " - Batch no: 8, train loss: 0.6934254765510559\n",
      " - Batch no: 16, train loss: 0.5491052269935608\n",
      " - Batch no: 24, train loss: 0.3974165916442871\n",
      "-Validation loss after epoc: 0.49762752652168274 \n",
      "Epoch:495 \n",
      " - Batch no: 0, train loss: 0.6511256098747253\n",
      " - Batch no: 8, train loss: 0.7615578770637512\n",
      " - Batch no: 16, train loss: 0.46087074279785156\n",
      " - Batch no: 24, train loss: 0.6177970767021179\n",
      "-Validation loss after epoc: 0.4974241554737091 \n",
      "Epoch:496 \n",
      " - Batch no: 0, train loss: 0.49180978536605835\n",
      " - Batch no: 8, train loss: 0.6449713706970215\n",
      " - Batch no: 16, train loss: 0.5663331747055054\n",
      " - Batch no: 24, train loss: 0.5911130309104919\n",
      "-Validation loss after epoc: 0.49786263704299927 \n",
      "Epoch:497 \n",
      " - Batch no: 0, train loss: 0.4141329526901245\n",
      " - Batch no: 8, train loss: 0.38756126165390015\n",
      " - Batch no: 16, train loss: 0.6428337097167969\n",
      " - Batch no: 24, train loss: 0.5611532926559448\n",
      "-Validation loss after epoc: 0.49762386083602905 \n",
      "Epoch:498 \n",
      " - Batch no: 0, train loss: 0.3513094484806061\n",
      " - Batch no: 8, train loss: 0.4542263150215149\n",
      " - Batch no: 16, train loss: 0.39149776101112366\n",
      " - Batch no: 24, train loss: 0.5809786319732666\n",
      "-Validation loss after epoc: 0.4979792833328247 \n",
      "Epoch:499 \n",
      " - Batch no: 0, train loss: 0.4884280264377594\n",
      " - Batch no: 8, train loss: 0.47793495655059814\n",
      " - Batch no: 16, train loss: 0.5367979407310486\n",
      " - Batch no: 24, train loss: 0.5928280353546143\n",
      "-Validation loss after epoc: 0.49771299958229065 \n",
      "Epoch:500 \n",
      " - Batch no: 0, train loss: 0.5037774443626404\n",
      " - Batch no: 8, train loss: 0.4357069134712219\n",
      " - Batch no: 16, train loss: 0.567014753818512\n",
      " - Batch no: 24, train loss: 0.5238585472106934\n",
      "-Validation loss after epoc: 0.49791401624679565 \n",
      "Epoch:501 \n",
      " - Batch no: 0, train loss: 0.47278356552124023\n",
      " - Batch no: 8, train loss: 0.7178019881248474\n",
      " - Batch no: 16, train loss: 0.5770233273506165\n",
      " - Batch no: 24, train loss: 0.5798597931861877\n",
      "-Validation loss after epoc: 0.4975745677947998 \n",
      "Epoch:502 \n",
      " - Batch no: 0, train loss: 0.5169029235839844\n",
      " - Batch no: 8, train loss: 0.5794789791107178\n",
      " - Batch no: 16, train loss: 0.47145989537239075\n",
      " - Batch no: 24, train loss: 0.5337660908699036\n",
      "-Validation loss after epoc: 0.49807822704315186 \n",
      "Epoch:503 \n",
      " - Batch no: 0, train loss: 0.6388208866119385\n",
      " - Batch no: 8, train loss: 0.4565601050853729\n",
      " - Batch no: 16, train loss: 0.5602080225944519\n",
      " - Batch no: 24, train loss: 0.6170664429664612\n",
      "-Validation loss after epoc: 0.4982869327068329 \n",
      "Epoch:504 \n",
      " - Batch no: 0, train loss: 0.5501872301101685\n",
      " - Batch no: 8, train loss: 0.72538822889328\n",
      " - Batch no: 16, train loss: 0.6611059904098511\n",
      " - Batch no: 24, train loss: 0.6213074922561646\n",
      "-Validation loss after epoc: 0.498251348733902 \n",
      "Epoch:505 \n",
      " - Batch no: 0, train loss: 0.5381383299827576\n",
      " - Batch no: 8, train loss: 0.41360798478126526\n",
      " - Batch no: 16, train loss: 0.8205503225326538\n",
      " - Batch no: 24, train loss: 0.658959686756134\n",
      "-Validation loss after epoc: 0.4976380467414856 \n",
      "Epoch:506 \n",
      " - Batch no: 0, train loss: 0.46192288398742676\n",
      " - Batch no: 8, train loss: 0.5757436156272888\n",
      " - Batch no: 16, train loss: 0.46790292859077454\n",
      " - Batch no: 24, train loss: 0.45452702045440674\n",
      "-Validation loss after epoc: 0.4981021285057068 \n",
      "Epoch:507 \n",
      " - Batch no: 0, train loss: 0.5977131724357605\n",
      " - Batch no: 8, train loss: 0.4789293706417084\n",
      " - Batch no: 16, train loss: 0.625839352607727\n",
      " - Batch no: 24, train loss: 0.4517688751220703\n",
      "-Validation loss after epoc: 0.4970593750476837 \n",
      "Epoch:508 \n",
      " - Batch no: 0, train loss: 0.5161349177360535\n",
      " - Batch no: 8, train loss: 0.6387404203414917\n",
      " - Batch no: 16, train loss: 0.5861372947692871\n",
      " - Batch no: 24, train loss: 0.6404556632041931\n",
      "-Validation loss after epoc: 0.4969501793384552 \n",
      "Epoch:509 \n",
      " - Batch no: 0, train loss: 0.4679425358772278\n",
      " - Batch no: 8, train loss: 0.5259868502616882\n",
      " - Batch no: 16, train loss: 0.5130202770233154\n",
      " - Batch no: 24, train loss: 0.5498985052108765\n",
      "-Validation loss after epoc: 0.4971780478954315 \n",
      "Epoch:510 \n",
      " - Batch no: 0, train loss: 0.6250978112220764\n",
      " - Batch no: 8, train loss: 0.39102742075920105\n",
      " - Batch no: 16, train loss: 0.5622823238372803\n",
      " - Batch no: 24, train loss: 0.5730895400047302\n",
      "-Validation loss after epoc: 0.49679386615753174 \n",
      "Epoch:511 \n",
      " - Batch no: 0, train loss: 0.5008329153060913\n",
      " - Batch no: 8, train loss: 0.4662001430988312\n",
      " - Batch no: 16, train loss: 0.6060011982917786\n",
      " - Batch no: 24, train loss: 0.4006394147872925\n",
      "-Validation loss after epoc: 0.497012197971344 \n",
      "Epoch:512 \n",
      " - Batch no: 0, train loss: 0.5022308826446533\n",
      " - Batch no: 8, train loss: 0.40975871682167053\n",
      " - Batch no: 16, train loss: 0.566291868686676\n",
      " - Batch no: 24, train loss: 0.588563859462738\n",
      "-Validation loss after epoc: 0.49687010049819946 \n",
      "Epoch:513 \n",
      " - Batch no: 0, train loss: 0.6249021887779236\n",
      " - Batch no: 8, train loss: 0.608940064907074\n",
      " - Batch no: 16, train loss: 0.7432354688644409\n",
      " - Batch no: 24, train loss: 0.5304685235023499\n",
      "-Validation loss after epoc: 0.49723562598228455 \n",
      "Epoch:514 \n",
      " - Batch no: 0, train loss: 0.6794711351394653\n",
      " - Batch no: 8, train loss: 0.5346653461456299\n",
      " - Batch no: 16, train loss: 0.5032723546028137\n",
      " - Batch no: 24, train loss: 0.6590771079063416\n",
      "-Validation loss after epoc: 0.4970664381980896 \n",
      "Epoch:515 \n",
      " - Batch no: 0, train loss: 0.5555528998374939\n",
      " - Batch no: 8, train loss: 0.6747749447822571\n",
      " - Batch no: 16, train loss: 0.47508952021598816\n",
      " - Batch no: 24, train loss: 0.5177615284919739\n",
      "-Validation loss after epoc: 0.4966931641101837 \n",
      "Epoch:516 \n",
      " - Batch no: 0, train loss: 0.49667292833328247\n",
      " - Batch no: 8, train loss: 0.5293764472007751\n",
      " - Batch no: 16, train loss: 0.5223940014839172\n",
      " - Batch no: 24, train loss: 0.7692741751670837\n",
      "-Validation loss after epoc: 0.49623870849609375 \n",
      "Epoch:517 \n",
      " - Batch no: 0, train loss: 0.5583914518356323\n",
      " - Batch no: 8, train loss: 0.5069139003753662\n",
      " - Batch no: 16, train loss: 0.6977450251579285\n",
      " - Batch no: 24, train loss: 0.4287486970424652\n",
      "-Validation loss after epoc: 0.49629542231559753 \n",
      "Epoch:518 \n",
      " - Batch no: 0, train loss: 0.6414884924888611\n",
      " - Batch no: 8, train loss: 0.5148070454597473\n",
      " - Batch no: 16, train loss: 0.7272263169288635\n",
      " - Batch no: 24, train loss: 0.7006179690361023\n",
      "-Validation loss after epoc: 0.49654433131217957 \n",
      "Epoch:519 \n",
      " - Batch no: 0, train loss: 0.4722803831100464\n",
      " - Batch no: 8, train loss: 0.4495919942855835\n",
      " - Batch no: 16, train loss: 0.6943601369857788\n",
      " - Batch no: 24, train loss: 0.38975563645362854\n",
      "-Validation loss after epoc: 0.4961608350276947 \n",
      "Epoch:520 \n",
      " - Batch no: 0, train loss: 0.6274752020835876\n",
      " - Batch no: 8, train loss: 0.5255678296089172\n",
      " - Batch no: 16, train loss: 0.5880157947540283\n",
      " - Batch no: 24, train loss: 0.5438522100448608\n",
      "-Validation loss after epoc: 0.49641671776771545 \n",
      "Epoch:521 \n",
      " - Batch no: 0, train loss: 0.5364491939544678\n",
      " - Batch no: 8, train loss: 0.6614208817481995\n",
      " - Batch no: 16, train loss: 0.3986201286315918\n",
      " - Batch no: 24, train loss: 0.6084889769554138\n",
      "-Validation loss after epoc: 0.4968115985393524 \n",
      "Epoch:522 \n",
      " - Batch no: 0, train loss: 0.5187197923660278\n",
      " - Batch no: 8, train loss: 0.5711662173271179\n",
      " - Batch no: 16, train loss: 0.4870615303516388\n",
      " - Batch no: 24, train loss: 0.4316081404685974\n",
      "-Validation loss after epoc: 0.4965377449989319 \n",
      "Epoch:523 \n",
      " - Batch no: 0, train loss: 0.553816020488739\n",
      " - Batch no: 8, train loss: 0.59825199842453\n",
      " - Batch no: 16, train loss: 0.6553578972816467\n",
      " - Batch no: 24, train loss: 0.3606288433074951\n",
      "-Validation loss after epoc: 0.49652379751205444 \n",
      "Epoch:524 \n",
      " - Batch no: 0, train loss: 0.37977591156959534\n",
      " - Batch no: 8, train loss: 0.6008060574531555\n",
      " - Batch no: 16, train loss: 0.7314274311065674\n",
      " - Batch no: 24, train loss: 0.553726315498352\n",
      "-Validation loss after epoc: 0.4964560568332672 \n",
      "Epoch:525 \n",
      " - Batch no: 0, train loss: 0.7243584990501404\n",
      " - Batch no: 8, train loss: 0.5835735201835632\n",
      " - Batch no: 16, train loss: 0.47274911403656006\n",
      " - Batch no: 24, train loss: 0.3495326638221741\n",
      "-Validation loss after epoc: 0.49634623527526855 \n",
      "Epoch:526 \n",
      " - Batch no: 0, train loss: 0.49826210737228394\n",
      " - Batch no: 8, train loss: 0.5709868669509888\n",
      " - Batch no: 16, train loss: 0.4704015851020813\n",
      " - Batch no: 24, train loss: 0.6106736063957214\n",
      "-Validation loss after epoc: 0.4964035749435425 \n",
      "Epoch:527 \n",
      " - Batch no: 0, train loss: 0.4106144607067108\n",
      " - Batch no: 8, train loss: 0.4082666337490082\n",
      " - Batch no: 16, train loss: 0.48134344816207886\n",
      " - Batch no: 24, train loss: 0.524722695350647\n",
      "-Validation loss after epoc: 0.49604061245918274 \n",
      "Epoch:528 \n",
      " - Batch no: 0, train loss: 0.5858597159385681\n",
      " - Batch no: 8, train loss: 0.5204885005950928\n",
      " - Batch no: 16, train loss: 0.7615795135498047\n",
      " - Batch no: 24, train loss: 0.6498965620994568\n",
      "-Validation loss after epoc: 0.4962463974952698 \n",
      "Epoch:529 \n",
      " - Batch no: 0, train loss: 0.4442843496799469\n",
      " - Batch no: 8, train loss: 0.49076029658317566\n",
      " - Batch no: 16, train loss: 0.47022438049316406\n",
      " - Batch no: 24, train loss: 0.588653564453125\n",
      "-Validation loss after epoc: 0.495468407869339 \n",
      "Epoch:530 \n",
      " - Batch no: 0, train loss: 0.5379481315612793\n",
      " - Batch no: 8, train loss: 0.5117443203926086\n",
      " - Batch no: 16, train loss: 0.47508835792541504\n",
      " - Batch no: 24, train loss: 0.7926163673400879\n",
      "-Validation loss after epoc: 0.49600207805633545 \n",
      "Epoch:531 \n",
      " - Batch no: 0, train loss: 0.4565563201904297\n",
      " - Batch no: 8, train loss: 0.5854247212409973\n",
      " - Batch no: 16, train loss: 0.5483841300010681\n",
      " - Batch no: 24, train loss: 0.5182080268859863\n",
      "-Validation loss after epoc: 0.4964522421360016 \n",
      "Epoch:532 \n",
      " - Batch no: 0, train loss: 0.5813168883323669\n",
      " - Batch no: 8, train loss: 0.5226296782493591\n",
      " - Batch no: 16, train loss: 0.664512574672699\n",
      " - Batch no: 24, train loss: 0.4173334538936615\n",
      "-Validation loss after epoc: 0.49640029668807983 \n",
      "Epoch:533 \n",
      " - Batch no: 0, train loss: 0.6271868348121643\n",
      " - Batch no: 8, train loss: 0.501095712184906\n",
      " - Batch no: 16, train loss: 0.6049063205718994\n",
      " - Batch no: 24, train loss: 0.5167980194091797\n",
      "-Validation loss after epoc: 0.49684736132621765 \n",
      "Epoch:534 \n",
      " - Batch no: 0, train loss: 0.48071998357772827\n",
      " - Batch no: 8, train loss: 0.5735726356506348\n",
      " - Batch no: 16, train loss: 0.5134926438331604\n",
      " - Batch no: 24, train loss: 0.5205121636390686\n",
      "-Validation loss after epoc: 0.49578896164894104 \n",
      "Epoch:535 \n",
      " - Batch no: 0, train loss: 0.5196889042854309\n",
      " - Batch no: 8, train loss: 0.4261469542980194\n",
      " - Batch no: 16, train loss: 0.6037859320640564\n",
      " - Batch no: 24, train loss: 0.49669086933135986\n",
      "-Validation loss after epoc: 0.4961632192134857 \n",
      "Epoch:536 \n",
      " - Batch no: 0, train loss: 0.656217634677887\n",
      " - Batch no: 8, train loss: 0.7156482338905334\n",
      " - Batch no: 16, train loss: 0.6713094711303711\n",
      " - Batch no: 24, train loss: 0.5852492451667786\n",
      "-Validation loss after epoc: 0.4965830445289612 \n",
      "Epoch:537 \n",
      " - Batch no: 0, train loss: 0.5217618346214294\n",
      " - Batch no: 8, train loss: 0.6532543897628784\n",
      " - Batch no: 16, train loss: 0.422948956489563\n",
      " - Batch no: 24, train loss: 0.38872718811035156\n",
      "-Validation loss after epoc: 0.49630406498908997 \n",
      "Epoch:538 \n",
      " - Batch no: 0, train loss: 0.676190197467804\n",
      " - Batch no: 8, train loss: 0.4510129690170288\n",
      " - Batch no: 16, train loss: 0.4653048515319824\n",
      " - Batch no: 24, train loss: 0.676917552947998\n",
      "-Validation loss after epoc: 0.4955504834651947 \n",
      "Epoch:539 \n",
      " - Batch no: 0, train loss: 0.5166487693786621\n",
      " - Batch no: 8, train loss: 0.4173172116279602\n",
      " - Batch no: 16, train loss: 0.4673400819301605\n",
      " - Batch no: 24, train loss: 0.5501512289047241\n",
      "-Validation loss after epoc: 0.4956735670566559 \n",
      "Epoch:540 \n",
      " - Batch no: 0, train loss: 0.44928714632987976\n",
      " - Batch no: 8, train loss: 0.5779679417610168\n",
      " - Batch no: 16, train loss: 0.5217903852462769\n",
      " - Batch no: 24, train loss: 0.8342212438583374\n",
      "-Validation loss after epoc: 0.4958401620388031 \n",
      "Epoch:541 \n",
      " - Batch no: 0, train loss: 0.37107357382774353\n",
      " - Batch no: 8, train loss: 0.5994155406951904\n",
      " - Batch no: 16, train loss: 0.4909161925315857\n",
      " - Batch no: 24, train loss: 0.6109133958816528\n",
      "-Validation loss after epoc: 0.4959460198879242 \n",
      "Epoch:542 \n",
      " - Batch no: 0, train loss: 0.39984840154647827\n",
      " - Batch no: 8, train loss: 0.43961140513420105\n",
      " - Batch no: 16, train loss: 0.6083810925483704\n",
      " - Batch no: 24, train loss: 0.5437660813331604\n",
      "-Validation loss after epoc: 0.49626460671424866 \n",
      "Epoch:543 \n",
      " - Batch no: 0, train loss: 0.643700897693634\n",
      " - Batch no: 8, train loss: 0.5438754558563232\n",
      " - Batch no: 16, train loss: 0.5839328765869141\n",
      " - Batch no: 24, train loss: 0.4950353801250458\n",
      "-Validation loss after epoc: 0.49514105916023254 \n",
      "Epoch:544 \n",
      " - Batch no: 0, train loss: 0.6076112985610962\n",
      " - Batch no: 8, train loss: 0.47425538301467896\n",
      " - Batch no: 16, train loss: 0.4913196861743927\n",
      " - Batch no: 24, train loss: 0.5009416937828064\n",
      "-Validation loss after epoc: 0.4957393407821655 \n",
      "Epoch:545 \n",
      " - Batch no: 0, train loss: 0.5495102405548096\n",
      " - Batch no: 8, train loss: 0.6301273703575134\n",
      " - Batch no: 16, train loss: 0.47730907797813416\n",
      " - Batch no: 24, train loss: 0.43983396887779236\n",
      "-Validation loss after epoc: 0.49602198600769043 \n",
      "Epoch:546 \n",
      " - Batch no: 0, train loss: 0.546191930770874\n",
      " - Batch no: 8, train loss: 0.43382346630096436\n",
      " - Batch no: 16, train loss: 0.6646192073822021\n",
      " - Batch no: 24, train loss: 0.649600625038147\n",
      "-Validation loss after epoc: 0.4958280324935913 \n",
      "Epoch:547 \n",
      " - Batch no: 0, train loss: 0.4679659307003021\n",
      " - Batch no: 8, train loss: 0.47291111946105957\n",
      " - Batch no: 16, train loss: 0.4992002844810486\n",
      " - Batch no: 24, train loss: 0.5530055165290833\n",
      "-Validation loss after epoc: 0.49529606103897095 \n",
      "Epoch:548 \n",
      " - Batch no: 0, train loss: 0.563347578048706\n",
      " - Batch no: 8, train loss: 0.47243738174438477\n",
      " - Batch no: 16, train loss: 0.4905765950679779\n",
      " - Batch no: 24, train loss: 0.5330557823181152\n",
      "-Validation loss after epoc: 0.495316743850708 \n",
      "Epoch:549 \n",
      " - Batch no: 0, train loss: 0.43690475821495056\n",
      " - Batch no: 8, train loss: 0.5160386562347412\n",
      " - Batch no: 16, train loss: 0.42503780126571655\n",
      " - Batch no: 24, train loss: 0.70136559009552\n",
      "-Validation loss after epoc: 0.4954575300216675 \n",
      "Epoch:550 \n",
      " - Batch no: 0, train loss: 0.5882726311683655\n",
      " - Batch no: 8, train loss: 0.6484121084213257\n",
      " - Batch no: 16, train loss: 0.43559175729751587\n",
      " - Batch no: 24, train loss: 0.6385415196418762\n",
      "-Validation loss after epoc: 0.4948071539402008 \n",
      "Epoch:551 \n",
      " - Batch no: 0, train loss: 0.598854660987854\n",
      " - Batch no: 8, train loss: 0.35654765367507935\n",
      " - Batch no: 16, train loss: 0.5899768471717834\n",
      " - Batch no: 24, train loss: 0.6568305492401123\n",
      "-Validation loss after epoc: 0.4948602318763733 \n",
      "Epoch:552 \n",
      " - Batch no: 0, train loss: 0.3621115982532501\n",
      " - Batch no: 8, train loss: 0.47641414403915405\n",
      " - Batch no: 16, train loss: 0.7358961701393127\n",
      " - Batch no: 24, train loss: 0.43044188618659973\n",
      "-Validation loss after epoc: 0.49506980180740356 \n",
      "Epoch:553 \n",
      " - Batch no: 0, train loss: 0.5033568143844604\n",
      " - Batch no: 8, train loss: 0.6888395547866821\n",
      " - Batch no: 16, train loss: 0.5260058641433716\n",
      " - Batch no: 24, train loss: 0.5273406505584717\n",
      "-Validation loss after epoc: 0.4949571490287781 \n",
      "Epoch:554 \n",
      " - Batch no: 0, train loss: 0.6502624154090881\n",
      " - Batch no: 8, train loss: 0.4664205014705658\n",
      " - Batch no: 16, train loss: 0.6247153282165527\n",
      " - Batch no: 24, train loss: 0.4813372492790222\n",
      "-Validation loss after epoc: 0.4952457845211029 \n",
      "Epoch:555 \n",
      " - Batch no: 0, train loss: 0.6101582050323486\n",
      " - Batch no: 8, train loss: 0.4542730450630188\n",
      " - Batch no: 16, train loss: 0.6181641221046448\n",
      " - Batch no: 24, train loss: 0.4288829267024994\n",
      "-Validation loss after epoc: 0.49522078037261963 \n",
      "Epoch:556 \n",
      " - Batch no: 0, train loss: 0.599819540977478\n",
      " - Batch no: 8, train loss: 0.49408772587776184\n",
      " - Batch no: 16, train loss: 0.5392016172409058\n",
      " - Batch no: 24, train loss: 0.7638995051383972\n",
      "-Validation loss after epoc: 0.4955179691314697 \n",
      "Epoch:557 \n",
      " - Batch no: 0, train loss: 0.47551068663597107\n",
      " - Batch no: 8, train loss: 0.6409551501274109\n",
      " - Batch no: 16, train loss: 0.3542701303958893\n",
      " - Batch no: 24, train loss: 0.6728734970092773\n",
      "-Validation loss after epoc: 0.4952954947948456 \n",
      "Epoch:558 \n",
      " - Batch no: 0, train loss: 0.4745667576789856\n",
      " - Batch no: 8, train loss: 0.4680825173854828\n",
      " - Batch no: 16, train loss: 0.524006724357605\n",
      " - Batch no: 24, train loss: 0.7262078523635864\n",
      "-Validation loss after epoc: 0.49499550461769104 \n",
      "Epoch:559 \n",
      " - Batch no: 0, train loss: 0.5655321478843689\n",
      " - Batch no: 8, train loss: 0.5038710832595825\n",
      " - Batch no: 16, train loss: 0.6872900724411011\n",
      " - Batch no: 24, train loss: 0.45425134897232056\n",
      "-Validation loss after epoc: 0.4952336251735687 \n",
      "Epoch:560 \n",
      " - Batch no: 0, train loss: 0.4684617817401886\n",
      " - Batch no: 8, train loss: 0.5454837679862976\n",
      " - Batch no: 16, train loss: 0.5855143666267395\n",
      " - Batch no: 24, train loss: 0.8160237669944763\n",
      "-Validation loss after epoc: 0.4950691759586334 \n",
      "Epoch:561 \n",
      " - Batch no: 0, train loss: 0.35853731632232666\n",
      " - Batch no: 8, train loss: 0.565500020980835\n",
      " - Batch no: 16, train loss: 0.5150191187858582\n",
      " - Batch no: 24, train loss: 0.66036456823349\n",
      "-Validation loss after epoc: 0.4962378740310669 \n",
      "Epoch:562 \n",
      " - Batch no: 0, train loss: 0.5175383687019348\n",
      " - Batch no: 8, train loss: 0.42083823680877686\n",
      " - Batch no: 16, train loss: 0.4596148133277893\n",
      " - Batch no: 24, train loss: 0.8298784494400024\n",
      "-Validation loss after epoc: 0.49470651149749756 \n",
      "Epoch:563 \n",
      " - Batch no: 0, train loss: 0.5556293725967407\n",
      " - Batch no: 8, train loss: 0.4109795093536377\n",
      " - Batch no: 16, train loss: 0.4615354537963867\n",
      " - Batch no: 24, train loss: 0.7473233342170715\n",
      "-Validation loss after epoc: 0.4952615797519684 \n",
      "Epoch:564 \n",
      " - Batch no: 0, train loss: 0.5788043737411499\n",
      " - Batch no: 8, train loss: 0.5363929271697998\n",
      " - Batch no: 16, train loss: 0.6669037342071533\n",
      " - Batch no: 24, train loss: 0.5934747457504272\n",
      "-Validation loss after epoc: 0.49513232707977295 \n",
      "Epoch:565 \n",
      " - Batch no: 0, train loss: 0.5944667458534241\n",
      " - Batch no: 8, train loss: 0.5028899908065796\n",
      " - Batch no: 16, train loss: 0.7157982587814331\n",
      " - Batch no: 24, train loss: 0.3428097069263458\n",
      "-Validation loss after epoc: 0.49502772092819214 \n",
      "Epoch:566 \n",
      " - Batch no: 0, train loss: 0.41534024477005005\n",
      " - Batch no: 8, train loss: 0.46688714623451233\n",
      " - Batch no: 16, train loss: 0.6595304608345032\n",
      " - Batch no: 24, train loss: 0.6275790929794312\n",
      "-Validation loss after epoc: 0.49455004930496216 \n",
      "Epoch:567 \n",
      " - Batch no: 0, train loss: 0.37652671337127686\n",
      " - Batch no: 8, train loss: 0.5287826657295227\n",
      " - Batch no: 16, train loss: 0.7079704403877258\n",
      " - Batch no: 24, train loss: 0.6146536469459534\n",
      "-Validation loss after epoc: 0.4947137236595154 \n",
      "Epoch:568 \n",
      " - Batch no: 0, train loss: 0.6526679396629333\n",
      " - Batch no: 8, train loss: 0.7180908918380737\n",
      " - Batch no: 16, train loss: 0.5144569873809814\n",
      " - Batch no: 24, train loss: 0.5401410460472107\n",
      "-Validation loss after epoc: 0.4949757754802704 \n",
      "Epoch:569 \n",
      " - Batch no: 0, train loss: 0.3324985206127167\n",
      " - Batch no: 8, train loss: 0.5663543939590454\n",
      " - Batch no: 16, train loss: 0.5382040739059448\n",
      " - Batch no: 24, train loss: 0.38865089416503906\n",
      "-Validation loss after epoc: 0.4940241277217865 \n",
      "Epoch:570 \n",
      " - Batch no: 0, train loss: 0.49435070157051086\n",
      " - Batch no: 8, train loss: 0.44139158725738525\n",
      " - Batch no: 16, train loss: 0.9092324376106262\n",
      " - Batch no: 24, train loss: 0.5901417136192322\n",
      "-Validation loss after epoc: 0.4944395124912262 \n",
      "Epoch:571 \n",
      " - Batch no: 0, train loss: 0.5491982102394104\n",
      " - Batch no: 8, train loss: 0.722481369972229\n",
      " - Batch no: 16, train loss: 0.4962679147720337\n",
      " - Batch no: 24, train loss: 0.7572516202926636\n",
      "-Validation loss after epoc: 0.49448367953300476 \n",
      "Epoch:572 \n",
      " - Batch no: 0, train loss: 0.6064772009849548\n",
      " - Batch no: 8, train loss: 0.5795924663543701\n",
      " - Batch no: 16, train loss: 0.5444597005844116\n",
      " - Batch no: 24, train loss: 0.5042059421539307\n",
      "-Validation loss after epoc: 0.4939509332180023 \n",
      "Epoch:573 \n",
      " - Batch no: 0, train loss: 0.5324337482452393\n",
      " - Batch no: 8, train loss: 0.5036134123802185\n",
      " - Batch no: 16, train loss: 0.5991390943527222\n",
      " - Batch no: 24, train loss: 0.547851026058197\n",
      "-Validation loss after epoc: 0.4946088492870331 \n",
      "Epoch:574 \n",
      " - Batch no: 0, train loss: 0.6763637065887451\n",
      " - Batch no: 8, train loss: 0.5453874468803406\n",
      " - Batch no: 16, train loss: 0.5066453218460083\n",
      " - Batch no: 24, train loss: 0.4951750338077545\n",
      "-Validation loss after epoc: 0.4939761757850647 \n",
      "Epoch:575 \n",
      " - Batch no: 0, train loss: 0.6088982224464417\n",
      " - Batch no: 8, train loss: 0.4746377766132355\n",
      " - Batch no: 16, train loss: 0.5519802570343018\n",
      " - Batch no: 24, train loss: 0.5656212568283081\n",
      "-Validation loss after epoc: 0.4942038357257843 \n",
      "Epoch:576 \n",
      " - Batch no: 0, train loss: 0.5873416066169739\n",
      " - Batch no: 8, train loss: 0.5099900364875793\n",
      " - Batch no: 16, train loss: 0.6243106722831726\n",
      " - Batch no: 24, train loss: 0.5593670606613159\n",
      "-Validation loss after epoc: 0.49573197960853577 \n",
      "Epoch:577 \n",
      " - Batch no: 0, train loss: 0.6657876968383789\n",
      " - Batch no: 8, train loss: 0.42063671350479126\n",
      " - Batch no: 16, train loss: 0.5957047343254089\n",
      " - Batch no: 24, train loss: 0.5216177105903625\n",
      "-Validation loss after epoc: 0.49503767490386963 \n",
      "Epoch:578 \n",
      " - Batch no: 0, train loss: 0.6283782124519348\n",
      " - Batch no: 8, train loss: 0.4819849729537964\n",
      " - Batch no: 16, train loss: 0.5003747344017029\n",
      " - Batch no: 24, train loss: 0.3640018701553345\n",
      "-Validation loss after epoc: 0.49463894963264465 \n",
      "Epoch:579 \n",
      " - Batch no: 0, train loss: 0.6914291977882385\n",
      " - Batch no: 8, train loss: 0.6751347184181213\n",
      " - Batch no: 16, train loss: 0.5628095865249634\n",
      " - Batch no: 24, train loss: 0.39830082654953003\n",
      "-Validation loss after epoc: 0.4946541488170624 \n",
      "Epoch:580 \n",
      " - Batch no: 0, train loss: 0.5251265168190002\n",
      " - Batch no: 8, train loss: 0.39710742235183716\n",
      " - Batch no: 16, train loss: 0.5958846211433411\n",
      " - Batch no: 24, train loss: 0.5789849758148193\n",
      "-Validation loss after epoc: 0.49410179257392883 \n",
      "Epoch:581 \n",
      " - Batch no: 0, train loss: 0.47417429089546204\n",
      " - Batch no: 8, train loss: 0.7091994285583496\n",
      " - Batch no: 16, train loss: 0.4358658790588379\n",
      " - Batch no: 24, train loss: 0.3813624978065491\n",
      "-Validation loss after epoc: 0.49376916885375977 \n",
      "Epoch:582 \n",
      " - Batch no: 0, train loss: 0.6866582036018372\n",
      " - Batch no: 8, train loss: 0.549490749835968\n",
      " - Batch no: 16, train loss: 0.6771978139877319\n",
      " - Batch no: 24, train loss: 0.5390875339508057\n",
      "-Validation loss after epoc: 0.49415645003318787 \n",
      "Epoch:583 \n",
      " - Batch no: 0, train loss: 0.5352115035057068\n",
      " - Batch no: 8, train loss: 0.6158707737922668\n",
      " - Batch no: 16, train loss: 0.42456015944480896\n",
      " - Batch no: 24, train loss: 0.6087313890457153\n",
      "-Validation loss after epoc: 0.4937823414802551 \n",
      "Epoch:584 \n",
      " - Batch no: 0, train loss: 0.42695316672325134\n",
      " - Batch no: 8, train loss: 0.5143895745277405\n",
      " - Batch no: 16, train loss: 0.509999692440033\n",
      " - Batch no: 24, train loss: 0.39728230237960815\n",
      "-Validation loss after epoc: 0.49417823553085327 \n",
      "Epoch:585 \n",
      " - Batch no: 0, train loss: 0.6407783627510071\n",
      " - Batch no: 8, train loss: 0.41700783371925354\n",
      " - Batch no: 16, train loss: 0.637492299079895\n",
      " - Batch no: 24, train loss: 0.8005366325378418\n",
      "-Validation loss after epoc: 0.49385392665863037 \n",
      "Epoch:586 \n",
      " - Batch no: 0, train loss: 0.6121915578842163\n",
      " - Batch no: 8, train loss: 0.6686527132987976\n",
      " - Batch no: 16, train loss: 0.36837542057037354\n",
      " - Batch no: 24, train loss: 0.5065211057662964\n",
      "-Validation loss after epoc: 0.4936250150203705 \n",
      "Epoch:587 \n",
      " - Batch no: 0, train loss: 0.820024847984314\n",
      " - Batch no: 8, train loss: 0.39864617586135864\n",
      " - Batch no: 16, train loss: 0.6061477661132812\n",
      " - Batch no: 24, train loss: 0.4955385625362396\n",
      "-Validation loss after epoc: 0.4956388473510742 \n",
      "Epoch:588 \n",
      " - Batch no: 0, train loss: 0.5591830611228943\n",
      " - Batch no: 8, train loss: 0.6948472261428833\n",
      " - Batch no: 16, train loss: 0.5834478735923767\n",
      " - Batch no: 24, train loss: 0.6002521514892578\n",
      "-Validation loss after epoc: 0.4933316707611084 \n",
      "Epoch:589 \n",
      " - Batch no: 0, train loss: 0.8468461036682129\n",
      " - Batch no: 8, train loss: 0.5361592769622803\n",
      " - Batch no: 16, train loss: 0.4738383889198303\n",
      " - Batch no: 24, train loss: 0.536399245262146\n",
      "-Validation loss after epoc: 0.4937798082828522 \n",
      "Epoch:590 \n",
      " - Batch no: 0, train loss: 0.48061811923980713\n",
      " - Batch no: 8, train loss: 0.47342830896377563\n",
      " - Batch no: 16, train loss: 0.7215918302536011\n",
      " - Batch no: 24, train loss: 0.7423985600471497\n",
      "-Validation loss after epoc: 0.4936062693595886 \n",
      "Epoch:591 \n",
      " - Batch no: 0, train loss: 0.5905300974845886\n",
      " - Batch no: 8, train loss: 0.49630653858184814\n",
      " - Batch no: 16, train loss: 0.3723715543746948\n",
      " - Batch no: 24, train loss: 0.45769253373146057\n",
      "-Validation loss after epoc: 0.4930555820465088 \n",
      "Epoch:592 \n",
      " - Batch no: 0, train loss: 0.6525416374206543\n",
      " - Batch no: 8, train loss: 0.4996075928211212\n",
      " - Batch no: 16, train loss: 0.651678204536438\n",
      " - Batch no: 24, train loss: 0.47141051292419434\n",
      "-Validation loss after epoc: 0.4933386743068695 \n",
      "Epoch:593 \n",
      " - Batch no: 0, train loss: 0.6286363005638123\n",
      " - Batch no: 8, train loss: 0.5551646947860718\n",
      " - Batch no: 16, train loss: 0.5913927555084229\n",
      " - Batch no: 24, train loss: 0.5015210509300232\n",
      "-Validation loss after epoc: 0.49289846420288086 \n",
      "Epoch:594 \n",
      " - Batch no: 0, train loss: 0.5479836463928223\n",
      " - Batch no: 8, train loss: 0.5191868543624878\n",
      " - Batch no: 16, train loss: 0.5306815505027771\n",
      " - Batch no: 24, train loss: 0.509140133857727\n",
      "-Validation loss after epoc: 0.49339085817337036 \n",
      "Epoch:595 \n",
      " - Batch no: 0, train loss: 0.5209776163101196\n",
      " - Batch no: 8, train loss: 0.46232524514198303\n",
      " - Batch no: 16, train loss: 0.4890742599964142\n",
      " - Batch no: 24, train loss: 0.5784475803375244\n",
      "-Validation loss after epoc: 0.4928496181964874 \n",
      "Epoch:596 \n",
      " - Batch no: 0, train loss: 0.4350203573703766\n",
      " - Batch no: 8, train loss: 0.7001489996910095\n",
      " - Batch no: 16, train loss: 0.41027069091796875\n",
      " - Batch no: 24, train loss: 0.6331145763397217\n",
      "-Validation loss after epoc: 0.49284666776657104 \n",
      "Epoch:597 \n",
      " - Batch no: 0, train loss: 0.5971102714538574\n",
      " - Batch no: 8, train loss: 0.5228854417800903\n",
      " - Batch no: 16, train loss: 0.6237707734107971\n",
      " - Batch no: 24, train loss: 0.3813551366329193\n",
      "-Validation loss after epoc: 0.49304917454719543 \n",
      "Epoch:598 \n",
      " - Batch no: 0, train loss: 0.5538039803504944\n",
      " - Batch no: 8, train loss: 0.6043963432312012\n",
      " - Batch no: 16, train loss: 0.5290496349334717\n",
      " - Batch no: 24, train loss: 0.5862188935279846\n",
      "-Validation loss after epoc: 0.4926585257053375 \n",
      "Epoch:599 \n",
      " - Batch no: 0, train loss: 0.5651441812515259\n",
      " - Batch no: 8, train loss: 0.40861713886260986\n",
      " - Batch no: 16, train loss: 0.3680591881275177\n",
      " - Batch no: 24, train loss: 0.48195621371269226\n",
      "-Validation loss after epoc: 0.49288037419319153 \n",
      "Epoch:600 \n",
      " - Batch no: 0, train loss: 0.6843119263648987\n",
      " - Batch no: 8, train loss: 0.5355749726295471\n",
      " - Batch no: 16, train loss: 0.5743227005004883\n",
      " - Batch no: 24, train loss: 0.5067105889320374\n",
      "-Validation loss after epoc: 0.49508753418922424 \n",
      "Epoch:601 \n",
      " - Batch no: 0, train loss: 0.5716993808746338\n",
      " - Batch no: 8, train loss: 0.6157575249671936\n",
      " - Batch no: 16, train loss: 0.43396759033203125\n",
      " - Batch no: 24, train loss: 0.5939168930053711\n",
      "-Validation loss after epoc: 0.49286583065986633 \n",
      "Epoch:602 \n",
      " - Batch no: 0, train loss: 0.49658510088920593\n",
      " - Batch no: 8, train loss: 0.4891692101955414\n",
      " - Batch no: 16, train loss: 0.5483991503715515\n",
      " - Batch no: 24, train loss: 0.39407891035079956\n",
      "-Validation loss after epoc: 0.4928828477859497 \n",
      "Epoch:603 \n",
      " - Batch no: 0, train loss: 0.5243198275566101\n",
      " - Batch no: 8, train loss: 0.792998194694519\n",
      " - Batch no: 16, train loss: 0.6114035248756409\n",
      " - Batch no: 24, train loss: 0.3788832128047943\n",
      "-Validation loss after epoc: 0.49219945073127747 \n",
      "Epoch:604 \n",
      " - Batch no: 0, train loss: 0.7164961695671082\n",
      " - Batch no: 8, train loss: 0.39062145352363586\n",
      " - Batch no: 16, train loss: 0.8571892976760864\n",
      " - Batch no: 24, train loss: 0.4601989686489105\n",
      "-Validation loss after epoc: 0.4922913610935211 \n",
      "Epoch:605 \n",
      " - Batch no: 0, train loss: 0.5138670802116394\n",
      " - Batch no: 8, train loss: 0.6697897911071777\n",
      " - Batch no: 16, train loss: 0.38712355494499207\n",
      " - Batch no: 24, train loss: 0.4137496054172516\n",
      "-Validation loss after epoc: 0.4934082329273224 \n",
      "Epoch:606 \n",
      " - Batch no: 0, train loss: 0.54002845287323\n",
      " - Batch no: 8, train loss: 0.5979039072990417\n",
      " - Batch no: 16, train loss: 0.36408481001853943\n",
      " - Batch no: 24, train loss: 0.46273285150527954\n",
      "-Validation loss after epoc: 0.4929100573062897 \n",
      "Epoch:607 \n",
      " - Batch no: 0, train loss: 0.6729797720909119\n",
      " - Batch no: 8, train loss: 0.6206676363945007\n",
      " - Batch no: 16, train loss: 0.4160166084766388\n",
      " - Batch no: 24, train loss: 0.6139933466911316\n",
      "-Validation loss after epoc: 0.493309885263443 \n",
      "Epoch:608 \n",
      " - Batch no: 0, train loss: 0.45828571915626526\n",
      " - Batch no: 8, train loss: 0.5507397651672363\n",
      " - Batch no: 16, train loss: 0.7314879298210144\n",
      " - Batch no: 24, train loss: 0.62674480676651\n",
      "-Validation loss after epoc: 0.49235281348228455 \n",
      "Epoch:609 \n",
      " - Batch no: 0, train loss: 0.6431711912155151\n",
      " - Batch no: 8, train loss: 0.5701083540916443\n",
      " - Batch no: 16, train loss: 0.5029943585395813\n",
      " - Batch no: 24, train loss: 0.45531532168388367\n",
      "-Validation loss after epoc: 0.4925834536552429 \n",
      "Epoch:610 \n",
      " - Batch no: 0, train loss: 0.5113620758056641\n",
      " - Batch no: 8, train loss: 0.42555326223373413\n",
      " - Batch no: 16, train loss: 0.5807688236236572\n",
      " - Batch no: 24, train loss: 0.604751467704773\n",
      "-Validation loss after epoc: 0.4918439984321594 \n",
      "Epoch:611 \n",
      " - Batch no: 0, train loss: 0.7715227603912354\n",
      " - Batch no: 8, train loss: 0.6787003874778748\n",
      " - Batch no: 16, train loss: 0.5654087066650391\n",
      " - Batch no: 24, train loss: 0.43524307012557983\n",
      "-Validation loss after epoc: 0.4917765259742737 \n",
      "Epoch:612 \n",
      " - Batch no: 0, train loss: 0.3444327712059021\n",
      " - Batch no: 8, train loss: 0.578113853931427\n",
      " - Batch no: 16, train loss: 0.5433511734008789\n",
      " - Batch no: 24, train loss: 0.5328390002250671\n",
      "-Validation loss after epoc: 0.491509348154068 \n",
      "Epoch:613 \n",
      " - Batch no: 0, train loss: 0.49612680077552795\n",
      " - Batch no: 8, train loss: 0.5581341981887817\n",
      " - Batch no: 16, train loss: 0.41380995512008667\n",
      " - Batch no: 24, train loss: 0.4465494453907013\n",
      "-Validation loss after epoc: 0.49175384640693665 \n",
      "Epoch:614 \n",
      " - Batch no: 0, train loss: 0.5753837823867798\n",
      " - Batch no: 8, train loss: 0.6195279359817505\n",
      " - Batch no: 16, train loss: 0.4815220534801483\n",
      " - Batch no: 24, train loss: 0.518481969833374\n",
      "-Validation loss after epoc: 0.49148795008659363 \n",
      "Epoch:615 \n",
      " - Batch no: 0, train loss: 0.5820411443710327\n",
      " - Batch no: 8, train loss: 0.5997619032859802\n",
      " - Batch no: 16, train loss: 0.7274181246757507\n",
      " - Batch no: 24, train loss: 0.546886146068573\n",
      "-Validation loss after epoc: 0.49170157313346863 \n",
      "Epoch:616 \n",
      " - Batch no: 0, train loss: 0.6247983574867249\n",
      " - Batch no: 8, train loss: 0.5024427771568298\n",
      " - Batch no: 16, train loss: 0.48641151189804077\n",
      " - Batch no: 24, train loss: 0.41165876388549805\n",
      "-Validation loss after epoc: 0.4911802113056183 \n",
      "Epoch:617 \n",
      " - Batch no: 0, train loss: 0.4570520520210266\n",
      " - Batch no: 8, train loss: 0.5849630832672119\n",
      " - Batch no: 16, train loss: 0.5126401782035828\n",
      " - Batch no: 24, train loss: 0.562367856502533\n",
      "-Validation loss after epoc: 0.49134859442710876 \n",
      "Epoch:618 \n",
      " - Batch no: 0, train loss: 0.5084050297737122\n",
      " - Batch no: 8, train loss: 0.49996718764305115\n",
      " - Batch no: 16, train loss: 0.37435057759284973\n",
      " - Batch no: 24, train loss: 0.45173877477645874\n",
      "-Validation loss after epoc: 0.4913862943649292 \n",
      "Epoch:619 \n",
      " - Batch no: 0, train loss: 0.6132038235664368\n",
      " - Batch no: 8, train loss: 0.4486691653728485\n",
      " - Batch no: 16, train loss: 0.5925105214118958\n",
      " - Batch no: 24, train loss: 0.49682343006134033\n",
      "-Validation loss after epoc: 0.4912915825843811 \n",
      "Epoch:620 \n",
      " - Batch no: 0, train loss: 0.5364490747451782\n",
      " - Batch no: 8, train loss: 0.39428985118865967\n",
      " - Batch no: 16, train loss: 0.6270835399627686\n",
      " - Batch no: 24, train loss: 0.4168573021888733\n",
      "-Validation loss after epoc: 0.49175888299942017 \n",
      "Epoch:621 \n",
      " - Batch no: 0, train loss: 0.5681079030036926\n",
      " - Batch no: 8, train loss: 0.622628390789032\n",
      " - Batch no: 16, train loss: 0.44257986545562744\n",
      " - Batch no: 24, train loss: 0.6269515156745911\n",
      "-Validation loss after epoc: 0.4913924038410187 \n",
      "Epoch:622 \n",
      " - Batch no: 0, train loss: 0.575843870639801\n",
      " - Batch no: 8, train loss: 0.6538379192352295\n",
      " - Batch no: 16, train loss: 0.5775150656700134\n",
      " - Batch no: 24, train loss: 0.549285888671875\n",
      "-Validation loss after epoc: 0.491226464509964 \n",
      "Epoch:623 \n",
      " - Batch no: 0, train loss: 0.5190445184707642\n",
      " - Batch no: 8, train loss: 0.8028891682624817\n",
      " - Batch no: 16, train loss: 0.5358916521072388\n",
      " - Batch no: 24, train loss: 0.6759129166603088\n",
      "-Validation loss after epoc: 0.4918650984764099 \n",
      "Epoch:624 \n",
      " - Batch no: 0, train loss: 0.7179927825927734\n",
      " - Batch no: 8, train loss: 0.6107181906700134\n",
      " - Batch no: 16, train loss: 0.5652691125869751\n",
      " - Batch no: 24, train loss: 0.43239086866378784\n",
      "-Validation loss after epoc: 0.49108535051345825 \n",
      "Epoch:625 \n",
      " - Batch no: 0, train loss: 0.6415615081787109\n",
      " - Batch no: 8, train loss: 0.7178954482078552\n",
      " - Batch no: 16, train loss: 0.4200999140739441\n",
      " - Batch no: 24, train loss: 0.657069981098175\n",
      "-Validation loss after epoc: 0.4906467795372009 \n",
      "Epoch:626 \n",
      " - Batch no: 0, train loss: 0.5611637234687805\n",
      " - Batch no: 8, train loss: 0.5527615547180176\n",
      " - Batch no: 16, train loss: 0.7405966520309448\n",
      " - Batch no: 24, train loss: 0.49313536286354065\n",
      "-Validation loss after epoc: 0.49062320590019226 \n",
      "Epoch:627 \n",
      " - Batch no: 0, train loss: 0.6264163851737976\n",
      " - Batch no: 8, train loss: 0.6982364654541016\n",
      " - Batch no: 16, train loss: 0.8136396408081055\n",
      " - Batch no: 24, train loss: 0.422977477312088\n",
      "-Validation loss after epoc: 0.4907818138599396 \n",
      "Epoch:628 \n",
      " - Batch no: 0, train loss: 0.6706657409667969\n",
      " - Batch no: 8, train loss: 0.468966543674469\n",
      " - Batch no: 16, train loss: 0.45006415247917175\n",
      " - Batch no: 24, train loss: 0.5957607626914978\n",
      "-Validation loss after epoc: 0.4905264377593994 \n",
      "Epoch:629 \n",
      " - Batch no: 0, train loss: 0.47877752780914307\n",
      " - Batch no: 8, train loss: 0.4670492708683014\n",
      " - Batch no: 16, train loss: 0.5131465196609497\n",
      " - Batch no: 24, train loss: 0.6727514266967773\n",
      "-Validation loss after epoc: 0.490374356508255 \n",
      "Epoch:630 \n",
      " - Batch no: 0, train loss: 0.6491328477859497\n",
      " - Batch no: 8, train loss: 0.3588821589946747\n",
      " - Batch no: 16, train loss: 0.5125331282615662\n",
      " - Batch no: 24, train loss: 0.5443419218063354\n",
      "-Validation loss after epoc: 0.49052244424819946 \n",
      "Epoch:631 \n",
      " - Batch no: 0, train loss: 0.6834345459938049\n",
      " - Batch no: 8, train loss: 0.42365941405296326\n",
      " - Batch no: 16, train loss: 0.7106199264526367\n",
      " - Batch no: 24, train loss: 0.5078288316726685\n",
      "-Validation loss after epoc: 0.49070248007774353 \n",
      "Epoch:632 \n",
      " - Batch no: 0, train loss: 0.3738899827003479\n",
      " - Batch no: 8, train loss: 0.4195811152458191\n",
      " - Batch no: 16, train loss: 0.6587926745414734\n",
      " - Batch no: 24, train loss: 0.592103123664856\n",
      "-Validation loss after epoc: 0.49119508266448975 \n",
      "Epoch:633 \n",
      " - Batch no: 0, train loss: 0.5175495147705078\n",
      " - Batch no: 8, train loss: 0.5657668113708496\n",
      " - Batch no: 16, train loss: 0.7768777012825012\n",
      " - Batch no: 24, train loss: 0.6216224431991577\n",
      "-Validation loss after epoc: 0.49017319083213806 \n",
      "Epoch:634 \n",
      " - Batch no: 0, train loss: 0.6480867266654968\n",
      " - Batch no: 8, train loss: 0.6597124934196472\n",
      " - Batch no: 16, train loss: 0.5539146065711975\n",
      " - Batch no: 24, train loss: 0.5319028496742249\n",
      "-Validation loss after epoc: 0.4904281198978424 \n",
      "Epoch:635 \n",
      " - Batch no: 0, train loss: 0.5902965664863586\n",
      " - Batch no: 8, train loss: 0.642870306968689\n",
      " - Batch no: 16, train loss: 0.39901918172836304\n",
      " - Batch no: 24, train loss: 0.5035182237625122\n",
      "-Validation loss after epoc: 0.4900902211666107 \n",
      "Epoch:636 \n",
      " - Batch no: 0, train loss: 0.6623059511184692\n",
      " - Batch no: 8, train loss: 0.5909743905067444\n",
      " - Batch no: 16, train loss: 0.43355634808540344\n",
      " - Batch no: 24, train loss: 0.6113003492355347\n",
      "-Validation loss after epoc: 0.4899892210960388 \n",
      "Epoch:637 \n",
      " - Batch no: 0, train loss: 0.5954054594039917\n",
      " - Batch no: 8, train loss: 0.5454578995704651\n",
      " - Batch no: 16, train loss: 0.542725145816803\n",
      " - Batch no: 24, train loss: 0.43232646584510803\n",
      "-Validation loss after epoc: 0.4905015826225281 \n",
      "Epoch:638 \n",
      " - Batch no: 0, train loss: 0.4831116497516632\n",
      " - Batch no: 8, train loss: 0.46889010071754456\n",
      " - Batch no: 16, train loss: 0.5027520060539246\n",
      " - Batch no: 24, train loss: 0.5729058980941772\n",
      "-Validation loss after epoc: 0.49093395471572876 \n",
      "Epoch:639 \n",
      " - Batch no: 0, train loss: 0.5191618800163269\n",
      " - Batch no: 8, train loss: 0.5242993235588074\n",
      " - Batch no: 16, train loss: 0.517296552658081\n",
      " - Batch no: 24, train loss: 0.3390379846096039\n",
      "-Validation loss after epoc: 0.49094799160957336 \n",
      "Epoch:640 \n",
      " - Batch no: 0, train loss: 0.8371449112892151\n",
      " - Batch no: 8, train loss: 0.4804997146129608\n",
      " - Batch no: 16, train loss: 0.4549092948436737\n",
      " - Batch no: 24, train loss: 0.488647997379303\n",
      "-Validation loss after epoc: 0.489249587059021 \n",
      "Epoch:641 \n",
      " - Batch no: 0, train loss: 0.6801296472549438\n",
      " - Batch no: 8, train loss: 0.4838756024837494\n",
      " - Batch no: 16, train loss: 0.6146517395973206\n",
      " - Batch no: 24, train loss: 0.5102558135986328\n",
      "-Validation loss after epoc: 0.489489883184433 \n",
      "Epoch:642 \n",
      " - Batch no: 0, train loss: 0.49832379817962646\n",
      " - Batch no: 8, train loss: 0.600496232509613\n",
      " - Batch no: 16, train loss: 0.42119652032852173\n",
      " - Batch no: 24, train loss: 0.6093839406967163\n",
      "-Validation loss after epoc: 0.4895288050174713 \n",
      "Epoch:643 \n",
      " - Batch no: 0, train loss: 0.3777048885822296\n",
      " - Batch no: 8, train loss: 0.5154064893722534\n",
      " - Batch no: 16, train loss: 0.42233744263648987\n",
      " - Batch no: 24, train loss: 0.5512756109237671\n",
      "-Validation loss after epoc: 0.489162415266037 \n",
      "Epoch:644 \n",
      " - Batch no: 0, train loss: 0.6776605844497681\n",
      " - Batch no: 8, train loss: 0.5809684991836548\n",
      " - Batch no: 16, train loss: 0.5296714305877686\n",
      " - Batch no: 24, train loss: 0.35167863965034485\n",
      "-Validation loss after epoc: 0.48908352851867676 \n",
      "Epoch:645 \n",
      " - Batch no: 0, train loss: 0.4481658935546875\n",
      " - Batch no: 8, train loss: 0.40853893756866455\n",
      " - Batch no: 16, train loss: 0.721572995185852\n",
      " - Batch no: 24, train loss: 0.591097354888916\n",
      "-Validation loss after epoc: 0.48942553997039795 \n",
      "Epoch:646 \n",
      " - Batch no: 0, train loss: 0.6061291694641113\n",
      " - Batch no: 8, train loss: 0.7400466799736023\n",
      " - Batch no: 16, train loss: 0.5856982469558716\n",
      " - Batch no: 24, train loss: 0.6373459100723267\n",
      "-Validation loss after epoc: 0.48942068219184875 \n",
      "Epoch:647 \n",
      " - Batch no: 0, train loss: 0.41769224405288696\n",
      " - Batch no: 8, train loss: 0.34306830167770386\n",
      " - Batch no: 16, train loss: 0.5147556662559509\n",
      " - Batch no: 24, train loss: 0.7783568501472473\n",
      "-Validation loss after epoc: 0.4896635115146637 \n",
      "Epoch:648 \n",
      " - Batch no: 0, train loss: 0.40499845147132874\n",
      " - Batch no: 8, train loss: 0.6798500418663025\n",
      " - Batch no: 16, train loss: 0.7752065658569336\n",
      " - Batch no: 24, train loss: 0.6773909330368042\n",
      "-Validation loss after epoc: 0.4887442886829376 \n",
      "Epoch:649 \n",
      " - Batch no: 0, train loss: 0.5794133543968201\n",
      " - Batch no: 8, train loss: 0.5638669729232788\n",
      " - Batch no: 16, train loss: 0.5444607138633728\n",
      " - Batch no: 24, train loss: 0.5719809532165527\n",
      "-Validation loss after epoc: 0.4889468550682068 \n",
      "Epoch:650 \n",
      " - Batch no: 0, train loss: 0.6320647597312927\n",
      " - Batch no: 8, train loss: 0.4666344225406647\n",
      " - Batch no: 16, train loss: 0.5473301410675049\n",
      " - Batch no: 24, train loss: 0.6053465604782104\n",
      "-Validation loss after epoc: 0.4890173375606537 \n",
      "Epoch:651 \n",
      " - Batch no: 0, train loss: 0.7170557975769043\n",
      " - Batch no: 8, train loss: 0.6975285410881042\n",
      " - Batch no: 16, train loss: 0.5903995037078857\n",
      " - Batch no: 24, train loss: 0.6315720081329346\n",
      "-Validation loss after epoc: 0.49000421166419983 \n",
      "Epoch:652 \n",
      " - Batch no: 0, train loss: 0.489541232585907\n",
      " - Batch no: 8, train loss: 0.4247722029685974\n",
      " - Batch no: 16, train loss: 0.5500742793083191\n",
      " - Batch no: 24, train loss: 0.6667612791061401\n",
      "-Validation loss after epoc: 0.4895581901073456 \n",
      "Epoch:653 \n",
      " - Batch no: 0, train loss: 0.3950021266937256\n",
      " - Batch no: 8, train loss: 0.47801610827445984\n",
      " - Batch no: 16, train loss: 0.629581093788147\n",
      " - Batch no: 24, train loss: 0.6161780953407288\n",
      "-Validation loss after epoc: 0.48884516954421997 \n",
      "Epoch:654 \n",
      " - Batch no: 0, train loss: 0.7142888903617859\n",
      " - Batch no: 8, train loss: 0.6091381311416626\n",
      " - Batch no: 16, train loss: 0.704853355884552\n",
      " - Batch no: 24, train loss: 0.472915917634964\n",
      "-Validation loss after epoc: 0.4896058142185211 \n",
      "Epoch:655 \n",
      " - Batch no: 0, train loss: 0.5056799650192261\n",
      " - Batch no: 8, train loss: 0.6400473713874817\n",
      " - Batch no: 16, train loss: 0.49655699729919434\n",
      " - Batch no: 24, train loss: 0.47049063444137573\n",
      "-Validation loss after epoc: 0.4893564283847809 \n",
      "Epoch:656 \n",
      " - Batch no: 0, train loss: 0.4864793121814728\n",
      " - Batch no: 8, train loss: 0.5367093086242676\n",
      " - Batch no: 16, train loss: 0.45010116696357727\n",
      " - Batch no: 24, train loss: 0.4969659447669983\n",
      "-Validation loss after epoc: 0.4888818860054016 \n",
      "Epoch:657 \n",
      " - Batch no: 0, train loss: 0.6589162349700928\n",
      " - Batch no: 8, train loss: 0.5135639905929565\n",
      " - Batch no: 16, train loss: 0.4985044300556183\n",
      " - Batch no: 24, train loss: 0.6386388540267944\n",
      "-Validation loss after epoc: 0.4889894127845764 \n",
      "Epoch:658 \n",
      " - Batch no: 0, train loss: 0.8373057246208191\n",
      " - Batch no: 8, train loss: 0.8133082985877991\n",
      " - Batch no: 16, train loss: 0.5527534484863281\n",
      " - Batch no: 24, train loss: 0.6818228363990784\n",
      "-Validation loss after epoc: 0.48913395404815674 \n",
      "Epoch:659 \n",
      " - Batch no: 0, train loss: 0.6792084574699402\n",
      " - Batch no: 8, train loss: 0.5245317220687866\n",
      " - Batch no: 16, train loss: 0.6204343438148499\n",
      " - Batch no: 24, train loss: 0.43874746561050415\n",
      "-Validation loss after epoc: 0.48843008279800415 \n",
      "Epoch:660 \n",
      " - Batch no: 0, train loss: 0.5221084356307983\n",
      " - Batch no: 8, train loss: 0.5520100593566895\n",
      " - Batch no: 16, train loss: 0.559461236000061\n",
      " - Batch no: 24, train loss: 0.7600171566009521\n",
      "-Validation loss after epoc: 0.48848456144332886 \n",
      "Epoch:661 \n",
      " - Batch no: 0, train loss: 0.5736824870109558\n",
      " - Batch no: 8, train loss: 0.5011304616928101\n",
      " - Batch no: 16, train loss: 0.5169739723205566\n",
      " - Batch no: 24, train loss: 0.7495158910751343\n",
      "-Validation loss after epoc: 0.4889259934425354 \n",
      "Epoch:662 \n",
      " - Batch no: 0, train loss: 0.3443065285682678\n",
      " - Batch no: 8, train loss: 0.5240514278411865\n",
      " - Batch no: 16, train loss: 0.4358024597167969\n",
      " - Batch no: 24, train loss: 0.5958685278892517\n",
      "-Validation loss after epoc: 0.48875609040260315 \n",
      "Epoch:663 \n",
      " - Batch no: 0, train loss: 0.42677417397499084\n",
      " - Batch no: 8, train loss: 0.7156212329864502\n",
      " - Batch no: 16, train loss: 0.4372158646583557\n",
      " - Batch no: 24, train loss: 0.5341063141822815\n",
      "-Validation loss after epoc: 0.48906609416007996 \n",
      "Epoch:664 \n",
      " - Batch no: 0, train loss: 0.4873034358024597\n",
      " - Batch no: 8, train loss: 0.44764235615730286\n",
      " - Batch no: 16, train loss: 0.716732919216156\n",
      " - Batch no: 24, train loss: 0.6650664210319519\n",
      "-Validation loss after epoc: 0.4882621169090271 \n",
      "Epoch:665 \n",
      " - Batch no: 0, train loss: 0.3597596287727356\n",
      " - Batch no: 8, train loss: 0.38141846656799316\n",
      " - Batch no: 16, train loss: 0.6317723989486694\n",
      " - Batch no: 24, train loss: 0.5610427856445312\n",
      "-Validation loss after epoc: 0.48878127336502075 \n",
      "Epoch:666 \n",
      " - Batch no: 0, train loss: 0.5211512446403503\n",
      " - Batch no: 8, train loss: 0.5349997282028198\n",
      " - Batch no: 16, train loss: 0.7357907295227051\n",
      " - Batch no: 24, train loss: 0.6839008331298828\n",
      "-Validation loss after epoc: 0.4887201488018036 \n",
      "Epoch:667 \n",
      " - Batch no: 0, train loss: 0.49874061346054077\n",
      " - Batch no: 8, train loss: 0.5603203773498535\n",
      " - Batch no: 16, train loss: 0.41354089975357056\n",
      " - Batch no: 24, train loss: 0.4031354784965515\n",
      "-Validation loss after epoc: 0.48854875564575195 \n",
      "Epoch:668 \n",
      " - Batch no: 0, train loss: 0.5643637180328369\n",
      " - Batch no: 8, train loss: 0.5471685528755188\n",
      " - Batch no: 16, train loss: 0.3335462808609009\n",
      " - Batch no: 24, train loss: 0.47950324416160583\n",
      "-Validation loss after epoc: 0.4882865250110626 \n",
      "Epoch:669 \n",
      " - Batch no: 0, train loss: 0.5746204853057861\n",
      " - Batch no: 8, train loss: 0.4718240797519684\n",
      " - Batch no: 16, train loss: 0.6576879620552063\n",
      " - Batch no: 24, train loss: 0.5794411301612854\n",
      "-Validation loss after epoc: 0.48798489570617676 \n",
      "Epoch:670 \n",
      " - Batch no: 0, train loss: 0.4442580044269562\n",
      " - Batch no: 8, train loss: 0.4969109296798706\n",
      " - Batch no: 16, train loss: 0.4481200873851776\n",
      " - Batch no: 24, train loss: 0.6769847869873047\n",
      "-Validation loss after epoc: 0.48781508207321167 \n",
      "Epoch:671 \n",
      " - Batch no: 0, train loss: 0.4125923812389374\n",
      " - Batch no: 8, train loss: 0.6666301488876343\n",
      " - Batch no: 16, train loss: 0.4204340875148773\n",
      " - Batch no: 24, train loss: 0.4887331426143646\n",
      "-Validation loss after epoc: 0.4882769286632538 \n",
      "Epoch:672 \n",
      " - Batch no: 0, train loss: 0.6973824501037598\n",
      " - Batch no: 8, train loss: 0.615971565246582\n",
      " - Batch no: 16, train loss: 0.4425773620605469\n",
      " - Batch no: 24, train loss: 0.5613400340080261\n",
      "-Validation loss after epoc: 0.4879681169986725 \n",
      "Epoch:673 \n",
      " - Batch no: 0, train loss: 0.527186393737793\n",
      " - Batch no: 8, train loss: 0.4926397502422333\n",
      " - Batch no: 16, train loss: 0.3991932272911072\n",
      " - Batch no: 24, train loss: 0.5132333636283875\n",
      "-Validation loss after epoc: 0.4882410764694214 \n",
      "Epoch:674 \n",
      " - Batch no: 0, train loss: 0.4801448583602905\n",
      " - Batch no: 8, train loss: 0.4690796136856079\n",
      " - Batch no: 16, train loss: 0.599147379398346\n",
      " - Batch no: 24, train loss: 0.5288546681404114\n",
      "-Validation loss after epoc: 0.488716185092926 \n",
      "Epoch:675 \n",
      " - Batch no: 0, train loss: 0.5892216563224792\n",
      " - Batch no: 8, train loss: 0.4150318205356598\n",
      " - Batch no: 16, train loss: 0.4804673492908478\n",
      " - Batch no: 24, train loss: 0.5005927681922913\n",
      "-Validation loss after epoc: 0.4881431460380554 \n",
      "Epoch:676 \n",
      " - Batch no: 0, train loss: 0.6007256507873535\n",
      " - Batch no: 8, train loss: 0.4425513744354248\n",
      " - Batch no: 16, train loss: 0.5651493668556213\n",
      " - Batch no: 24, train loss: 0.5852575302124023\n",
      "-Validation loss after epoc: 0.48811089992523193 \n",
      "Epoch:677 \n",
      " - Batch no: 0, train loss: 0.6798014044761658\n",
      " - Batch no: 8, train loss: 0.4519486427307129\n",
      " - Batch no: 16, train loss: 0.6384857892990112\n",
      " - Batch no: 24, train loss: 0.5604299902915955\n",
      "-Validation loss after epoc: 0.4881459176540375 \n",
      "Epoch:678 \n",
      " - Batch no: 0, train loss: 0.479762464761734\n",
      " - Batch no: 8, train loss: 0.6027612686157227\n",
      " - Batch no: 16, train loss: 0.5986307263374329\n",
      " - Batch no: 24, train loss: 0.5347683429718018\n",
      "-Validation loss after epoc: 0.4876912534236908 \n",
      "Epoch:679 \n",
      " - Batch no: 0, train loss: 0.4659288227558136\n",
      " - Batch no: 8, train loss: 0.5129624605178833\n",
      " - Batch no: 16, train loss: 0.5579503774642944\n",
      " - Batch no: 24, train loss: 0.48927509784698486\n",
      "-Validation loss after epoc: 0.48790496587753296 \n",
      "Epoch:680 \n",
      " - Batch no: 0, train loss: 0.3372270464897156\n",
      " - Batch no: 8, train loss: 0.616037130355835\n",
      " - Batch no: 16, train loss: 0.437599241733551\n",
      " - Batch no: 24, train loss: 0.6067681908607483\n",
      "-Validation loss after epoc: 0.4875674843788147 \n",
      "Epoch:681 \n",
      " - Batch no: 0, train loss: 0.539727509021759\n",
      " - Batch no: 8, train loss: 0.4652461111545563\n",
      " - Batch no: 16, train loss: 0.5623717904090881\n",
      " - Batch no: 24, train loss: 0.4976392388343811\n",
      "-Validation loss after epoc: 0.487741082906723 \n",
      "Epoch:682 \n",
      " - Batch no: 0, train loss: 0.3457961082458496\n",
      " - Batch no: 8, train loss: 0.525262713432312\n",
      " - Batch no: 16, train loss: 0.5737759470939636\n",
      " - Batch no: 24, train loss: 0.5374459624290466\n",
      "-Validation loss after epoc: 0.48756858706474304 \n",
      "Epoch:683 \n",
      " - Batch no: 0, train loss: 0.6378525495529175\n",
      " - Batch no: 8, train loss: 0.3287666141986847\n",
      " - Batch no: 16, train loss: 0.5373641848564148\n",
      " - Batch no: 24, train loss: 0.6027792096138\n",
      "-Validation loss after epoc: 0.48799559473991394 \n",
      "Epoch:684 \n",
      " - Batch no: 0, train loss: 0.5609909296035767\n",
      " - Batch no: 8, train loss: 0.39216676354408264\n",
      " - Batch no: 16, train loss: 0.57979816198349\n",
      " - Batch no: 24, train loss: 0.6231191158294678\n",
      "-Validation loss after epoc: 0.4877951145172119 \n",
      "Epoch:685 \n",
      " - Batch no: 0, train loss: 0.49795281887054443\n",
      " - Batch no: 8, train loss: 0.607832133769989\n",
      " - Batch no: 16, train loss: 0.37707647681236267\n",
      " - Batch no: 24, train loss: 0.7645276784896851\n",
      "-Validation loss after epoc: 0.487326055765152 \n",
      "Epoch:686 \n",
      " - Batch no: 0, train loss: 0.5587771534919739\n",
      " - Batch no: 8, train loss: 0.471528559923172\n",
      " - Batch no: 16, train loss: 0.6612483263015747\n",
      " - Batch no: 24, train loss: 0.4915989935398102\n",
      "-Validation loss after epoc: 0.48798415064811707 \n",
      "Epoch:687 \n",
      " - Batch no: 0, train loss: 0.552638053894043\n",
      " - Batch no: 8, train loss: 0.4649060368537903\n",
      " - Batch no: 16, train loss: 0.5613964796066284\n",
      " - Batch no: 24, train loss: 0.6088605523109436\n",
      "-Validation loss after epoc: 0.488066166639328 \n",
      "Epoch:688 \n",
      " - Batch no: 0, train loss: 0.44534000754356384\n",
      " - Batch no: 8, train loss: 0.40743547677993774\n",
      " - Batch no: 16, train loss: 0.5632715821266174\n",
      " - Batch no: 24, train loss: 0.7435855865478516\n",
      "-Validation loss after epoc: 0.48752811551094055 \n",
      "Epoch:689 \n",
      " - Batch no: 0, train loss: 0.5480685234069824\n",
      " - Batch no: 8, train loss: 0.5463011264801025\n",
      " - Batch no: 16, train loss: 0.40241995453834534\n",
      " - Batch no: 24, train loss: 0.45679911971092224\n",
      "-Validation loss after epoc: 0.4870915412902832 \n",
      "Epoch:690 \n",
      " - Batch no: 0, train loss: 0.5702944993972778\n",
      " - Batch no: 8, train loss: 0.6031224131584167\n",
      " - Batch no: 16, train loss: 0.6787487864494324\n",
      " - Batch no: 24, train loss: 0.5908882021903992\n",
      "-Validation loss after epoc: 0.48745402693748474 \n",
      "Epoch:691 \n",
      " - Batch no: 0, train loss: 0.5697648525238037\n",
      " - Batch no: 8, train loss: 0.6045648455619812\n",
      " - Batch no: 16, train loss: 0.5521710515022278\n",
      " - Batch no: 24, train loss: 0.4865022897720337\n",
      "-Validation loss after epoc: 0.48751428723335266 \n",
      "Epoch:692 \n",
      " - Batch no: 0, train loss: 0.6693689823150635\n",
      " - Batch no: 8, train loss: 0.6290278434753418\n",
      " - Batch no: 16, train loss: 0.5542006492614746\n",
      " - Batch no: 24, train loss: 0.4890254735946655\n",
      "-Validation loss after epoc: 0.4876430034637451 \n",
      "Epoch:693 \n",
      " - Batch no: 0, train loss: 0.3618921637535095\n",
      " - Batch no: 8, train loss: 0.3879624307155609\n",
      " - Batch no: 16, train loss: 0.6896553635597229\n",
      " - Batch no: 24, train loss: 0.5527731776237488\n",
      "-Validation loss after epoc: 0.4872854948043823 \n",
      "Epoch:694 \n",
      " - Batch no: 0, train loss: 0.5326510071754456\n",
      " - Batch no: 8, train loss: 0.3832767903804779\n",
      " - Batch no: 16, train loss: 0.3638060390949249\n",
      " - Batch no: 24, train loss: 0.5946691632270813\n",
      "-Validation loss after epoc: 0.4875841438770294 \n",
      "Epoch:695 \n",
      " - Batch no: 0, train loss: 0.5786378979682922\n",
      " - Batch no: 8, train loss: 0.462838351726532\n",
      " - Batch no: 16, train loss: 0.45268696546554565\n",
      " - Batch no: 24, train loss: 0.5739279985427856\n",
      "-Validation loss after epoc: 0.48782262206077576 \n",
      "Epoch:696 \n",
      " - Batch no: 0, train loss: 0.5012821555137634\n",
      " - Batch no: 8, train loss: 0.8575672507286072\n",
      " - Batch no: 16, train loss: 0.44402405619621277\n",
      " - Batch no: 24, train loss: 0.48651543259620667\n",
      "-Validation loss after epoc: 0.4875943064689636 \n",
      "Epoch:697 \n",
      " - Batch no: 0, train loss: 0.5863263607025146\n",
      " - Batch no: 8, train loss: 0.5188127756118774\n",
      " - Batch no: 16, train loss: 0.7970917224884033\n",
      " - Batch no: 24, train loss: 0.6131646037101746\n",
      "-Validation loss after epoc: 0.4878580570220947 \n",
      "Epoch:698 \n",
      " - Batch no: 0, train loss: 0.5750501751899719\n",
      " - Batch no: 8, train loss: 0.6024090051651001\n",
      " - Batch no: 16, train loss: 0.5841866731643677\n",
      " - Batch no: 24, train loss: 0.6319149732589722\n",
      "-Validation loss after epoc: 0.48721444606781006 \n",
      "Epoch:699 \n",
      " - Batch no: 0, train loss: 0.42370039224624634\n",
      " - Batch no: 8, train loss: 0.643200159072876\n",
      " - Batch no: 16, train loss: 0.6157302260398865\n",
      " - Batch no: 24, train loss: 0.53140789270401\n",
      "-Validation loss after epoc: 0.487527996301651 \n",
      "Epoch:700 \n",
      " - Batch no: 0, train loss: 0.3622661530971527\n",
      " - Batch no: 8, train loss: 0.4919099509716034\n",
      " - Batch no: 16, train loss: 0.5780813694000244\n",
      " - Batch no: 24, train loss: 0.6421800255775452\n",
      "-Validation loss after epoc: 0.4871092140674591 \n",
      "Epoch:701 \n",
      " - Batch no: 0, train loss: 0.6874880790710449\n",
      " - Batch no: 8, train loss: 0.5578547716140747\n",
      " - Batch no: 16, train loss: 0.4668923020362854\n",
      " - Batch no: 24, train loss: 0.5832463502883911\n",
      "-Validation loss after epoc: 0.48700517416000366 \n",
      "Epoch:702 \n",
      " - Batch no: 0, train loss: 0.4912048280239105\n",
      " - Batch no: 8, train loss: 0.724170446395874\n",
      " - Batch no: 16, train loss: 0.7784076929092407\n",
      " - Batch no: 24, train loss: 0.4074557423591614\n",
      "-Validation loss after epoc: 0.48726364970207214 \n",
      "Epoch:703 \n",
      " - Batch no: 0, train loss: 0.4508817493915558\n",
      " - Batch no: 8, train loss: 0.528752863407135\n",
      " - Batch no: 16, train loss: 0.3796220123767853\n",
      " - Batch no: 24, train loss: 0.4014711380004883\n",
      "-Validation loss after epoc: 0.48740169405937195 \n",
      "Epoch:704 \n",
      " - Batch no: 0, train loss: 0.40524715185165405\n",
      " - Batch no: 8, train loss: 0.5817952752113342\n",
      " - Batch no: 16, train loss: 0.4550517201423645\n",
      " - Batch no: 24, train loss: 0.46200355887413025\n",
      "-Validation loss after epoc: 0.48719194531440735 \n",
      "Epoch:705 \n",
      " - Batch no: 0, train loss: 0.6074548363685608\n",
      " - Batch no: 8, train loss: 0.6146217584609985\n",
      " - Batch no: 16, train loss: 0.6803624629974365\n",
      " - Batch no: 24, train loss: 0.4379050135612488\n",
      "-Validation loss after epoc: 0.48675301671028137 \n",
      "Epoch:706 \n",
      " - Batch no: 0, train loss: 0.583587646484375\n",
      " - Batch no: 8, train loss: 0.5522087812423706\n",
      " - Batch no: 16, train loss: 0.5257495641708374\n",
      " - Batch no: 24, train loss: 0.5169641971588135\n",
      "-Validation loss after epoc: 0.4868602752685547 \n",
      "Epoch:707 \n",
      " - Batch no: 0, train loss: 0.5620659589767456\n",
      " - Batch no: 8, train loss: 0.5736031532287598\n",
      " - Batch no: 16, train loss: 0.5067591071128845\n",
      " - Batch no: 24, train loss: 0.7780989408493042\n",
      "-Validation loss after epoc: 0.4868466556072235 \n",
      "Epoch:708 \n",
      " - Batch no: 0, train loss: 0.592890739440918\n",
      " - Batch no: 8, train loss: 0.45540592074394226\n",
      " - Batch no: 16, train loss: 0.4850236475467682\n",
      " - Batch no: 24, train loss: 0.5468554496765137\n",
      "-Validation loss after epoc: 0.48737069964408875 \n",
      "Epoch:709 \n",
      " - Batch no: 0, train loss: 0.41582754254341125\n",
      " - Batch no: 8, train loss: 0.6415374279022217\n",
      " - Batch no: 16, train loss: 0.4078654944896698\n",
      " - Batch no: 24, train loss: 0.8457195162773132\n",
      "-Validation loss after epoc: 0.4869500696659088 \n",
      "Epoch:710 \n",
      " - Batch no: 0, train loss: 0.5896947979927063\n",
      " - Batch no: 8, train loss: 0.4926968812942505\n",
      " - Batch no: 16, train loss: 0.6018275618553162\n",
      " - Batch no: 24, train loss: 0.5835850238800049\n",
      "-Validation loss after epoc: 0.4868107736110687 \n",
      "Epoch:711 \n",
      " - Batch no: 0, train loss: 0.6317830681800842\n",
      " - Batch no: 8, train loss: 0.9513998031616211\n",
      " - Batch no: 16, train loss: 0.3286533057689667\n",
      " - Batch no: 24, train loss: 0.5310379862785339\n",
      "-Validation loss after epoc: 0.48607566952705383 \n",
      "Epoch:712 \n",
      " - Batch no: 0, train loss: 0.4505656361579895\n",
      " - Batch no: 8, train loss: 0.5402490496635437\n",
      " - Batch no: 16, train loss: 0.46912944316864014\n",
      " - Batch no: 24, train loss: 0.5508999228477478\n",
      "-Validation loss after epoc: 0.48686492443084717 \n",
      "Epoch:713 \n",
      " - Batch no: 0, train loss: 0.4034382998943329\n",
      " - Batch no: 8, train loss: 0.5233479142189026\n",
      " - Batch no: 16, train loss: 0.4533127248287201\n",
      " - Batch no: 24, train loss: 0.5398997068405151\n",
      "-Validation loss after epoc: 0.4865512251853943 \n",
      "Epoch:714 \n",
      " - Batch no: 0, train loss: 0.7025066614151001\n",
      " - Batch no: 8, train loss: 0.5663973093032837\n",
      " - Batch no: 16, train loss: 0.5956908464431763\n",
      " - Batch no: 24, train loss: 0.5605362057685852\n",
      "-Validation loss after epoc: 0.48691096901893616 \n",
      "Epoch:715 \n",
      " - Batch no: 0, train loss: 0.5257194638252258\n",
      " - Batch no: 8, train loss: 0.39987435936927795\n",
      " - Batch no: 16, train loss: 0.6244465112686157\n",
      " - Batch no: 24, train loss: 0.45613589882850647\n",
      "-Validation loss after epoc: 0.4867709279060364 \n",
      "Epoch:716 \n",
      " - Batch no: 0, train loss: 0.7366065979003906\n",
      " - Batch no: 8, train loss: 0.520182728767395\n",
      " - Batch no: 16, train loss: 0.6546017527580261\n",
      " - Batch no: 24, train loss: 0.3950252830982208\n",
      "-Validation loss after epoc: 0.486528605222702 \n",
      "Epoch:717 \n",
      " - Batch no: 0, train loss: 0.5141594409942627\n",
      " - Batch no: 8, train loss: 0.5604821443557739\n",
      " - Batch no: 16, train loss: 0.5555610060691833\n",
      " - Batch no: 24, train loss: 0.45551687479019165\n",
      "-Validation loss after epoc: 0.4865265488624573 \n",
      "Epoch:718 \n",
      " - Batch no: 0, train loss: 0.43663904070854187\n",
      " - Batch no: 8, train loss: 0.45739108324050903\n",
      " - Batch no: 16, train loss: 0.5023970603942871\n",
      " - Batch no: 24, train loss: 0.4866670072078705\n",
      "-Validation loss after epoc: 0.4862310290336609 \n",
      "Epoch:719 \n",
      " - Batch no: 0, train loss: 0.45641568303108215\n",
      " - Batch no: 8, train loss: 0.6593924164772034\n",
      " - Batch no: 16, train loss: 0.5472902655601501\n",
      " - Batch no: 24, train loss: 0.4313434362411499\n",
      "-Validation loss after epoc: 0.4866464138031006 \n",
      "Epoch:720 \n",
      " - Batch no: 0, train loss: 0.5401996970176697\n",
      " - Batch no: 8, train loss: 0.6164644956588745\n",
      " - Batch no: 16, train loss: 0.5530452132225037\n",
      " - Batch no: 24, train loss: 0.6051034331321716\n",
      "-Validation loss after epoc: 0.4864119589328766 \n",
      "Epoch:721 \n",
      " - Batch no: 0, train loss: 0.43065065145492554\n",
      " - Batch no: 8, train loss: 0.5366125702857971\n",
      " - Batch no: 16, train loss: 0.41776448488235474\n",
      " - Batch no: 24, train loss: 0.5880948305130005\n",
      "-Validation loss after epoc: 0.48649710416793823 \n",
      "Epoch:722 \n",
      " - Batch no: 0, train loss: 0.3967692255973816\n",
      " - Batch no: 8, train loss: 0.700687050819397\n",
      " - Batch no: 16, train loss: 0.4574612081050873\n",
      " - Batch no: 24, train loss: 0.5163062810897827\n",
      "-Validation loss after epoc: 0.48636630177497864 \n",
      "Epoch:723 \n",
      " - Batch no: 0, train loss: 0.6252869963645935\n",
      " - Batch no: 8, train loss: 0.47199615836143494\n",
      " - Batch no: 16, train loss: 0.7549675703048706\n",
      " - Batch no: 24, train loss: 0.7241344451904297\n",
      "-Validation loss after epoc: 0.4870433211326599 \n",
      "Epoch:724 \n",
      " - Batch no: 0, train loss: 0.5067334175109863\n",
      " - Batch no: 8, train loss: 0.5842766761779785\n",
      " - Batch no: 16, train loss: 0.37651553750038147\n",
      " - Batch no: 24, train loss: 0.7641497850418091\n",
      "-Validation loss after epoc: 0.4862007200717926 \n",
      "Epoch:725 \n",
      " - Batch no: 0, train loss: 0.6100591421127319\n",
      " - Batch no: 8, train loss: 0.42571258544921875\n",
      " - Batch no: 16, train loss: 0.48477980494499207\n",
      " - Batch no: 24, train loss: 0.6642484664916992\n",
      "-Validation loss after epoc: 0.48661839962005615 \n",
      "Epoch:726 \n",
      " - Batch no: 0, train loss: 0.6829167604446411\n",
      " - Batch no: 8, train loss: 0.4681001901626587\n",
      " - Batch no: 16, train loss: 0.5846124291419983\n",
      " - Batch no: 24, train loss: 0.6757020950317383\n",
      "-Validation loss after epoc: 0.4862910807132721 \n",
      "Epoch:727 \n",
      " - Batch no: 0, train loss: 0.5027538537979126\n",
      " - Batch no: 8, train loss: 0.48174986243247986\n",
      " - Batch no: 16, train loss: 0.5040081143379211\n",
      " - Batch no: 24, train loss: 0.45574069023132324\n",
      "-Validation loss after epoc: 0.4862978458404541 \n",
      "Epoch:728 \n",
      " - Batch no: 0, train loss: 0.6932106614112854\n",
      " - Batch no: 8, train loss: 0.37596195936203003\n",
      " - Batch no: 16, train loss: 0.6550124287605286\n",
      " - Batch no: 24, train loss: 0.710886538028717\n",
      "-Validation loss after epoc: 0.48627567291259766 \n",
      "Epoch:729 \n",
      " - Batch no: 0, train loss: 0.49402377009391785\n",
      " - Batch no: 8, train loss: 0.461720734834671\n",
      " - Batch no: 16, train loss: 0.5026935338973999\n",
      " - Batch no: 24, train loss: 0.5409054756164551\n",
      "-Validation loss after epoc: 0.48691967129707336 \n",
      "Epoch:730 \n",
      " - Batch no: 0, train loss: 0.4917627274990082\n",
      " - Batch no: 8, train loss: 0.43791747093200684\n",
      " - Batch no: 16, train loss: 0.4726645052433014\n",
      " - Batch no: 24, train loss: 0.5512741208076477\n",
      "-Validation loss after epoc: 0.4862810969352722 \n",
      "Epoch:731 \n",
      " - Batch no: 0, train loss: 0.4863641560077667\n",
      " - Batch no: 8, train loss: 0.4756428897380829\n",
      " - Batch no: 16, train loss: 0.5159093737602234\n",
      " - Batch no: 24, train loss: 0.8834889531135559\n",
      "-Validation loss after epoc: 0.4859503209590912 \n",
      "Epoch:732 \n",
      " - Batch no: 0, train loss: 0.7133911848068237\n",
      " - Batch no: 8, train loss: 0.5860050320625305\n",
      " - Batch no: 16, train loss: 0.6815046072006226\n",
      " - Batch no: 24, train loss: 0.6884437799453735\n",
      "-Validation loss after epoc: 0.48616212606430054 \n",
      "Epoch:733 \n",
      " - Batch no: 0, train loss: 0.5741320848464966\n",
      " - Batch no: 8, train loss: 0.5806676149368286\n",
      " - Batch no: 16, train loss: 0.4776179790496826\n",
      " - Batch no: 24, train loss: 0.5799201130867004\n",
      "-Validation loss after epoc: 0.48633459210395813 \n",
      "Epoch:734 \n",
      " - Batch no: 0, train loss: 0.7213156223297119\n",
      " - Batch no: 8, train loss: 0.6247649788856506\n",
      " - Batch no: 16, train loss: 0.5161981582641602\n",
      " - Batch no: 24, train loss: 0.6852439045906067\n",
      "-Validation loss after epoc: 0.48632046580314636 \n",
      "Epoch:735 \n",
      " - Batch no: 0, train loss: 0.5589190721511841\n",
      " - Batch no: 8, train loss: 0.6891925930976868\n",
      " - Batch no: 16, train loss: 0.47842171788215637\n",
      " - Batch no: 24, train loss: 0.5109765529632568\n",
      "-Validation loss after epoc: 0.48602989315986633 \n",
      "Epoch:736 \n",
      " - Batch no: 0, train loss: 0.5024677515029907\n",
      " - Batch no: 8, train loss: 0.49099019169807434\n",
      " - Batch no: 16, train loss: 0.5260500907897949\n",
      " - Batch no: 24, train loss: 0.3731551468372345\n",
      "-Validation loss after epoc: 0.48628416657447815 \n",
      "Epoch:737 \n",
      " - Batch no: 0, train loss: 0.4825114607810974\n",
      " - Batch no: 8, train loss: 0.5353447198867798\n",
      " - Batch no: 16, train loss: 0.5276755094528198\n",
      " - Batch no: 24, train loss: 0.5196695327758789\n",
      "-Validation loss after epoc: 0.4865076541900635 \n",
      "Epoch:738 \n",
      " - Batch no: 0, train loss: 0.4853438138961792\n",
      " - Batch no: 8, train loss: 0.4529670774936676\n",
      " - Batch no: 16, train loss: 0.48997655510902405\n",
      " - Batch no: 24, train loss: 0.47722694277763367\n",
      "-Validation loss after epoc: 0.4862253665924072 \n",
      "Epoch:739 \n",
      " - Batch no: 0, train loss: 0.6149855256080627\n",
      " - Batch no: 8, train loss: 0.4669608175754547\n",
      " - Batch no: 16, train loss: 0.5402604341506958\n",
      " - Batch no: 24, train loss: 0.42754071950912476\n",
      "-Validation loss after epoc: 0.486359566450119 \n",
      "Epoch:740 \n",
      " - Batch no: 0, train loss: 0.8963574171066284\n",
      " - Batch no: 8, train loss: 0.7153061032295227\n",
      " - Batch no: 16, train loss: 0.5645715594291687\n",
      " - Batch no: 24, train loss: 0.6419166922569275\n",
      "-Validation loss after epoc: 0.48629218339920044 \n",
      "Epoch:741 \n",
      " - Batch no: 0, train loss: 0.5215225219726562\n",
      " - Batch no: 8, train loss: 0.5783607363700867\n",
      " - Batch no: 16, train loss: 0.7818954586982727\n",
      " - Batch no: 24, train loss: 0.5481784343719482\n",
      "-Validation loss after epoc: 0.4857977032661438 \n",
      "Epoch:742 \n",
      " - Batch no: 0, train loss: 0.41146692633628845\n",
      " - Batch no: 8, train loss: 0.6705427765846252\n",
      " - Batch no: 16, train loss: 0.6332986950874329\n",
      " - Batch no: 24, train loss: 0.4187513589859009\n",
      "-Validation loss after epoc: 0.4865151047706604 \n",
      "Epoch:743 \n",
      " - Batch no: 0, train loss: 0.6785815954208374\n",
      " - Batch no: 8, train loss: 0.5559434294700623\n",
      " - Batch no: 16, train loss: 0.577552318572998\n",
      " - Batch no: 24, train loss: 0.4466802179813385\n",
      "-Validation loss after epoc: 0.48672232031822205 \n",
      "Epoch:744 \n",
      " - Batch no: 0, train loss: 0.6909791231155396\n",
      " - Batch no: 8, train loss: 0.43465468287467957\n",
      " - Batch no: 16, train loss: 0.6484478712081909\n",
      " - Batch no: 24, train loss: 0.5717029571533203\n",
      "-Validation loss after epoc: 0.48602452874183655 \n",
      "Epoch:745 \n",
      " - Batch no: 0, train loss: 0.6145288348197937\n",
      " - Batch no: 8, train loss: 0.4669871926307678\n",
      " - Batch no: 16, train loss: 0.5733138918876648\n",
      " - Batch no: 24, train loss: 0.6007257699966431\n",
      "-Validation loss after epoc: 0.4857131540775299 \n",
      "Epoch:746 \n",
      " - Batch no: 0, train loss: 0.4719877243041992\n",
      " - Batch no: 8, train loss: 0.6574128270149231\n",
      " - Batch no: 16, train loss: 0.4150335192680359\n",
      " - Batch no: 24, train loss: 0.53363037109375\n",
      "-Validation loss after epoc: 0.4860576093196869 \n",
      "Epoch:747 \n",
      " - Batch no: 0, train loss: 0.40601295232772827\n",
      " - Batch no: 8, train loss: 0.614074170589447\n",
      " - Batch no: 16, train loss: 0.6573843359947205\n",
      " - Batch no: 24, train loss: 0.5243805050849915\n",
      "-Validation loss after epoc: 0.4855667054653168 \n",
      "Epoch:748 \n",
      " - Batch no: 0, train loss: 0.6625111103057861\n",
      " - Batch no: 8, train loss: 0.6734082102775574\n",
      " - Batch no: 16, train loss: 0.5471874475479126\n",
      " - Batch no: 24, train loss: 0.5558754801750183\n",
      "-Validation loss after epoc: 0.4859234094619751 \n",
      "Epoch:749 \n",
      " - Batch no: 0, train loss: 0.5378576517105103\n",
      " - Batch no: 8, train loss: 0.6280705332756042\n",
      " - Batch no: 16, train loss: 0.4115503132343292\n",
      " - Batch no: 24, train loss: 0.8761947751045227\n",
      "-Validation loss after epoc: 0.4862079620361328 \n",
      "Epoch:750 \n",
      " - Batch no: 0, train loss: 0.47334742546081543\n",
      " - Batch no: 8, train loss: 0.42834779620170593\n",
      " - Batch no: 16, train loss: 0.43197986483573914\n",
      " - Batch no: 24, train loss: 0.5079736113548279\n",
      "-Validation loss after epoc: 0.4860934317111969 \n",
      "Epoch:751 \n",
      " - Batch no: 0, train loss: 0.6836181282997131\n",
      " - Batch no: 8, train loss: 0.5493850111961365\n",
      " - Batch no: 16, train loss: 0.3087751567363739\n",
      " - Batch no: 24, train loss: 0.685925304889679\n",
      "-Validation loss after epoc: 0.486056923866272 \n",
      "Epoch:752 \n",
      " - Batch no: 0, train loss: 0.5321418046951294\n",
      " - Batch no: 8, train loss: 0.5055559277534485\n",
      " - Batch no: 16, train loss: 0.5480396151542664\n",
      " - Batch no: 24, train loss: 0.6402508020401001\n",
      "-Validation loss after epoc: 0.4856138229370117 \n",
      "Epoch:753 \n",
      " - Batch no: 0, train loss: 0.48885712027549744\n",
      " - Batch no: 8, train loss: 0.8281000852584839\n",
      " - Batch no: 16, train loss: 0.419528067111969\n",
      " - Batch no: 24, train loss: 0.4354882538318634\n",
      "-Validation loss after epoc: 0.4855654537677765 \n",
      "Epoch:754 \n",
      " - Batch no: 0, train loss: 0.5426445007324219\n",
      " - Batch no: 8, train loss: 0.6450298428535461\n",
      " - Batch no: 16, train loss: 0.5046147704124451\n",
      " - Batch no: 24, train loss: 0.5218513607978821\n",
      "-Validation loss after epoc: 0.48543137311935425 \n",
      "Epoch:755 \n",
      " - Batch no: 0, train loss: 0.5032062530517578\n",
      " - Batch no: 8, train loss: 0.5988461971282959\n",
      " - Batch no: 16, train loss: 0.6869754791259766\n",
      " - Batch no: 24, train loss: 0.5337511897087097\n",
      "-Validation loss after epoc: 0.4854646325111389 \n",
      "Epoch:756 \n",
      " - Batch no: 0, train loss: 0.5952858924865723\n",
      " - Batch no: 8, train loss: 0.5800422430038452\n",
      " - Batch no: 16, train loss: 0.4860707223415375\n",
      " - Batch no: 24, train loss: 0.4713859260082245\n",
      "-Validation loss after epoc: 0.4851743280887604 \n",
      "Epoch:757 \n",
      " - Batch no: 0, train loss: 0.40475764870643616\n",
      " - Batch no: 8, train loss: 0.4270060658454895\n",
      " - Batch no: 16, train loss: 0.6569604873657227\n",
      " - Batch no: 24, train loss: 0.6380812525749207\n",
      "-Validation loss after epoc: 0.4853811264038086 \n",
      "Epoch:758 \n",
      " - Batch no: 0, train loss: 0.3984718918800354\n",
      " - Batch no: 8, train loss: 0.5260142087936401\n",
      " - Batch no: 16, train loss: 0.5394748449325562\n",
      " - Batch no: 24, train loss: 0.44232428073883057\n",
      "-Validation loss after epoc: 0.48548808693885803 \n",
      "Epoch:759 \n",
      " - Batch no: 0, train loss: 0.5981022119522095\n",
      " - Batch no: 8, train loss: 0.565129280090332\n",
      " - Batch no: 16, train loss: 0.4931359589099884\n",
      " - Batch no: 24, train loss: 0.5546897053718567\n",
      "-Validation loss after epoc: 0.48520171642303467 \n",
      "Epoch:760 \n",
      " - Batch no: 0, train loss: 0.6779423356056213\n",
      " - Batch no: 8, train loss: 0.5209732055664062\n",
      " - Batch no: 16, train loss: 0.5473313927650452\n",
      " - Batch no: 24, train loss: 0.44495078921318054\n",
      "-Validation loss after epoc: 0.48560139536857605 \n",
      "Epoch:761 \n",
      " - Batch no: 0, train loss: 0.40082764625549316\n",
      " - Batch no: 8, train loss: 0.7769012451171875\n",
      " - Batch no: 16, train loss: 0.5224277377128601\n",
      " - Batch no: 24, train loss: 0.5144268274307251\n",
      "-Validation loss after epoc: 0.48551836609840393 \n",
      "Epoch:762 \n",
      " - Batch no: 0, train loss: 0.5159293413162231\n",
      " - Batch no: 8, train loss: 0.4946562647819519\n",
      " - Batch no: 16, train loss: 0.5342093110084534\n",
      " - Batch no: 24, train loss: 0.32657504081726074\n",
      "-Validation loss after epoc: 0.48551642894744873 \n",
      "Epoch:763 \n",
      " - Batch no: 0, train loss: 0.542700469493866\n",
      " - Batch no: 8, train loss: 0.5831477642059326\n",
      " - Batch no: 16, train loss: 0.6516477465629578\n",
      " - Batch no: 24, train loss: 0.5931235551834106\n",
      "-Validation loss after epoc: 0.48568615317344666 \n",
      "Epoch:764 \n",
      " - Batch no: 0, train loss: 0.546548068523407\n",
      " - Batch no: 8, train loss: 0.4834194481372833\n",
      " - Batch no: 16, train loss: 0.5467050671577454\n",
      " - Batch no: 24, train loss: 0.4008711278438568\n",
      "-Validation loss after epoc: 0.48555779457092285 \n",
      "Epoch:765 \n",
      " - Batch no: 0, train loss: 0.46410128474235535\n",
      " - Batch no: 8, train loss: 0.7694814205169678\n",
      " - Batch no: 16, train loss: 0.5564637780189514\n",
      " - Batch no: 24, train loss: 0.568351149559021\n",
      "-Validation loss after epoc: 0.48541921377182007 \n",
      "Epoch:766 \n",
      " - Batch no: 0, train loss: 0.369435578584671\n",
      " - Batch no: 8, train loss: 0.32711800932884216\n",
      " - Batch no: 16, train loss: 0.47381019592285156\n",
      " - Batch no: 24, train loss: 0.5949259400367737\n",
      "-Validation loss after epoc: 0.4853611886501312 \n",
      "Epoch:767 \n",
      " - Batch no: 0, train loss: 0.4609087109565735\n",
      " - Batch no: 8, train loss: 0.4129997789859772\n",
      " - Batch no: 16, train loss: 0.3827648460865021\n",
      " - Batch no: 24, train loss: 0.4823664426803589\n",
      "-Validation loss after epoc: 0.4855499565601349 \n",
      "Epoch:768 \n",
      " - Batch no: 0, train loss: 0.581612229347229\n",
      " - Batch no: 8, train loss: 0.6394811272621155\n",
      " - Batch no: 16, train loss: 0.4746131896972656\n",
      " - Batch no: 24, train loss: 0.48278576135635376\n",
      "-Validation loss after epoc: 0.4850925803184509 \n",
      "Epoch:769 \n",
      " - Batch no: 0, train loss: 0.48751041293144226\n",
      " - Batch no: 8, train loss: 0.523108959197998\n",
      " - Batch no: 16, train loss: 0.3641539514064789\n",
      " - Batch no: 24, train loss: 0.4310928285121918\n",
      "-Validation loss after epoc: 0.48526498675346375 \n",
      "Epoch:770 \n",
      " - Batch no: 0, train loss: 0.6444103717803955\n",
      " - Batch no: 8, train loss: 0.5279141068458557\n",
      " - Batch no: 16, train loss: 0.5289089679718018\n",
      " - Batch no: 24, train loss: 0.7841998934745789\n",
      "-Validation loss after epoc: 0.48547011613845825 \n",
      "Epoch:771 \n",
      " - Batch no: 0, train loss: 0.5934573411941528\n",
      " - Batch no: 8, train loss: 0.5856688022613525\n",
      " - Batch no: 16, train loss: 0.5201588869094849\n",
      " - Batch no: 24, train loss: 0.6042740345001221\n",
      "-Validation loss after epoc: 0.4849492013454437 \n",
      "Epoch:772 \n",
      " - Batch no: 0, train loss: 0.5274485349655151\n",
      " - Batch no: 8, train loss: 0.8129389882087708\n",
      " - Batch no: 16, train loss: 0.7134540677070618\n",
      " - Batch no: 24, train loss: 0.3738863170146942\n",
      "-Validation loss after epoc: 0.4850406050682068 \n",
      "Epoch:773 \n",
      " - Batch no: 0, train loss: 0.4183584749698639\n",
      " - Batch no: 8, train loss: 0.6767076849937439\n",
      " - Batch no: 16, train loss: 0.36856919527053833\n",
      " - Batch no: 24, train loss: 0.5987894535064697\n",
      "-Validation loss after epoc: 0.48514533042907715 \n",
      "Epoch:774 \n",
      " - Batch no: 0, train loss: 0.7408190369606018\n",
      " - Batch no: 8, train loss: 0.6074860692024231\n",
      " - Batch no: 16, train loss: 0.47415390610694885\n",
      " - Batch no: 24, train loss: 0.5656546950340271\n",
      "-Validation loss after epoc: 0.48481613397598267 \n",
      "Epoch:775 \n",
      " - Batch no: 0, train loss: 0.5113033652305603\n",
      " - Batch no: 8, train loss: 0.4660976231098175\n",
      " - Batch no: 16, train loss: 0.5631644129753113\n",
      " - Batch no: 24, train loss: 0.6674350500106812\n",
      "-Validation loss after epoc: 0.4852059483528137 \n",
      "Epoch:776 \n",
      " - Batch no: 0, train loss: 0.4569423794746399\n",
      " - Batch no: 8, train loss: 0.4214960038661957\n",
      " - Batch no: 16, train loss: 0.6233566403388977\n",
      " - Batch no: 24, train loss: 0.4805638790130615\n",
      "-Validation loss after epoc: 0.48500746488571167 \n",
      "Epoch:777 \n",
      " - Batch no: 0, train loss: 0.5400447845458984\n",
      " - Batch no: 8, train loss: 0.49248597025871277\n",
      " - Batch no: 16, train loss: 0.4357779324054718\n",
      " - Batch no: 24, train loss: 0.6663918495178223\n",
      "-Validation loss after epoc: 0.4849061667919159 \n",
      "Epoch:778 \n",
      " - Batch no: 0, train loss: 0.46229857206344604\n",
      " - Batch no: 8, train loss: 0.4733339548110962\n",
      " - Batch no: 16, train loss: 0.49260854721069336\n",
      " - Batch no: 24, train loss: 0.6032759547233582\n",
      "-Validation loss after epoc: 0.4848542809486389 \n",
      "Epoch:779 \n",
      " - Batch no: 0, train loss: 0.4595944285392761\n",
      " - Batch no: 8, train loss: 0.4663342833518982\n",
      " - Batch no: 16, train loss: 0.5704149603843689\n",
      " - Batch no: 24, train loss: 0.782883882522583\n",
      "-Validation loss after epoc: 0.4849802255630493 \n",
      "Epoch:780 \n",
      " - Batch no: 0, train loss: 0.43807724118232727\n",
      " - Batch no: 8, train loss: 0.5490772724151611\n",
      " - Batch no: 16, train loss: 0.5601596236228943\n",
      " - Batch no: 24, train loss: 0.6562902331352234\n",
      "-Validation loss after epoc: 0.4851675033569336 \n",
      "Epoch:781 \n",
      " - Batch no: 0, train loss: 0.6051009297370911\n",
      " - Batch no: 8, train loss: 0.49872031807899475\n",
      " - Batch no: 16, train loss: 0.5499354004859924\n",
      " - Batch no: 24, train loss: 0.6258687376976013\n",
      "-Validation loss after epoc: 0.48505789041519165 \n",
      "Epoch:782 \n",
      " - Batch no: 0, train loss: 0.5595993995666504\n",
      " - Batch no: 8, train loss: 0.5881804823875427\n",
      " - Batch no: 16, train loss: 0.6112063527107239\n",
      " - Batch no: 24, train loss: 0.43533143401145935\n",
      "-Validation loss after epoc: 0.485463410615921 \n",
      "Epoch:783 \n",
      " - Batch no: 0, train loss: 0.4710763096809387\n",
      " - Batch no: 8, train loss: 0.5470713973045349\n",
      " - Batch no: 16, train loss: 0.4461672604084015\n",
      " - Batch no: 24, train loss: 0.48213183879852295\n",
      "-Validation loss after epoc: 0.48500925302505493 \n",
      "Epoch:784 \n",
      " - Batch no: 0, train loss: 0.4266285002231598\n",
      " - Batch no: 8, train loss: 0.4075532853603363\n",
      " - Batch no: 16, train loss: 0.5731421709060669\n",
      " - Batch no: 24, train loss: 0.5435922145843506\n",
      "-Validation loss after epoc: 0.48525530099868774 \n",
      "Epoch:785 \n",
      " - Batch no: 0, train loss: 0.538229763507843\n",
      " - Batch no: 8, train loss: 0.6580747365951538\n",
      " - Batch no: 16, train loss: 0.5825085043907166\n",
      " - Batch no: 24, train loss: 0.7286878824234009\n",
      "-Validation loss after epoc: 0.4848972260951996 \n",
      "Epoch:786 \n",
      " - Batch no: 0, train loss: 0.4996274709701538\n",
      " - Batch no: 8, train loss: 0.4385640621185303\n",
      " - Batch no: 16, train loss: 0.41888707876205444\n",
      " - Batch no: 24, train loss: 0.6538589000701904\n",
      "-Validation loss after epoc: 0.4848165214061737 \n",
      "Epoch:787 \n",
      " - Batch no: 0, train loss: 0.45607057213783264\n",
      " - Batch no: 8, train loss: 0.655838131904602\n",
      " - Batch no: 16, train loss: 0.45982521772384644\n",
      " - Batch no: 24, train loss: 0.42208194732666016\n",
      "-Validation loss after epoc: 0.4847669005393982 \n",
      "Epoch:788 \n",
      " - Batch no: 0, train loss: 0.6269469261169434\n",
      " - Batch no: 8, train loss: 0.3580937087535858\n",
      " - Batch no: 16, train loss: 0.7013642191886902\n",
      " - Batch no: 24, train loss: 0.4331102967262268\n",
      "-Validation loss after epoc: 0.48452505469322205 \n",
      "Epoch:789 \n",
      " - Batch no: 0, train loss: 0.3623599410057068\n",
      " - Batch no: 8, train loss: 0.4485006630420685\n",
      " - Batch no: 16, train loss: 0.5185143351554871\n",
      " - Batch no: 24, train loss: 0.5710427165031433\n",
      "-Validation loss after epoc: 0.48437732458114624 \n",
      "Epoch:790 \n",
      " - Batch no: 0, train loss: 0.5137513279914856\n",
      " - Batch no: 8, train loss: 0.531389057636261\n",
      " - Batch no: 16, train loss: 0.5035874247550964\n",
      " - Batch no: 24, train loss: 0.4486125111579895\n",
      "-Validation loss after epoc: 0.4844973385334015 \n",
      "Epoch:791 \n",
      " - Batch no: 0, train loss: 0.5905000567436218\n",
      " - Batch no: 8, train loss: 0.5293005108833313\n",
      " - Batch no: 16, train loss: 0.4351218342781067\n",
      " - Batch no: 24, train loss: 0.5594829320907593\n",
      "-Validation loss after epoc: 0.48508912324905396 \n",
      "Epoch:792 \n",
      " - Batch no: 0, train loss: 0.5577812790870667\n",
      " - Batch no: 8, train loss: 0.5801810026168823\n",
      " - Batch no: 16, train loss: 0.4905458092689514\n",
      " - Batch no: 24, train loss: 0.6381323933601379\n",
      "-Validation loss after epoc: 0.4848685562610626 \n",
      "Epoch:793 \n",
      " - Batch no: 0, train loss: 0.554021418094635\n",
      " - Batch no: 8, train loss: 0.7618858814239502\n",
      " - Batch no: 16, train loss: 0.41978153586387634\n",
      " - Batch no: 24, train loss: 0.5095443725585938\n",
      "-Validation loss after epoc: 0.48468074202537537 \n",
      "Epoch:794 \n",
      " - Batch no: 0, train loss: 0.5196323394775391\n",
      " - Batch no: 8, train loss: 0.5199567079544067\n",
      " - Batch no: 16, train loss: 0.40009433031082153\n",
      " - Batch no: 24, train loss: 0.6264739036560059\n",
      "-Validation loss after epoc: 0.4843459129333496 \n",
      "Epoch:795 \n",
      " - Batch no: 0, train loss: 0.49273574352264404\n",
      " - Batch no: 8, train loss: 0.7064998745918274\n",
      " - Batch no: 16, train loss: 0.4390368163585663\n",
      " - Batch no: 24, train loss: 0.5577048063278198\n",
      "-Validation loss after epoc: 0.48431745171546936 \n",
      "Epoch:796 \n",
      " - Batch no: 0, train loss: 0.31939277052879333\n",
      " - Batch no: 8, train loss: 0.7824684977531433\n",
      " - Batch no: 16, train loss: 0.34632331132888794\n",
      " - Batch no: 24, train loss: 0.5086879730224609\n",
      "-Validation loss after epoc: 0.48430347442626953 \n",
      "Epoch:797 \n",
      " - Batch no: 0, train loss: 0.4519590735435486\n",
      " - Batch no: 8, train loss: 0.59820157289505\n",
      " - Batch no: 16, train loss: 0.582416296005249\n",
      " - Batch no: 24, train loss: 0.5120095014572144\n",
      "-Validation loss after epoc: 0.48421138525009155 \n",
      "Epoch:798 \n",
      " - Batch no: 0, train loss: 0.6885202527046204\n",
      " - Batch no: 8, train loss: 0.6118151545524597\n",
      " - Batch no: 16, train loss: 0.5802828669548035\n",
      " - Batch no: 24, train loss: 0.5466662645339966\n",
      "-Validation loss after epoc: 0.4843401610851288 \n",
      "Epoch:799 \n",
      " - Batch no: 0, train loss: 0.6193990707397461\n",
      " - Batch no: 8, train loss: 0.5966578125953674\n",
      " - Batch no: 16, train loss: 0.45987993478775024\n",
      " - Batch no: 24, train loss: 0.5264618992805481\n",
      "-Validation loss after epoc: 0.48383939266204834 \n",
      "Epoch:800 \n",
      " - Batch no: 0, train loss: 0.5183185935020447\n",
      " - Batch no: 8, train loss: 0.5093439817428589\n",
      " - Batch no: 16, train loss: 0.515850841999054\n",
      " - Batch no: 24, train loss: 0.48989784717559814\n",
      "-Validation loss after epoc: 0.4843609929084778 \n",
      "Epoch:801 \n",
      " - Batch no: 0, train loss: 0.6155675053596497\n",
      " - Batch no: 8, train loss: 0.5937258005142212\n",
      " - Batch no: 16, train loss: 0.4508989155292511\n",
      " - Batch no: 24, train loss: 0.5689553618431091\n",
      "-Validation loss after epoc: 0.4839763641357422 \n",
      "Epoch:802 \n",
      " - Batch no: 0, train loss: 0.5184193849563599\n",
      " - Batch no: 8, train loss: 0.5937817692756653\n",
      " - Batch no: 16, train loss: 0.4884699583053589\n",
      " - Batch no: 24, train loss: 0.5560334920883179\n",
      "-Validation loss after epoc: 0.48435425758361816 \n",
      "Epoch:803 \n",
      " - Batch no: 0, train loss: 0.5456884503364563\n",
      " - Batch no: 8, train loss: 0.5157142281532288\n",
      " - Batch no: 16, train loss: 0.6724995970726013\n",
      " - Batch no: 24, train loss: 0.48831573128700256\n",
      "-Validation loss after epoc: 0.4839923679828644 \n",
      "Epoch:804 \n",
      " - Batch no: 0, train loss: 0.7079581022262573\n",
      " - Batch no: 8, train loss: 0.8457524180412292\n",
      " - Batch no: 16, train loss: 0.5500738620758057\n",
      " - Batch no: 24, train loss: 0.41096457839012146\n",
      "-Validation loss after epoc: 0.48432686924934387 \n",
      "Epoch:805 \n",
      " - Batch no: 0, train loss: 0.5140211582183838\n",
      " - Batch no: 8, train loss: 0.5764936208724976\n",
      " - Batch no: 16, train loss: 0.5452202558517456\n",
      " - Batch no: 24, train loss: 0.5043557286262512\n",
      "-Validation loss after epoc: 0.4840414822101593 \n",
      "Epoch:806 \n",
      " - Batch no: 0, train loss: 0.4801195561885834\n",
      " - Batch no: 8, train loss: 0.5997744798660278\n",
      " - Batch no: 16, train loss: 0.5829656720161438\n",
      " - Batch no: 24, train loss: 0.5797324776649475\n",
      "-Validation loss after epoc: 0.4846735894680023 \n",
      "Epoch:807 \n",
      " - Batch no: 0, train loss: 0.5103676915168762\n",
      " - Batch no: 8, train loss: 0.6950941681861877\n",
      " - Batch no: 16, train loss: 0.6082732677459717\n",
      " - Batch no: 24, train loss: 0.597460925579071\n",
      "-Validation loss after epoc: 0.48480844497680664 \n",
      "Epoch:808 \n",
      " - Batch no: 0, train loss: 0.5671204924583435\n",
      " - Batch no: 8, train loss: 0.5494364500045776\n",
      " - Batch no: 16, train loss: 0.5374437570571899\n",
      " - Batch no: 24, train loss: 0.5167255997657776\n",
      "-Validation loss after epoc: 0.48417261242866516 \n",
      "Epoch:809 \n",
      " - Batch no: 0, train loss: 0.679411768913269\n",
      " - Batch no: 8, train loss: 0.5626285672187805\n",
      " - Batch no: 16, train loss: 0.4545200765132904\n",
      " - Batch no: 24, train loss: 0.5438967943191528\n",
      "-Validation loss after epoc: 0.48431307077407837 \n",
      "Epoch:810 \n",
      " - Batch no: 0, train loss: 0.36472901701927185\n",
      " - Batch no: 8, train loss: 0.35773301124572754\n",
      " - Batch no: 16, train loss: 0.6794454455375671\n",
      " - Batch no: 24, train loss: 0.42815348505973816\n",
      "-Validation loss after epoc: 0.4840264320373535 \n",
      "Epoch:811 \n",
      " - Batch no: 0, train loss: 0.42385321855545044\n",
      " - Batch no: 8, train loss: 0.5162678360939026\n",
      " - Batch no: 16, train loss: 0.5574726462364197\n",
      " - Batch no: 24, train loss: 0.5711812973022461\n",
      "-Validation loss after epoc: 0.48378029465675354 \n",
      "Epoch:812 \n",
      " - Batch no: 0, train loss: 0.6718597412109375\n",
      " - Batch no: 8, train loss: 0.47891685366630554\n",
      " - Batch no: 16, train loss: 0.5919277667999268\n",
      " - Batch no: 24, train loss: 0.5379778742790222\n",
      "-Validation loss after epoc: 0.483473539352417 \n",
      "Epoch:813 \n",
      " - Batch no: 0, train loss: 0.5512444376945496\n",
      " - Batch no: 8, train loss: 0.5390783548355103\n",
      " - Batch no: 16, train loss: 0.6307632327079773\n",
      " - Batch no: 24, train loss: 0.6206190586090088\n",
      "-Validation loss after epoc: 0.4836849570274353 \n",
      "Epoch:814 \n",
      " - Batch no: 0, train loss: 0.5638495683670044\n",
      " - Batch no: 8, train loss: 0.435920387506485\n",
      " - Batch no: 16, train loss: 0.49660244584083557\n",
      " - Batch no: 24, train loss: 0.6089264154434204\n",
      "-Validation loss after epoc: 0.4846729338169098 \n",
      "Epoch:815 \n",
      " - Batch no: 0, train loss: 0.3594414293766022\n",
      " - Batch no: 8, train loss: 0.5591598153114319\n",
      " - Batch no: 16, train loss: 0.3986832797527313\n",
      " - Batch no: 24, train loss: 0.7026603817939758\n",
      "-Validation loss after epoc: 0.4840315878391266 \n",
      "Epoch:816 \n",
      " - Batch no: 0, train loss: 0.5575578212738037\n",
      " - Batch no: 8, train loss: 0.701832115650177\n",
      " - Batch no: 16, train loss: 0.5973822474479675\n",
      " - Batch no: 24, train loss: 0.5218470692634583\n",
      "-Validation loss after epoc: 0.4838609993457794 \n",
      "Epoch:817 \n",
      " - Batch no: 0, train loss: 0.5379610061645508\n",
      " - Batch no: 8, train loss: 0.6759994029998779\n",
      " - Batch no: 16, train loss: 0.5083431005477905\n",
      " - Batch no: 24, train loss: 0.4416372776031494\n",
      "-Validation loss after epoc: 0.4841795265674591 \n",
      "Epoch:818 \n",
      " - Batch no: 0, train loss: 0.4322166442871094\n",
      " - Batch no: 8, train loss: 0.4564460217952728\n",
      " - Batch no: 16, train loss: 0.5892317891120911\n",
      " - Batch no: 24, train loss: 0.7248749136924744\n",
      "-Validation loss after epoc: 0.48416343331336975 \n",
      "Epoch:819 \n",
      " - Batch no: 0, train loss: 0.349217027425766\n",
      " - Batch no: 8, train loss: 0.6481743454933167\n",
      " - Batch no: 16, train loss: 0.536193311214447\n",
      " - Batch no: 24, train loss: 0.40474408864974976\n",
      "-Validation loss after epoc: 0.48379892110824585 \n",
      "Epoch:820 \n",
      " - Batch no: 0, train loss: 0.6184111833572388\n",
      " - Batch no: 8, train loss: 0.811604917049408\n",
      " - Batch no: 16, train loss: 0.6229156255722046\n",
      " - Batch no: 24, train loss: 0.4374944865703583\n",
      "-Validation loss after epoc: 0.4833747148513794 \n",
      "Epoch:821 \n",
      " - Batch no: 0, train loss: 0.5934491157531738\n",
      " - Batch no: 8, train loss: 0.5225889086723328\n",
      " - Batch no: 16, train loss: 0.5493118166923523\n",
      " - Batch no: 24, train loss: 0.4754859209060669\n",
      "-Validation loss after epoc: 0.48345062136650085 \n",
      "Epoch:822 \n",
      " - Batch no: 0, train loss: 0.5632889270782471\n",
      " - Batch no: 8, train loss: 0.44033128023147583\n",
      " - Batch no: 16, train loss: 0.6334667801856995\n",
      " - Batch no: 24, train loss: 0.5905194282531738\n",
      "-Validation loss after epoc: 0.48375988006591797 \n",
      "Epoch:823 \n",
      " - Batch no: 0, train loss: 0.5282987952232361\n",
      " - Batch no: 8, train loss: 0.5690901279449463\n",
      " - Batch no: 16, train loss: 0.7215464115142822\n",
      " - Batch no: 24, train loss: 0.5447864532470703\n",
      "-Validation loss after epoc: 0.48436877131462097 \n",
      "Epoch:824 \n",
      " - Batch no: 0, train loss: 0.4387821555137634\n",
      " - Batch no: 8, train loss: 0.48432838916778564\n",
      " - Batch no: 16, train loss: 0.5773876905441284\n",
      " - Batch no: 24, train loss: 0.586798906326294\n",
      "-Validation loss after epoc: 0.4838503897190094 \n",
      "Epoch:825 \n",
      " - Batch no: 0, train loss: 0.43514376878738403\n",
      " - Batch no: 8, train loss: 0.6967195272445679\n",
      " - Batch no: 16, train loss: 0.5460011959075928\n",
      " - Batch no: 24, train loss: 0.6671363115310669\n",
      "-Validation loss after epoc: 0.48343443870544434 \n",
      "Epoch:826 \n",
      " - Batch no: 0, train loss: 0.4498283565044403\n",
      " - Batch no: 8, train loss: 0.49188485741615295\n",
      " - Batch no: 16, train loss: 0.49289506673812866\n",
      " - Batch no: 24, train loss: 0.6680305600166321\n",
      "-Validation loss after epoc: 0.4834252893924713 \n",
      "Epoch:827 \n",
      " - Batch no: 0, train loss: 0.4273192882537842\n",
      " - Batch no: 8, train loss: 0.4719181954860687\n",
      " - Batch no: 16, train loss: 0.5679998397827148\n",
      " - Batch no: 24, train loss: 0.514815092086792\n",
      "-Validation loss after epoc: 0.4837063252925873 \n",
      "Epoch:828 \n",
      " - Batch no: 0, train loss: 0.5154427289962769\n",
      " - Batch no: 8, train loss: 0.6130030751228333\n",
      " - Batch no: 16, train loss: 0.47113335132598877\n",
      " - Batch no: 24, train loss: 0.704732358455658\n",
      "-Validation loss after epoc: 0.48367977142333984 \n",
      "Epoch:829 \n",
      " - Batch no: 0, train loss: 0.6050625443458557\n",
      " - Batch no: 8, train loss: 0.45393702387809753\n",
      " - Batch no: 16, train loss: 0.617568850517273\n",
      " - Batch no: 24, train loss: 0.4299677014350891\n",
      "-Validation loss after epoc: 0.4837249517440796 \n",
      "Epoch:830 \n",
      " - Batch no: 0, train loss: 0.4534626007080078\n",
      " - Batch no: 8, train loss: 0.5518851280212402\n",
      " - Batch no: 16, train loss: 0.5087528228759766\n",
      " - Batch no: 24, train loss: 0.4256349205970764\n",
      "-Validation loss after epoc: 0.48362013697624207 \n",
      "Epoch:831 \n",
      " - Batch no: 0, train loss: 0.5972070097923279\n",
      " - Batch no: 8, train loss: 0.6439001560211182\n",
      " - Batch no: 16, train loss: 0.490875244140625\n",
      " - Batch no: 24, train loss: 0.7665324211120605\n",
      "-Validation loss after epoc: 0.48305243253707886 \n",
      "Epoch:832 \n",
      " - Batch no: 0, train loss: 0.5176857709884644\n",
      " - Batch no: 8, train loss: 0.5902199149131775\n",
      " - Batch no: 16, train loss: 0.4337131083011627\n",
      " - Batch no: 24, train loss: 0.7167682647705078\n",
      "-Validation loss after epoc: 0.4841606616973877 \n",
      "Epoch:833 \n",
      " - Batch no: 0, train loss: 0.5423951745033264\n",
      " - Batch no: 8, train loss: 0.5761122107505798\n",
      " - Batch no: 16, train loss: 0.6336541771888733\n",
      " - Batch no: 24, train loss: 0.5089411735534668\n",
      "-Validation loss after epoc: 0.4837478995323181 \n",
      "Epoch:834 \n",
      " - Batch no: 0, train loss: 0.48331722617149353\n",
      " - Batch no: 8, train loss: 0.5980491638183594\n",
      " - Batch no: 16, train loss: 0.6005479097366333\n",
      " - Batch no: 24, train loss: 0.43029484152793884\n",
      "-Validation loss after epoc: 0.4840428829193115 \n",
      "Epoch:835 \n",
      " - Batch no: 0, train loss: 0.6863253712654114\n",
      " - Batch no: 8, train loss: 0.5707566142082214\n",
      " - Batch no: 16, train loss: 0.4206359088420868\n",
      " - Batch no: 24, train loss: 0.4804464876651764\n",
      "-Validation loss after epoc: 0.4836021065711975 \n",
      "Epoch:836 \n",
      " - Batch no: 0, train loss: 0.40628281235694885\n",
      " - Batch no: 8, train loss: 0.5050066113471985\n",
      " - Batch no: 16, train loss: 0.527153491973877\n",
      " - Batch no: 24, train loss: 0.6560160517692566\n",
      "-Validation loss after epoc: 0.48366644978523254 \n",
      "Epoch:837 \n",
      " - Batch no: 0, train loss: 0.392871618270874\n",
      " - Batch no: 8, train loss: 0.4869142472743988\n",
      " - Batch no: 16, train loss: 0.48557335138320923\n",
      " - Batch no: 24, train loss: 0.6030439734458923\n",
      "-Validation loss after epoc: 0.4835774302482605 \n",
      "Epoch:838 \n",
      " - Batch no: 0, train loss: 0.6160977482795715\n",
      " - Batch no: 8, train loss: 0.47421297430992126\n",
      " - Batch no: 16, train loss: 0.5804481506347656\n",
      " - Batch no: 24, train loss: 0.48754242062568665\n",
      "-Validation loss after epoc: 0.48375362157821655 \n",
      "Epoch:839 \n",
      " - Batch no: 0, train loss: 0.6568533182144165\n",
      " - Batch no: 8, train loss: 0.47442615032196045\n",
      " - Batch no: 16, train loss: 0.4211757183074951\n",
      " - Batch no: 24, train loss: 0.49669843912124634\n",
      "-Validation loss after epoc: 0.48331117630004883 \n",
      "Epoch:840 \n",
      " - Batch no: 0, train loss: 0.7144563794136047\n",
      " - Batch no: 8, train loss: 0.6686573028564453\n",
      " - Batch no: 16, train loss: 0.6230347752571106\n",
      " - Batch no: 24, train loss: 0.3751470148563385\n",
      "-Validation loss after epoc: 0.48340147733688354 \n",
      "Epoch:841 \n",
      " - Batch no: 0, train loss: 0.4835401177406311\n",
      " - Batch no: 8, train loss: 0.6346685886383057\n",
      " - Batch no: 16, train loss: 0.46620920300483704\n",
      " - Batch no: 24, train loss: 0.5454017519950867\n",
      "-Validation loss after epoc: 0.4833854138851166 \n",
      "Epoch:842 \n",
      " - Batch no: 0, train loss: 0.4953334629535675\n",
      " - Batch no: 8, train loss: 0.44538968801498413\n",
      " - Batch no: 16, train loss: 0.5233069062232971\n",
      " - Batch no: 24, train loss: 0.5056689977645874\n",
      "-Validation loss after epoc: 0.4834579825401306 \n",
      "Epoch:843 \n",
      " - Batch no: 0, train loss: 0.5652319192886353\n",
      " - Batch no: 8, train loss: 0.6687319874763489\n",
      " - Batch no: 16, train loss: 0.7005395293235779\n",
      " - Batch no: 24, train loss: 0.6193258762359619\n",
      "-Validation loss after epoc: 0.48298799991607666 \n",
      "Epoch:844 \n",
      " - Batch no: 0, train loss: 0.7252534031867981\n",
      " - Batch no: 8, train loss: 0.5508482456207275\n",
      " - Batch no: 16, train loss: 0.48446595668792725\n",
      " - Batch no: 24, train loss: 0.6712342500686646\n",
      "-Validation loss after epoc: 0.4833580255508423 \n",
      "Epoch:845 \n",
      " - Batch no: 0, train loss: 0.7046887874603271\n",
      " - Batch no: 8, train loss: 0.5626809597015381\n",
      " - Batch no: 16, train loss: 0.7325729131698608\n",
      " - Batch no: 24, train loss: 0.5392615795135498\n",
      "-Validation loss after epoc: 0.48312094807624817 \n",
      "Epoch:846 \n",
      " - Batch no: 0, train loss: 0.5087611675262451\n",
      " - Batch no: 8, train loss: 0.7697476744651794\n",
      " - Batch no: 16, train loss: 0.6511605381965637\n",
      " - Batch no: 24, train loss: 0.5884113907814026\n",
      "-Validation loss after epoc: 0.48311683535575867 \n",
      "Epoch:847 \n",
      " - Batch no: 0, train loss: 0.46456602215766907\n",
      " - Batch no: 8, train loss: 0.390034019947052\n",
      " - Batch no: 16, train loss: 0.6375380754470825\n",
      " - Batch no: 24, train loss: 0.5018884539604187\n",
      "-Validation loss after epoc: 0.4828333258628845 \n",
      "Epoch:848 \n",
      " - Batch no: 0, train loss: 0.33113908767700195\n",
      " - Batch no: 8, train loss: 0.7044774293899536\n",
      " - Batch no: 16, train loss: 0.6964387893676758\n",
      " - Batch no: 24, train loss: 0.4529733657836914\n",
      "-Validation loss after epoc: 0.4828544855117798 \n",
      "Epoch:849 \n",
      " - Batch no: 0, train loss: 0.4931701421737671\n",
      " - Batch no: 8, train loss: 0.45242828130722046\n",
      " - Batch no: 16, train loss: 0.5559866428375244\n",
      " - Batch no: 24, train loss: 0.7207399010658264\n",
      "-Validation loss after epoc: 0.4840203821659088 \n",
      "Epoch:850 \n",
      " - Batch no: 0, train loss: 0.47127965092658997\n",
      " - Batch no: 8, train loss: 0.5134897828102112\n",
      " - Batch no: 16, train loss: 0.44154220819473267\n",
      " - Batch no: 24, train loss: 0.5990499258041382\n",
      "-Validation loss after epoc: 0.4826313257217407 \n",
      "Epoch:851 \n",
      " - Batch no: 0, train loss: 0.8442009687423706\n",
      " - Batch no: 8, train loss: 0.4827319383621216\n",
      " - Batch no: 16, train loss: 0.48390910029411316\n",
      " - Batch no: 24, train loss: 0.4830910563468933\n",
      "-Validation loss after epoc: 0.4826388359069824 \n",
      "Epoch:852 \n",
      " - Batch no: 0, train loss: 0.6805880665779114\n",
      " - Batch no: 8, train loss: 0.6653106808662415\n",
      " - Batch no: 16, train loss: 0.5139982104301453\n",
      " - Batch no: 24, train loss: 0.47862350940704346\n",
      "-Validation loss after epoc: 0.4825294017791748 \n",
      "Epoch:853 \n",
      " - Batch no: 0, train loss: 0.6426106691360474\n",
      " - Batch no: 8, train loss: 0.5916550159454346\n",
      " - Batch no: 16, train loss: 0.34714698791503906\n",
      " - Batch no: 24, train loss: 0.4205659031867981\n",
      "-Validation loss after epoc: 0.48272231221199036 \n",
      "Epoch:854 \n",
      " - Batch no: 0, train loss: 0.541799783706665\n",
      " - Batch no: 8, train loss: 0.40779009461402893\n",
      " - Batch no: 16, train loss: 0.7177683711051941\n",
      " - Batch no: 24, train loss: 0.4607636630535126\n",
      "-Validation loss after epoc: 0.48233455419540405 \n",
      "Epoch:855 \n",
      " - Batch no: 0, train loss: 0.5735383033752441\n",
      " - Batch no: 8, train loss: 0.8833380937576294\n",
      " - Batch no: 16, train loss: 0.6936749219894409\n",
      " - Batch no: 24, train loss: 0.4825787842273712\n",
      "-Validation loss after epoc: 0.4825863540172577 \n",
      "Epoch:856 \n",
      " - Batch no: 0, train loss: 0.4611576199531555\n",
      " - Batch no: 8, train loss: 0.35280144214630127\n",
      " - Batch no: 16, train loss: 0.49454399943351746\n",
      " - Batch no: 24, train loss: 0.5926632881164551\n",
      "-Validation loss after epoc: 0.48262670636177063 \n",
      "Epoch:857 \n",
      " - Batch no: 0, train loss: 0.6279656887054443\n",
      " - Batch no: 8, train loss: 0.4305700659751892\n",
      " - Batch no: 16, train loss: 0.43171966075897217\n",
      " - Batch no: 24, train loss: 0.6292159557342529\n",
      "-Validation loss after epoc: 0.48237884044647217 \n",
      "Epoch:858 \n",
      " - Batch no: 0, train loss: 0.6501161456108093\n",
      " - Batch no: 8, train loss: 0.47689318656921387\n",
      " - Batch no: 16, train loss: 0.49910593032836914\n",
      " - Batch no: 24, train loss: 0.3666808605194092\n",
      "-Validation loss after epoc: 0.4824407398700714 \n",
      "Epoch:859 \n",
      " - Batch no: 0, train loss: 0.5276803374290466\n",
      " - Batch no: 8, train loss: 0.5435865521430969\n",
      " - Batch no: 16, train loss: 0.43899020552635193\n",
      " - Batch no: 24, train loss: 0.3675471246242523\n",
      "-Validation loss after epoc: 0.4827377200126648 \n",
      "Epoch:860 \n",
      " - Batch no: 0, train loss: 0.5069506764411926\n",
      " - Batch no: 8, train loss: 0.5596128106117249\n",
      " - Batch no: 16, train loss: 0.8285397887229919\n",
      " - Batch no: 24, train loss: 0.5051071643829346\n",
      "-Validation loss after epoc: 0.4823971390724182 \n",
      "Epoch:861 \n",
      " - Batch no: 0, train loss: 0.40516915917396545\n",
      " - Batch no: 8, train loss: 0.5116536617279053\n",
      " - Batch no: 16, train loss: 0.4941585958003998\n",
      " - Batch no: 24, train loss: 0.564401388168335\n",
      "-Validation loss after epoc: 0.4824206233024597 \n",
      "Epoch:862 \n",
      " - Batch no: 0, train loss: 0.43156611919403076\n",
      " - Batch no: 8, train loss: 0.38083311915397644\n",
      " - Batch no: 16, train loss: 0.7204322218894958\n",
      " - Batch no: 24, train loss: 0.547161340713501\n",
      "-Validation loss after epoc: 0.4822305142879486 \n",
      "Epoch:863 \n",
      " - Batch no: 0, train loss: 0.47860994935035706\n",
      " - Batch no: 8, train loss: 0.6027481555938721\n",
      " - Batch no: 16, train loss: 0.6412860751152039\n",
      " - Batch no: 24, train loss: 0.5402511954307556\n",
      "-Validation loss after epoc: 0.48264065384864807 \n",
      "Epoch:864 \n",
      " - Batch no: 0, train loss: 0.4164734184741974\n",
      " - Batch no: 8, train loss: 0.36635714769363403\n",
      " - Batch no: 16, train loss: 0.46902716159820557\n",
      " - Batch no: 24, train loss: 0.5189670920372009\n",
      "-Validation loss after epoc: 0.4826151728630066 \n",
      "Epoch:865 \n",
      " - Batch no: 0, train loss: 0.38077184557914734\n",
      " - Batch no: 8, train loss: 0.49365508556365967\n",
      " - Batch no: 16, train loss: 0.5520482659339905\n",
      " - Batch no: 24, train loss: 0.5373740196228027\n",
      "-Validation loss after epoc: 0.4829122722148895 \n",
      "Epoch:866 \n",
      " - Batch no: 0, train loss: 0.5692185163497925\n",
      " - Batch no: 8, train loss: 0.5732893943786621\n",
      " - Batch no: 16, train loss: 0.5569103956222534\n",
      " - Batch no: 24, train loss: 0.6426710486412048\n",
      "-Validation loss after epoc: 0.4826244115829468 \n",
      "Epoch:867 \n",
      " - Batch no: 0, train loss: 0.42048096656799316\n",
      " - Batch no: 8, train loss: 0.5822093486785889\n",
      " - Batch no: 16, train loss: 0.6181179285049438\n",
      " - Batch no: 24, train loss: 0.45557889342308044\n",
      "-Validation loss after epoc: 0.4826747477054596 \n",
      "Epoch:868 \n",
      " - Batch no: 0, train loss: 0.36575138568878174\n",
      " - Batch no: 8, train loss: 0.5896656513214111\n",
      " - Batch no: 16, train loss: 0.4654885530471802\n",
      " - Batch no: 24, train loss: 0.5327386260032654\n",
      "-Validation loss after epoc: 0.48227280378341675 \n",
      "Epoch:869 \n",
      " - Batch no: 0, train loss: 0.46700721979141235\n",
      " - Batch no: 8, train loss: 0.6452414393424988\n",
      " - Batch no: 16, train loss: 0.43172723054885864\n",
      " - Batch no: 24, train loss: 0.5246618390083313\n",
      "-Validation loss after epoc: 0.48268401622772217 \n",
      "Epoch:870 \n",
      " - Batch no: 0, train loss: 0.6404253840446472\n",
      " - Batch no: 8, train loss: 0.4525611102581024\n",
      " - Batch no: 16, train loss: 0.443871408700943\n",
      " - Batch no: 24, train loss: 0.6636168956756592\n",
      "-Validation loss after epoc: 0.48207569122314453 \n",
      "Epoch:871 \n",
      " - Batch no: 0, train loss: 0.45581385493278503\n",
      " - Batch no: 8, train loss: 0.3916015028953552\n",
      " - Batch no: 16, train loss: 0.6462313532829285\n",
      " - Batch no: 24, train loss: 0.4693642854690552\n",
      "-Validation loss after epoc: 0.48199665546417236 \n",
      "Epoch:872 \n",
      " - Batch no: 0, train loss: 0.7720466256141663\n",
      " - Batch no: 8, train loss: 0.617743968963623\n",
      " - Batch no: 16, train loss: 0.45648637413978577\n",
      " - Batch no: 24, train loss: 0.5065325498580933\n",
      "-Validation loss after epoc: 0.4818735420703888 \n",
      "Epoch:873 \n",
      " - Batch no: 0, train loss: 0.5294709801673889\n",
      " - Batch no: 8, train loss: 0.48415714502334595\n",
      " - Batch no: 16, train loss: 0.6621475219726562\n",
      " - Batch no: 24, train loss: 0.6902332305908203\n",
      "-Validation loss after epoc: 0.48186132311820984 \n",
      "Epoch:874 \n",
      " - Batch no: 0, train loss: 0.5281147956848145\n",
      " - Batch no: 8, train loss: 0.5925950407981873\n",
      " - Batch no: 16, train loss: 0.5414947867393494\n",
      " - Batch no: 24, train loss: 0.5066787004470825\n",
      "-Validation loss after epoc: 0.4820949137210846 \n",
      "Epoch:875 \n",
      " - Batch no: 0, train loss: 0.5078670382499695\n",
      " - Batch no: 8, train loss: 0.45756766200065613\n",
      " - Batch no: 16, train loss: 0.49246057868003845\n",
      " - Batch no: 24, train loss: 0.7806923389434814\n",
      "-Validation loss after epoc: 0.4819982349872589 \n",
      "Epoch:876 \n",
      " - Batch no: 0, train loss: 0.4917197525501251\n",
      " - Batch no: 8, train loss: 0.6580058336257935\n",
      " - Batch no: 16, train loss: 0.828555166721344\n",
      " - Batch no: 24, train loss: 0.44157952070236206\n",
      "-Validation loss after epoc: 0.48232972621917725 \n",
      "Epoch:877 \n",
      " - Batch no: 0, train loss: 0.47802481055259705\n",
      " - Batch no: 8, train loss: 0.598207414150238\n",
      " - Batch no: 16, train loss: 0.4575536549091339\n",
      " - Batch no: 24, train loss: 0.6163026094436646\n",
      "-Validation loss after epoc: 0.482265830039978 \n",
      "Epoch:878 \n",
      " - Batch no: 0, train loss: 0.5950825214385986\n",
      " - Batch no: 8, train loss: 0.6360757350921631\n",
      " - Batch no: 16, train loss: 0.3972724974155426\n",
      " - Batch no: 24, train loss: 0.4898698031902313\n",
      "-Validation loss after epoc: 0.4822169542312622 \n",
      "Epoch:879 \n",
      " - Batch no: 0, train loss: 0.4067700505256653\n",
      " - Batch no: 8, train loss: 0.38120102882385254\n",
      " - Batch no: 16, train loss: 0.385799765586853\n",
      " - Batch no: 24, train loss: 0.6328075528144836\n",
      "-Validation loss after epoc: 0.4822351336479187 \n",
      "Epoch:880 \n",
      " - Batch no: 0, train loss: 0.5466965436935425\n",
      " - Batch no: 8, train loss: 0.568509042263031\n",
      " - Batch no: 16, train loss: 0.5515704154968262\n",
      " - Batch no: 24, train loss: 0.5651788711547852\n",
      "-Validation loss after epoc: 0.4819667637348175 \n",
      "Epoch:881 \n",
      " - Batch no: 0, train loss: 0.5420676469802856\n",
      " - Batch no: 8, train loss: 0.3578779101371765\n",
      " - Batch no: 16, train loss: 0.40462568402290344\n",
      " - Batch no: 24, train loss: 0.5073132514953613\n",
      "-Validation loss after epoc: 0.4824046790599823 \n",
      "Epoch:882 \n",
      " - Batch no: 0, train loss: 0.6153337359428406\n",
      " - Batch no: 8, train loss: 0.5790520906448364\n",
      " - Batch no: 16, train loss: 0.4789140224456787\n",
      " - Batch no: 24, train loss: 0.3473864793777466\n",
      "-Validation loss after epoc: 0.48271316289901733 \n",
      "Epoch:883 \n",
      " - Batch no: 0, train loss: 0.4821459650993347\n",
      " - Batch no: 8, train loss: 0.3578755557537079\n",
      " - Batch no: 16, train loss: 0.5725436806678772\n",
      " - Batch no: 24, train loss: 0.40459492802619934\n",
      "-Validation loss after epoc: 0.4824226498603821 \n",
      "Epoch:884 \n",
      " - Batch no: 0, train loss: 0.4626408517360687\n",
      " - Batch no: 8, train loss: 0.6649959683418274\n",
      " - Batch no: 16, train loss: 0.5930736660957336\n",
      " - Batch no: 24, train loss: 0.5605413913726807\n",
      "-Validation loss after epoc: 0.4821721613407135 \n",
      "Epoch:885 \n",
      " - Batch no: 0, train loss: 0.4514211416244507\n",
      " - Batch no: 8, train loss: 0.745887279510498\n",
      " - Batch no: 16, train loss: 0.47439566254615784\n",
      " - Batch no: 24, train loss: 0.5057316422462463\n",
      "-Validation loss after epoc: 0.48217499256134033 \n",
      "Epoch:886 \n",
      " - Batch no: 0, train loss: 0.5400165319442749\n",
      " - Batch no: 8, train loss: 0.4789661169052124\n",
      " - Batch no: 16, train loss: 0.5217761397361755\n",
      " - Batch no: 24, train loss: 0.7254725098609924\n",
      "-Validation loss after epoc: 0.48228365182876587 \n",
      "Epoch:887 \n",
      " - Batch no: 0, train loss: 0.2945084571838379\n",
      " - Batch no: 8, train loss: 0.6286548376083374\n",
      " - Batch no: 16, train loss: 0.5209057927131653\n",
      " - Batch no: 24, train loss: 0.6045661568641663\n",
      "-Validation loss after epoc: 0.4821893274784088 \n",
      "Epoch:888 \n",
      " - Batch no: 0, train loss: 0.3805108964443207\n",
      " - Batch no: 8, train loss: 0.5841190814971924\n",
      " - Batch no: 16, train loss: 0.6092572212219238\n",
      " - Batch no: 24, train loss: 0.4437362849712372\n",
      "-Validation loss after epoc: 0.48163798451423645 \n",
      "Epoch:889 \n",
      " - Batch no: 0, train loss: 0.4991165101528168\n",
      " - Batch no: 8, train loss: 0.6531457901000977\n",
      " - Batch no: 16, train loss: 0.4821254312992096\n",
      " - Batch no: 24, train loss: 0.35525640845298767\n",
      "-Validation loss after epoc: 0.4818118214607239 \n",
      "Epoch:890 \n",
      " - Batch no: 0, train loss: 0.5935120582580566\n",
      " - Batch no: 8, train loss: 0.5568839311599731\n",
      " - Batch no: 16, train loss: 0.4824990928173065\n",
      " - Batch no: 24, train loss: 0.36714935302734375\n",
      "-Validation loss after epoc: 0.48178762197494507 \n",
      "Epoch:891 \n",
      " - Batch no: 0, train loss: 0.8082634210586548\n",
      " - Batch no: 8, train loss: 0.3724111318588257\n",
      " - Batch no: 16, train loss: 0.6898841857910156\n",
      " - Batch no: 24, train loss: 0.5122582912445068\n",
      "-Validation loss after epoc: 0.4815177321434021 \n",
      "Epoch:892 \n",
      " - Batch no: 0, train loss: 0.5928962826728821\n",
      " - Batch no: 8, train loss: 0.49887630343437195\n",
      " - Batch no: 16, train loss: 0.5892918705940247\n",
      " - Batch no: 24, train loss: 0.3926522135734558\n",
      "-Validation loss after epoc: 0.48195773363113403 \n",
      "Epoch:893 \n",
      " - Batch no: 0, train loss: 0.34965330362319946\n",
      " - Batch no: 8, train loss: 0.5733477473258972\n",
      " - Batch no: 16, train loss: 0.9012511968612671\n",
      " - Batch no: 24, train loss: 0.461786150932312\n",
      "-Validation loss after epoc: 0.4816106855869293 \n",
      "Epoch:894 \n",
      " - Batch no: 0, train loss: 0.4634191691875458\n",
      " - Batch no: 8, train loss: 0.5151857137680054\n",
      " - Batch no: 16, train loss: 0.7072004079818726\n",
      " - Batch no: 24, train loss: 0.35625728964805603\n",
      "-Validation loss after epoc: 0.4817660450935364 \n",
      "Epoch:895 \n",
      " - Batch no: 0, train loss: 0.7416163086891174\n",
      " - Batch no: 8, train loss: 0.4365053176879883\n",
      " - Batch no: 16, train loss: 0.512427031993866\n",
      " - Batch no: 24, train loss: 0.5031048655509949\n",
      "-Validation loss after epoc: 0.4812949001789093 \n",
      "Epoch:896 \n",
      " - Batch no: 0, train loss: 0.5118891000747681\n",
      " - Batch no: 8, train loss: 0.34891703724861145\n",
      " - Batch no: 16, train loss: 0.5943151712417603\n",
      " - Batch no: 24, train loss: 0.5716546773910522\n",
      "-Validation loss after epoc: 0.48168182373046875 \n",
      "Epoch:897 \n",
      " - Batch no: 0, train loss: 0.5250212550163269\n",
      " - Batch no: 8, train loss: 0.5111083984375\n",
      " - Batch no: 16, train loss: 0.7023154497146606\n",
      " - Batch no: 24, train loss: 0.46823614835739136\n",
      "-Validation loss after epoc: 0.48151254653930664 \n",
      "Epoch:898 \n",
      " - Batch no: 0, train loss: 0.5901866555213928\n",
      " - Batch no: 8, train loss: 0.5581797957420349\n",
      " - Batch no: 16, train loss: 0.5260886549949646\n",
      " - Batch no: 24, train loss: 0.456999808549881\n",
      "-Validation loss after epoc: 0.4815722405910492 \n",
      "Epoch:899 \n",
      " - Batch no: 0, train loss: 0.43177640438079834\n",
      " - Batch no: 8, train loss: 0.3018144369125366\n",
      " - Batch no: 16, train loss: 0.47085288166999817\n",
      " - Batch no: 24, train loss: 0.4587511122226715\n",
      "-Validation loss after epoc: 0.4817471504211426 \n",
      "Epoch:900 \n",
      " - Batch no: 0, train loss: 0.4041960835456848\n",
      " - Batch no: 8, train loss: 0.4560728073120117\n",
      " - Batch no: 16, train loss: 0.6306103467941284\n",
      " - Batch no: 24, train loss: 0.4962809979915619\n",
      "-Validation loss after epoc: 0.48137062788009644 \n",
      "Epoch:901 \n",
      " - Batch no: 0, train loss: 0.34149256348609924\n",
      " - Batch no: 8, train loss: 0.48156967759132385\n",
      " - Batch no: 16, train loss: 0.3474939465522766\n",
      " - Batch no: 24, train loss: 0.7619132399559021\n",
      "-Validation loss after epoc: 0.48228293657302856 \n",
      "Epoch:902 \n",
      " - Batch no: 0, train loss: 0.6411322355270386\n",
      " - Batch no: 8, train loss: 0.5537074208259583\n",
      " - Batch no: 16, train loss: 0.5328341722488403\n",
      " - Batch no: 24, train loss: 0.5437189340591431\n",
      "-Validation loss after epoc: 0.48319822549819946 \n",
      "Epoch:903 \n",
      " - Batch no: 0, train loss: 0.5835921168327332\n",
      " - Batch no: 8, train loss: 0.6268543601036072\n",
      " - Batch no: 16, train loss: 0.5453835129737854\n",
      " - Batch no: 24, train loss: 0.4979032576084137\n",
      "-Validation loss after epoc: 0.4814409613609314 \n",
      "Epoch:904 \n",
      " - Batch no: 0, train loss: 0.6281595826148987\n",
      " - Batch no: 8, train loss: 0.6183245182037354\n",
      " - Batch no: 16, train loss: 0.4345058798789978\n",
      " - Batch no: 24, train loss: 0.47914236783981323\n",
      "-Validation loss after epoc: 0.48196327686309814 \n",
      "Epoch:905 \n",
      " - Batch no: 0, train loss: 0.5554153919219971\n",
      " - Batch no: 8, train loss: 0.8916192054748535\n",
      " - Batch no: 16, train loss: 0.4261969327926636\n",
      " - Batch no: 24, train loss: 0.5432317852973938\n",
      "-Validation loss after epoc: 0.48159950971603394 \n",
      "Epoch:906 \n",
      " - Batch no: 0, train loss: 0.6275215744972229\n",
      " - Batch no: 8, train loss: 0.5439948439598083\n",
      " - Batch no: 16, train loss: 0.664106547832489\n",
      " - Batch no: 24, train loss: 0.5299733877182007\n",
      "-Validation loss after epoc: 0.4817953109741211 \n",
      "Epoch:907 \n",
      " - Batch no: 0, train loss: 0.48017433285713196\n",
      " - Batch no: 8, train loss: 0.589568555355072\n",
      " - Batch no: 16, train loss: 0.7080119848251343\n",
      " - Batch no: 24, train loss: 0.46451979875564575\n",
      "-Validation loss after epoc: 0.4814850687980652 \n",
      "Epoch:908 \n",
      " - Batch no: 0, train loss: 0.5955162644386292\n",
      " - Batch no: 8, train loss: 0.5724853873252869\n",
      " - Batch no: 16, train loss: 0.5806610584259033\n",
      " - Batch no: 24, train loss: 0.6016655564308167\n",
      "-Validation loss after epoc: 0.4818631708621979 \n",
      "Epoch:909 \n",
      " - Batch no: 0, train loss: 0.5321149826049805\n",
      " - Batch no: 8, train loss: 0.3749396800994873\n",
      " - Batch no: 16, train loss: 0.4836856722831726\n",
      " - Batch no: 24, train loss: 0.5987464785575867\n",
      "-Validation loss after epoc: 0.4817854166030884 \n",
      "Epoch:910 \n",
      " - Batch no: 0, train loss: 0.4822291135787964\n",
      " - Batch no: 8, train loss: 0.5382869243621826\n",
      " - Batch no: 16, train loss: 0.5675966739654541\n",
      " - Batch no: 24, train loss: 0.46080195903778076\n",
      "-Validation loss after epoc: 0.4812316298484802 \n",
      "Epoch:911 \n",
      " - Batch no: 0, train loss: 0.5027629733085632\n",
      " - Batch no: 8, train loss: 0.4640125334262848\n",
      " - Batch no: 16, train loss: 0.4343520998954773\n",
      " - Batch no: 24, train loss: 0.52970951795578\n",
      "-Validation loss after epoc: 0.4811434745788574 \n",
      "Epoch:912 \n",
      " - Batch no: 0, train loss: 0.5112122297286987\n",
      " - Batch no: 8, train loss: 0.6159887909889221\n",
      " - Batch no: 16, train loss: 0.4644223749637604\n",
      " - Batch no: 24, train loss: 0.7289770245552063\n",
      "-Validation loss after epoc: 0.48136356472969055 \n",
      "Epoch:913 \n",
      " - Batch no: 0, train loss: 0.4876269996166229\n",
      " - Batch no: 8, train loss: 0.5641830563545227\n",
      " - Batch no: 16, train loss: 0.4257221817970276\n",
      " - Batch no: 24, train loss: 0.35484540462493896\n",
      "-Validation loss after epoc: 0.4815259575843811 \n",
      "Epoch:914 \n",
      " - Batch no: 0, train loss: 0.6702980995178223\n",
      " - Batch no: 8, train loss: 0.4667983651161194\n",
      " - Batch no: 16, train loss: 0.6953040361404419\n",
      " - Batch no: 24, train loss: 0.39975592494010925\n",
      "-Validation loss after epoc: 0.48155781626701355 \n",
      "Epoch:915 \n",
      " - Batch no: 0, train loss: 0.43237775564193726\n",
      " - Batch no: 8, train loss: 0.38423243165016174\n",
      " - Batch no: 16, train loss: 0.4297529458999634\n",
      " - Batch no: 24, train loss: 0.3555322289466858\n",
      "-Validation loss after epoc: 0.48135343194007874 \n",
      "Epoch:916 \n",
      " - Batch no: 0, train loss: 0.42966556549072266\n",
      " - Batch no: 8, train loss: 0.801455557346344\n",
      " - Batch no: 16, train loss: 0.6623807549476624\n",
      " - Batch no: 24, train loss: 0.47513753175735474\n",
      "-Validation loss after epoc: 0.48145291209220886 \n",
      "Epoch:917 \n",
      " - Batch no: 0, train loss: 0.5801593661308289\n",
      " - Batch no: 8, train loss: 0.638410747051239\n",
      " - Batch no: 16, train loss: 0.7220912575721741\n",
      " - Batch no: 24, train loss: 0.5541021823883057\n",
      "-Validation loss after epoc: 0.48179513216018677 \n",
      "Epoch:918 \n",
      " - Batch no: 0, train loss: 0.5909632444381714\n",
      " - Batch no: 8, train loss: 0.5865809917449951\n",
      " - Batch no: 16, train loss: 0.5408315062522888\n",
      " - Batch no: 24, train loss: 0.6800699830055237\n",
      "-Validation loss after epoc: 0.48139020800590515 \n",
      "Epoch:919 \n",
      " - Batch no: 0, train loss: 0.4366453289985657\n",
      " - Batch no: 8, train loss: 0.513506293296814\n",
      " - Batch no: 16, train loss: 0.4619083106517792\n",
      " - Batch no: 24, train loss: 0.6161243319511414\n",
      "-Validation loss after epoc: 0.48162221908569336 \n",
      "Epoch:920 \n",
      " - Batch no: 0, train loss: 0.4857887029647827\n",
      " - Batch no: 8, train loss: 0.5341706871986389\n",
      " - Batch no: 16, train loss: 0.485548198223114\n",
      " - Batch no: 24, train loss: 0.664723813533783\n",
      "-Validation loss after epoc: 0.4819071292877197 \n",
      "Epoch:921 \n",
      " - Batch no: 0, train loss: 0.8047853112220764\n",
      " - Batch no: 8, train loss: 0.6110527515411377\n",
      " - Batch no: 16, train loss: 0.3107031583786011\n",
      " - Batch no: 24, train loss: 0.5940060615539551\n",
      "-Validation loss after epoc: 0.4812590777873993 \n",
      "Epoch:922 \n",
      " - Batch no: 0, train loss: 0.6846548318862915\n",
      " - Batch no: 8, train loss: 0.5377056002616882\n",
      " - Batch no: 16, train loss: 0.6165655255317688\n",
      " - Batch no: 24, train loss: 0.427002489566803\n",
      "-Validation loss after epoc: 0.48097002506256104 \n",
      "Epoch:923 \n",
      " - Batch no: 0, train loss: 0.45408305525779724\n",
      " - Batch no: 8, train loss: 0.5202180743217468\n",
      " - Batch no: 16, train loss: 0.4384947419166565\n",
      " - Batch no: 24, train loss: 0.6026942729949951\n",
      "-Validation loss after epoc: 0.4810818135738373 \n",
      "Epoch:924 \n",
      " - Batch no: 0, train loss: 0.5499033331871033\n",
      " - Batch no: 8, train loss: 0.683595597743988\n",
      " - Batch no: 16, train loss: 0.5687320232391357\n",
      " - Batch no: 24, train loss: 0.6113094687461853\n",
      "-Validation loss after epoc: 0.48080500960350037 \n",
      "Epoch:925 \n",
      " - Batch no: 0, train loss: 0.4731539189815521\n",
      " - Batch no: 8, train loss: 0.38362884521484375\n",
      " - Batch no: 16, train loss: 0.5288966298103333\n",
      " - Batch no: 24, train loss: 0.4601103365421295\n",
      "-Validation loss after epoc: 0.4807529151439667 \n",
      "Epoch:926 \n",
      " - Batch no: 0, train loss: 0.4473828077316284\n",
      " - Batch no: 8, train loss: 0.625973641872406\n",
      " - Batch no: 16, train loss: 0.582594633102417\n",
      " - Batch no: 24, train loss: 0.6271181106567383\n",
      "-Validation loss after epoc: 0.4808540344238281 \n",
      "Epoch:927 \n",
      " - Batch no: 0, train loss: 0.5314542055130005\n",
      " - Batch no: 8, train loss: 0.48747363686561584\n",
      " - Batch no: 16, train loss: 0.5981628894805908\n",
      " - Batch no: 24, train loss: 0.4381072223186493\n",
      "-Validation loss after epoc: 0.4813152551651001 \n",
      "Epoch:928 \n",
      " - Batch no: 0, train loss: 0.593830943107605\n",
      " - Batch no: 8, train loss: 0.44818249344825745\n",
      " - Batch no: 16, train loss: 0.4444162845611572\n",
      " - Batch no: 24, train loss: 0.7220279574394226\n",
      "-Validation loss after epoc: 0.48088136315345764 \n",
      "Epoch:929 \n",
      " - Batch no: 0, train loss: 0.6375700831413269\n",
      " - Batch no: 8, train loss: 0.38075143098831177\n",
      " - Batch no: 16, train loss: 0.4549678862094879\n",
      " - Batch no: 24, train loss: 0.3910978138446808\n",
      "-Validation loss after epoc: 0.48115721344947815 \n",
      "Epoch:930 \n",
      " - Batch no: 0, train loss: 0.6699052453041077\n",
      " - Batch no: 8, train loss: 0.44173097610473633\n",
      " - Batch no: 16, train loss: 0.44071730971336365\n",
      " - Batch no: 24, train loss: 0.40615352988243103\n",
      "-Validation loss after epoc: 0.48072391748428345 \n",
      "Epoch:931 \n",
      " - Batch no: 0, train loss: 0.5456344485282898\n",
      " - Batch no: 8, train loss: 0.4974803030490875\n",
      " - Batch no: 16, train loss: 0.5280666351318359\n",
      " - Batch no: 24, train loss: 0.43606382608413696\n",
      "-Validation loss after epoc: 0.48108866810798645 \n",
      "Epoch:932 \n",
      " - Batch no: 0, train loss: 0.513593316078186\n",
      " - Batch no: 8, train loss: 0.6044508814811707\n",
      " - Batch no: 16, train loss: 0.6094771027565002\n",
      " - Batch no: 24, train loss: 0.5648068189620972\n",
      "-Validation loss after epoc: 0.48035043478012085 \n",
      "Epoch:933 \n",
      " - Batch no: 0, train loss: 0.5273529291152954\n",
      " - Batch no: 8, train loss: 0.48974937200546265\n",
      " - Batch no: 16, train loss: 0.3426990211009979\n",
      " - Batch no: 24, train loss: 0.48296451568603516\n",
      "-Validation loss after epoc: 0.48044300079345703 \n",
      "Epoch:934 \n",
      " - Batch no: 0, train loss: 0.6103824973106384\n",
      " - Batch no: 8, train loss: 0.5164134502410889\n",
      " - Batch no: 16, train loss: 0.4437014162540436\n",
      " - Batch no: 24, train loss: 0.49495938420295715\n",
      "-Validation loss after epoc: 0.4802226126194 \n",
      "Epoch:935 \n",
      " - Batch no: 0, train loss: 0.47395116090774536\n",
      " - Batch no: 8, train loss: 0.45411548018455505\n",
      " - Batch no: 16, train loss: 0.5055325627326965\n",
      " - Batch no: 24, train loss: 0.5264086127281189\n",
      "-Validation loss after epoc: 0.48072293400764465 \n",
      "Epoch:936 \n",
      " - Batch no: 0, train loss: 0.51324063539505\n",
      " - Batch no: 8, train loss: 0.4788653254508972\n",
      " - Batch no: 16, train loss: 0.5702998042106628\n",
      " - Batch no: 24, train loss: 0.4470595419406891\n",
      "-Validation loss after epoc: 0.48068612813949585 \n",
      "Epoch:937 \n",
      " - Batch no: 0, train loss: 0.8234920501708984\n",
      " - Batch no: 8, train loss: 0.42770564556121826\n",
      " - Batch no: 16, train loss: 0.6127576231956482\n",
      " - Batch no: 24, train loss: 0.47893399000167847\n",
      "-Validation loss after epoc: 0.4808333218097687 \n",
      "Epoch:938 \n",
      " - Batch no: 0, train loss: 0.5906422138214111\n",
      " - Batch no: 8, train loss: 0.39418885111808777\n",
      " - Batch no: 16, train loss: 0.5231993794441223\n",
      " - Batch no: 24, train loss: 0.499741792678833\n",
      "-Validation loss after epoc: 0.4804399609565735 \n",
      "Epoch:939 \n",
      " - Batch no: 0, train loss: 0.5193442106246948\n",
      " - Batch no: 8, train loss: 0.42558753490448\n",
      " - Batch no: 16, train loss: 0.42731019854545593\n",
      " - Batch no: 24, train loss: 0.8028292655944824\n",
      "-Validation loss after epoc: 0.48094043135643005 \n",
      "Epoch:940 \n",
      " - Batch no: 0, train loss: 0.49705031514167786\n",
      " - Batch no: 8, train loss: 0.44012999534606934\n",
      " - Batch no: 16, train loss: 0.5921529531478882\n",
      " - Batch no: 24, train loss: 0.3324677050113678\n",
      "-Validation loss after epoc: 0.48042282462120056 \n",
      "Epoch:941 \n",
      " - Batch no: 0, train loss: 0.5561005473136902\n",
      " - Batch no: 8, train loss: 0.46288391947746277\n",
      " - Batch no: 16, train loss: 0.561758279800415\n",
      " - Batch no: 24, train loss: 0.5772683024406433\n",
      "-Validation loss after epoc: 0.48024478554725647 \n",
      "Epoch:942 \n",
      " - Batch no: 0, train loss: 0.6607439517974854\n",
      " - Batch no: 8, train loss: 0.5545485019683838\n",
      " - Batch no: 16, train loss: 0.44780829548835754\n",
      " - Batch no: 24, train loss: 0.701482892036438\n",
      "-Validation loss after epoc: 0.48030272126197815 \n",
      "Epoch:943 \n",
      " - Batch no: 0, train loss: 0.516029417514801\n",
      " - Batch no: 8, train loss: 0.3829807639122009\n",
      " - Batch no: 16, train loss: 0.3951380252838135\n",
      " - Batch no: 24, train loss: 0.4850434958934784\n",
      "-Validation loss after epoc: 0.48025208711624146 \n",
      "Epoch:944 \n",
      " - Batch no: 0, train loss: 0.5162636041641235\n",
      " - Batch no: 8, train loss: 0.6953612565994263\n",
      " - Batch no: 16, train loss: 0.4474303424358368\n",
      " - Batch no: 24, train loss: 0.5858337879180908\n",
      "-Validation loss after epoc: 0.4800598919391632 \n",
      "Epoch:945 \n",
      " - Batch no: 0, train loss: 0.6099373698234558\n",
      " - Batch no: 8, train loss: 0.4618694484233856\n",
      " - Batch no: 16, train loss: 0.6682278513908386\n",
      " - Batch no: 24, train loss: 0.4275147616863251\n",
      "-Validation loss after epoc: 0.47997334599494934 \n",
      "Epoch:946 \n",
      " - Batch no: 0, train loss: 0.34742599725723267\n",
      " - Batch no: 8, train loss: 0.7294931411743164\n",
      " - Batch no: 16, train loss: 0.5422247648239136\n",
      " - Batch no: 24, train loss: 0.7151458263397217\n",
      "-Validation loss after epoc: 0.4797566533088684 \n",
      "Epoch:947 \n",
      " - Batch no: 0, train loss: 0.6830170750617981\n",
      " - Batch no: 8, train loss: 0.5681846141815186\n",
      " - Batch no: 16, train loss: 0.34007105231285095\n",
      " - Batch no: 24, train loss: 0.5678696632385254\n",
      "-Validation loss after epoc: 0.4798802435398102 \n",
      "Epoch:948 \n",
      " - Batch no: 0, train loss: 0.6152419447898865\n",
      " - Batch no: 8, train loss: 0.6105400919914246\n",
      " - Batch no: 16, train loss: 0.525354266166687\n",
      " - Batch no: 24, train loss: 0.5037817358970642\n",
      "-Validation loss after epoc: 0.48111891746520996 \n",
      "Epoch:949 \n",
      " - Batch no: 0, train loss: 0.4719763398170471\n",
      " - Batch no: 8, train loss: 0.4959569573402405\n",
      " - Batch no: 16, train loss: 0.7274671196937561\n",
      " - Batch no: 24, train loss: 0.486171692609787\n",
      "-Validation loss after epoc: 0.4799918830394745 \n",
      "Epoch:950 \n",
      " - Batch no: 0, train loss: 0.4931902587413788\n",
      " - Batch no: 8, train loss: 0.5383555889129639\n",
      " - Batch no: 16, train loss: 0.49706676602363586\n",
      " - Batch no: 24, train loss: 0.7025115489959717\n",
      "-Validation loss after epoc: 0.479807049036026 \n",
      "Epoch:951 \n",
      " - Batch no: 0, train loss: 0.3839832842350006\n",
      " - Batch no: 8, train loss: 0.494511216878891\n",
      " - Batch no: 16, train loss: 0.5156458020210266\n",
      " - Batch no: 24, train loss: 0.49934253096580505\n",
      "-Validation loss after epoc: 0.48011553287506104 \n",
      "Epoch:952 \n",
      " - Batch no: 0, train loss: 0.48854145407676697\n",
      " - Batch no: 8, train loss: 0.4492563307285309\n",
      " - Batch no: 16, train loss: 0.5755888819694519\n",
      " - Batch no: 24, train loss: 0.4999449551105499\n",
      "-Validation loss after epoc: 0.4799674451351166 \n",
      "Epoch:953 \n",
      " - Batch no: 0, train loss: 0.475292831659317\n",
      " - Batch no: 8, train loss: 0.4774710237979889\n",
      " - Batch no: 16, train loss: 0.5787841081619263\n",
      " - Batch no: 24, train loss: 0.41015490889549255\n",
      "-Validation loss after epoc: 0.4800557792186737 \n",
      "Epoch:954 \n",
      " - Batch no: 0, train loss: 0.4989987313747406\n",
      " - Batch no: 8, train loss: 0.6446408629417419\n",
      " - Batch no: 16, train loss: 0.523870050907135\n",
      " - Batch no: 24, train loss: 0.4248410165309906\n",
      "-Validation loss after epoc: 0.48029881715774536 \n",
      "Epoch:955 \n",
      " - Batch no: 0, train loss: 0.7639042139053345\n",
      " - Batch no: 8, train loss: 0.5526294708251953\n",
      " - Batch no: 16, train loss: 0.5598030090332031\n",
      " - Batch no: 24, train loss: 0.2923627495765686\n",
      "-Validation loss after epoc: 0.47997990250587463 \n",
      "Epoch:956 \n",
      " - Batch no: 0, train loss: 0.38485580682754517\n",
      " - Batch no: 8, train loss: 0.6749857068061829\n",
      " - Batch no: 16, train loss: 0.593548059463501\n",
      " - Batch no: 24, train loss: 0.5121583938598633\n",
      "-Validation loss after epoc: 0.4799402952194214 \n",
      "Epoch:957 \n",
      " - Batch no: 0, train loss: 0.41036802530288696\n",
      " - Batch no: 8, train loss: 0.4787296950817108\n",
      " - Batch no: 16, train loss: 0.5707181692123413\n",
      " - Batch no: 24, train loss: 0.43211010098457336\n",
      "-Validation loss after epoc: 0.4797670841217041 \n",
      "Epoch:958 \n",
      " - Batch no: 0, train loss: 0.6222752928733826\n",
      " - Batch no: 8, train loss: 0.5901978015899658\n",
      " - Batch no: 16, train loss: 0.6264805793762207\n",
      " - Batch no: 24, train loss: 0.7679204940795898\n",
      "-Validation loss after epoc: 0.48000264167785645 \n",
      "Epoch:959 \n",
      " - Batch no: 0, train loss: 0.5768250226974487\n",
      " - Batch no: 8, train loss: 0.48701339960098267\n",
      " - Batch no: 16, train loss: 0.46659326553344727\n",
      " - Batch no: 24, train loss: 0.6166578531265259\n",
      "-Validation loss after epoc: 0.47982510924339294 \n",
      "Epoch:960 \n",
      " - Batch no: 0, train loss: 0.520903468132019\n",
      " - Batch no: 8, train loss: 0.4563865065574646\n",
      " - Batch no: 16, train loss: 0.6279257535934448\n",
      " - Batch no: 24, train loss: 0.42869845032691956\n",
      "-Validation loss after epoc: 0.47993555665016174 \n",
      "Epoch:961 \n",
      " - Batch no: 0, train loss: 0.42182862758636475\n",
      " - Batch no: 8, train loss: 0.5956280827522278\n",
      " - Batch no: 16, train loss: 0.4336105287075043\n",
      " - Batch no: 24, train loss: 0.6753237247467041\n",
      "-Validation loss after epoc: 0.4801757037639618 \n",
      "Epoch:962 \n",
      " - Batch no: 0, train loss: 0.5654126405715942\n",
      " - Batch no: 8, train loss: 0.5195124745368958\n",
      " - Batch no: 16, train loss: 0.5966203808784485\n",
      " - Batch no: 24, train loss: 0.4282463788986206\n",
      "-Validation loss after epoc: 0.48032787442207336 \n",
      "Epoch:963 \n",
      " - Batch no: 0, train loss: 0.6040061116218567\n",
      " - Batch no: 8, train loss: 0.6934910416603088\n",
      " - Batch no: 16, train loss: 0.5366799235343933\n",
      " - Batch no: 24, train loss: 0.5394122004508972\n",
      "-Validation loss after epoc: 0.4798445999622345 \n",
      "Epoch:964 \n",
      " - Batch no: 0, train loss: 0.47009941935539246\n",
      " - Batch no: 8, train loss: 0.539169192314148\n",
      " - Batch no: 16, train loss: 0.6263172626495361\n",
      " - Batch no: 24, train loss: 0.5491319894790649\n",
      "-Validation loss after epoc: 0.4794940650463104 \n",
      "Epoch:965 \n",
      " - Batch no: 0, train loss: 0.480205237865448\n",
      " - Batch no: 8, train loss: 0.48898065090179443\n",
      " - Batch no: 16, train loss: 0.6485554575920105\n",
      " - Batch no: 24, train loss: 0.5282670259475708\n",
      "-Validation loss after epoc: 0.4795229434967041 \n",
      "Epoch:966 \n",
      " - Batch no: 0, train loss: 0.44732460379600525\n",
      " - Batch no: 8, train loss: 0.5293641090393066\n",
      " - Batch no: 16, train loss: 0.5924732685089111\n",
      " - Batch no: 24, train loss: 0.5034329295158386\n",
      "-Validation loss after epoc: 0.47967368364334106 \n",
      "Epoch:967 \n",
      " - Batch no: 0, train loss: 0.5339701175689697\n",
      " - Batch no: 8, train loss: 0.4284755289554596\n",
      " - Batch no: 16, train loss: 0.564584493637085\n",
      " - Batch no: 24, train loss: 0.5010624527931213\n",
      "-Validation loss after epoc: 0.4798283576965332 \n",
      "Epoch:968 \n",
      " - Batch no: 0, train loss: 0.5031609535217285\n",
      " - Batch no: 8, train loss: 0.4574016332626343\n",
      " - Batch no: 16, train loss: 0.5995660424232483\n",
      " - Batch no: 24, train loss: 0.5749346613883972\n",
      "-Validation loss after epoc: 0.48041999340057373 \n",
      "Epoch:969 \n",
      " - Batch no: 0, train loss: 0.6533755660057068\n",
      " - Batch no: 8, train loss: 0.44915854930877686\n",
      " - Batch no: 16, train loss: 0.44316598773002625\n",
      " - Batch no: 24, train loss: 0.6447159647941589\n",
      "-Validation loss after epoc: 0.4802507758140564 \n",
      "Epoch:970 \n",
      " - Batch no: 0, train loss: 0.5471310615539551\n",
      " - Batch no: 8, train loss: 0.6094631552696228\n",
      " - Batch no: 16, train loss: 0.6497828960418701\n",
      " - Batch no: 24, train loss: 0.37652716040611267\n",
      "-Validation loss after epoc: 0.4801117181777954 \n",
      "Epoch:971 \n",
      " - Batch no: 0, train loss: 0.4328452944755554\n",
      " - Batch no: 8, train loss: 0.7000685334205627\n",
      " - Batch no: 16, train loss: 0.7151538729667664\n",
      " - Batch no: 24, train loss: 0.6927199959754944\n",
      "-Validation loss after epoc: 0.4803648591041565 \n",
      "Epoch:972 \n",
      " - Batch no: 0, train loss: 0.533301055431366\n",
      " - Batch no: 8, train loss: 0.5280801057815552\n",
      " - Batch no: 16, train loss: 0.33144065737724304\n",
      " - Batch no: 24, train loss: 0.5869430899620056\n",
      "-Validation loss after epoc: 0.48009470105171204 \n",
      "Epoch:973 \n",
      " - Batch no: 0, train loss: 0.7293615937232971\n",
      " - Batch no: 8, train loss: 0.5704623460769653\n",
      " - Batch no: 16, train loss: 0.5321465730667114\n",
      " - Batch no: 24, train loss: 0.43312591314315796\n",
      "-Validation loss after epoc: 0.48012620210647583 \n",
      "Epoch:974 \n",
      " - Batch no: 0, train loss: 0.46730154752731323\n",
      " - Batch no: 8, train loss: 0.6597214937210083\n",
      " - Batch no: 16, train loss: 0.6355734467506409\n",
      " - Batch no: 24, train loss: 0.6360780596733093\n",
      "-Validation loss after epoc: 0.48012620210647583 \n",
      "Epoch:975 \n",
      " - Batch no: 0, train loss: 0.7240585684776306\n",
      " - Batch no: 8, train loss: 0.49987733364105225\n",
      " - Batch no: 16, train loss: 0.5730419754981995\n",
      " - Batch no: 24, train loss: 0.5461995601654053\n",
      "-Validation loss after epoc: 0.4795513153076172 \n",
      "Epoch:976 \n",
      " - Batch no: 0, train loss: 0.4894537031650543\n",
      " - Batch no: 8, train loss: 0.7271413207054138\n",
      " - Batch no: 16, train loss: 0.6126216053962708\n",
      " - Batch no: 24, train loss: 0.6123405694961548\n",
      "-Validation loss after epoc: 0.4798921048641205 \n",
      "Epoch:977 \n",
      " - Batch no: 0, train loss: 0.5563352704048157\n",
      " - Batch no: 8, train loss: 0.49086689949035645\n",
      " - Batch no: 16, train loss: 0.49302172660827637\n",
      " - Batch no: 24, train loss: 0.4746686518192291\n",
      "-Validation loss after epoc: 0.4795845150947571 \n",
      "Epoch:978 \n",
      " - Batch no: 0, train loss: 0.5472912192344666\n",
      " - Batch no: 8, train loss: 0.5226675868034363\n",
      " - Batch no: 16, train loss: 0.4891769587993622\n",
      " - Batch no: 24, train loss: 0.5112308859825134\n",
      "-Validation loss after epoc: 0.4793509244918823 \n",
      "Epoch:979 \n",
      " - Batch no: 0, train loss: 0.6118196845054626\n",
      " - Batch no: 8, train loss: 0.5922709107398987\n",
      " - Batch no: 16, train loss: 0.5250604748725891\n",
      " - Batch no: 24, train loss: 0.6502442359924316\n",
      "-Validation loss after epoc: 0.47927936911582947 \n",
      "Epoch:980 \n",
      " - Batch no: 0, train loss: 0.43470898270606995\n",
      " - Batch no: 8, train loss: 0.4467746913433075\n",
      " - Batch no: 16, train loss: 0.5374835133552551\n",
      " - Batch no: 24, train loss: 0.6360311508178711\n",
      "-Validation loss after epoc: 0.47944214940071106 \n",
      "Epoch:981 \n",
      " - Batch no: 0, train loss: 0.5320709943771362\n",
      " - Batch no: 8, train loss: 0.46461400389671326\n",
      " - Batch no: 16, train loss: 0.49676331877708435\n",
      " - Batch no: 24, train loss: 0.5586273670196533\n",
      "-Validation loss after epoc: 0.4794749617576599 \n",
      "Epoch:982 \n",
      " - Batch no: 0, train loss: 0.5205988883972168\n",
      " - Batch no: 8, train loss: 0.5238375663757324\n",
      " - Batch no: 16, train loss: 0.4019968509674072\n",
      " - Batch no: 24, train loss: 0.5319030284881592\n",
      "-Validation loss after epoc: 0.4794606566429138 \n",
      "Epoch:983 \n",
      " - Batch no: 0, train loss: 0.5950743556022644\n",
      " - Batch no: 8, train loss: 0.5593718886375427\n",
      " - Batch no: 16, train loss: 0.499990314245224\n",
      " - Batch no: 24, train loss: 0.5308140516281128\n",
      "-Validation loss after epoc: 0.4794265329837799 \n",
      "Epoch:984 \n",
      " - Batch no: 0, train loss: 0.5642441511154175\n",
      " - Batch no: 8, train loss: 0.4865049421787262\n",
      " - Batch no: 16, train loss: 0.6132474541664124\n",
      " - Batch no: 24, train loss: 0.40885719656944275\n",
      "-Validation loss after epoc: 0.4792547821998596 \n",
      "Epoch:985 \n",
      " - Batch no: 0, train loss: 0.478325217962265\n",
      " - Batch no: 8, train loss: 0.4329557418823242\n",
      " - Batch no: 16, train loss: 0.5684664249420166\n",
      " - Batch no: 24, train loss: 0.44868069887161255\n",
      "-Validation loss after epoc: 0.4793797731399536 \n",
      "Epoch:986 \n",
      " - Batch no: 0, train loss: 0.5894500613212585\n",
      " - Batch no: 8, train loss: 0.4664641320705414\n",
      " - Batch no: 16, train loss: 0.5121402144432068\n",
      " - Batch no: 24, train loss: 0.3841848075389862\n",
      "-Validation loss after epoc: 0.47950392961502075 \n",
      "Epoch:987 \n",
      " - Batch no: 0, train loss: 0.6965571641921997\n",
      " - Batch no: 8, train loss: 0.35808083415031433\n",
      " - Batch no: 16, train loss: 0.5368537306785583\n",
      " - Batch no: 24, train loss: 0.5374530553817749\n",
      "-Validation loss after epoc: 0.4792187809944153 \n",
      "Epoch:988 \n",
      " - Batch no: 0, train loss: 0.3094691038131714\n",
      " - Batch no: 8, train loss: 0.5197758078575134\n",
      " - Batch no: 16, train loss: 0.6763195395469666\n",
      " - Batch no: 24, train loss: 0.4644378125667572\n",
      "-Validation loss after epoc: 0.479007750749588 \n",
      "Epoch:989 \n",
      " - Batch no: 0, train loss: 0.6792252063751221\n",
      " - Batch no: 8, train loss: 0.5740946531295776\n",
      " - Batch no: 16, train loss: 0.5898436307907104\n",
      " - Batch no: 24, train loss: 0.5371085405349731\n",
      "-Validation loss after epoc: 0.479391485452652 \n",
      "Epoch:990 \n",
      " - Batch no: 0, train loss: 0.4555692672729492\n",
      " - Batch no: 8, train loss: 0.7184963226318359\n",
      " - Batch no: 16, train loss: 0.507300615310669\n",
      " - Batch no: 24, train loss: 0.5730292201042175\n",
      "-Validation loss after epoc: 0.47951897978782654 \n",
      "Epoch:991 \n",
      " - Batch no: 0, train loss: 0.7102490663528442\n",
      " - Batch no: 8, train loss: 0.5798628926277161\n",
      " - Batch no: 16, train loss: 0.7759006023406982\n",
      " - Batch no: 24, train loss: 0.44832298159599304\n",
      "-Validation loss after epoc: 0.479053258895874 \n",
      "Epoch:992 \n",
      " - Batch no: 0, train loss: 0.781579315662384\n",
      " - Batch no: 8, train loss: 0.4694044589996338\n",
      " - Batch no: 16, train loss: 0.5882922410964966\n",
      " - Batch no: 24, train loss: 0.5833694338798523\n",
      "-Validation loss after epoc: 0.4796343743801117 \n",
      "Epoch:993 \n",
      " - Batch no: 0, train loss: 0.8583894968032837\n",
      " - Batch no: 8, train loss: 0.4374137222766876\n",
      " - Batch no: 16, train loss: 0.6433567404747009\n",
      " - Batch no: 24, train loss: 0.5428100824356079\n",
      "-Validation loss after epoc: 0.47911718487739563 \n",
      "Epoch:994 \n",
      " - Batch no: 0, train loss: 0.5840701460838318\n",
      " - Batch no: 8, train loss: 0.4977944791316986\n",
      " - Batch no: 16, train loss: 0.5577061772346497\n",
      " - Batch no: 24, train loss: 0.7369279265403748\n",
      "-Validation loss after epoc: 0.4788006842136383 \n",
      "Epoch:995 \n",
      " - Batch no: 0, train loss: 0.7065437436103821\n",
      " - Batch no: 8, train loss: 0.5498764514923096\n",
      " - Batch no: 16, train loss: 0.7219252586364746\n",
      " - Batch no: 24, train loss: 0.4395389258861542\n",
      "-Validation loss after epoc: 0.4798159599304199 \n",
      "Epoch:996 \n",
      " - Batch no: 0, train loss: 0.6247736811637878\n",
      " - Batch no: 8, train loss: 0.5698293447494507\n",
      " - Batch no: 16, train loss: 0.4980727732181549\n",
      " - Batch no: 24, train loss: 0.5066501498222351\n",
      "-Validation loss after epoc: 0.47930437326431274 \n",
      "Epoch:997 \n",
      " - Batch no: 0, train loss: 0.5864989757537842\n",
      " - Batch no: 8, train loss: 0.49295786023139954\n",
      " - Batch no: 16, train loss: 0.5256567597389221\n",
      " - Batch no: 24, train loss: 0.4806009829044342\n",
      "-Validation loss after epoc: 0.479701966047287 \n",
      "Epoch:998 \n",
      " - Batch no: 0, train loss: 0.7829392552375793\n",
      " - Batch no: 8, train loss: 0.6979450583457947\n",
      " - Batch no: 16, train loss: 0.5969260931015015\n",
      " - Batch no: 24, train loss: 0.5956748127937317\n",
      "-Validation loss after epoc: 0.47914186120033264 \n",
      "Epoch:999 \n",
      " - Batch no: 0, train loss: 0.4853353202342987\n",
      " - Batch no: 8, train loss: 0.6243587732315063\n",
      " - Batch no: 16, train loss: 0.4971776008605957\n",
      " - Batch no: 24, train loss: 0.4928579032421112\n",
      "-Validation loss after epoc: 0.47906240820884705 \n"
     ]
    }
   ],
   "source": [
    "global_step = 0\n",
    "with tf.Session() as sess:\n",
    "    tf_saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(logdir='./Iceberg/Autoencoder', graph=sess.graph)\n",
    "    for e in range(epochs):\n",
    "        n_batches = int(X_train.shape[0] / batch_size)\n",
    "        print(\"Epoch:{} \".format(e))\n",
    "        gen_train = datagen.flow(X_train, y_train,\n",
    "                                           batch_size=batch_size)\n",
    "        for batch in range(n_batches):\n",
    "            batch_data = gen_train.next()\n",
    "            sess.run(train_optimizer,feed_dict={x: np.reshape(batch_data[0],(-1,image_size*image_size)), \n",
    "                                                y_: np.reshape(batch_data[0],(-1,image_size*image_size))})\n",
    "            if(batch % 8 == 0):\n",
    "                batch_loss, summary = sess.run([loss, summary_ae], feed_dict={x: np.reshape(batch_data[0],(-1,image_size*image_size)), \n",
    "                                                                            y_: np.reshape(batch_data[0],(-1,image_size*image_size))})\n",
    "                print(\" - Batch no: {}, train loss: {}\".format(batch,batch_loss))\n",
    "            global_step += 1\n",
    "        batch_loss, summary = sess.run([loss, summary_ae], feed_dict={x: np.reshape(X_valid,(-1,image_size*image_size)), \n",
    "                                                                      y_: np.reshape(X_valid,(-1,image_size*image_size))})\n",
    "        print(\"-Validation loss after epoc: {} \".format(batch_loss))\n",
    "    #Save your model\n",
    "    tf_saver.save(sess, save_path='./Iceberg/Autoencoder/SavedModel')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
